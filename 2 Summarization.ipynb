{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "2 Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "cs182_hw3.1",
      "language": "python",
      "name": "cs182_hw3.1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvk67QawFLp4"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9k8OvG5FLqU"
      },
      "source": [
        "<center><h3>**Welcome to the Summarization Notebook.**</h3></center>\n",
        "\n",
        "In this assignment, you are going to train a neural network to summarize news articles.\n",
        "Your neural network is going to learn from example, as we provide you with (article, summary) pairs.\n",
        "We provide you with a **toy dataset** made of only articles about police related news.\n",
        "Usual datasets can be 20x larger in size, but we have reduced it for computational purposes.\n",
        "\n",
        "You will do this using a Transformer network, from the __[Attention is all you need](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)__ paper.\n",
        "In this assignment you will:\n",
        "- Learn to process text into sub-word tokens, to avoid fixed vocabulary sizes, and UNK tokens.\n",
        "- Implement the key conceptual blocks of a Transformer.\n",
        "- Use a Transformer to read a news article, and produce a summary.\n",
        "- Perform operations on learned word-vectors to examine what the model has learned.\n",
        "\n",
        "    \n",
        "** Before you start **\n",
        "\n",
        "You should read the Attention is all you need paper.\n",
        "We are providing you with skeleton code for the Transformer, but there will have to implement 5 conceptual blocks of the transformer yourself:\n",
        "-  AttentionQKV: the Query, Key, Value attention mechanism at the center of the Transformer\n",
        "- MultiHeadAttention: the multiple heads that enable each input to attend at many places at once.\n",
        "- PositionEmbedding: the sinusoid-based position embedding of the Transformer.\n",
        "- Encoder & Decoder: The encoder (that reads inputs, such as news articles), the decoder (that produces the output summary, one token at a time)\n",
        "- Full Transformer: piecing it all together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJVenhu9FLqX"
      },
      "source": [
        "All dataset files should be placed in the `dataset/` folder of this assignment.\n",
        "\n",
        "If you are using Google Colab, follow the instructions to mount your Google Drive onto the remote machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLzv6ksIFLqZ"
      },
      "source": [
        "# Library imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdFfsm8rI5ZG",
        "outputId": "b96e5ab5-57cb-434b-b873-e4626f2da47c"
      },
      "source": [
        "!pip install segtok\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting segtok\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok) (2019.12.20)\n",
            "Building wheels for collected packages: segtok\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25019 sha256=8054ff12b3256d90c3c67e791592e9802702799fa40a9885e7916dcfe8c6d561\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "Successfully built segtok\n",
            "Installing collected packages: segtok\n",
            "Successfully installed segtok-1.5.10\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 18.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne7LLf31hQlr"
      },
      "source": [
        "Run the first of the following two cells if you are running the homework locally, and run the second cell if you are running the homework in Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGyL5lIXI7qd"
      },
      "source": [
        "DRIVE=False\n",
        "root_folder = \"\"\n",
        "dataset_folder = \"dataset/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfwNJo3ihQlu",
        "outputId": "300a1bf0-cba0-4994-f6be-03e9dd2e02c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root_folder = \"/content/drive/My Drive/cs182_hw3/\"\n",
        "dataset_folder = \"/content/drive/My Drive/cs182_hw3_public/dataset/\""
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVz058nAmuG6",
        "outputId": "581cdd75-4ec8-4d38-8e70-24968d694fd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "drive.flush_and_unmount"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function google.colab.drive.flush_and_unmount>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdIkEfLxFLqb",
        "outputId": "28cf9ccf-435d-4023-9b9d-cd0a92d1479b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This cell autoreloads the notebook when you change you python file code.\n",
        "# If you think the notebook did not reload, rerun this cell.\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVY5twBBFLqe",
        "outputId": "eb2ca633-f05c-48ba-e700-630cdaccd971"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(root_folder)\n",
        "# from transformer import Transformer\n",
        "import sentencepiece as spm\n",
        "import torch as th\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import json\n",
        "import capita\n",
        "import os\n",
        "from transformer_utils import set_device\n",
        "import gc\n",
        "from utils import validate_to_array, model_out_to_list\n",
        "\n",
        "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "set_device(device)\n",
        "list_to_device = lambda th_obj: [tensor.to(device) for tensor in th_obj]"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQJeFimajElo",
        "outputId": "89a44c6a-6c1f-4389-c121-7a0f25f6a200",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(device)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IDVCg83FLqi"
      },
      "source": [
        "# Load the word piece model that will be used to tokenize the texts into\n",
        "# word pieces with a vocabulary size of 10000\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(root_folder+\"dataset/wp_vocab10000.model\")\n",
        "\n",
        "vocab = [line.split('\\t')[0] for line in open(root_folder+\"dataset/wp_vocab10000.vocab\", \"r\")]\n",
        "pad_index = vocab.index('#')\n",
        "\n",
        "def pad_sequence(numerized, pad_index, to_length):\n",
        "    pad = numerized[:to_length]\n",
        "    padded = pad + [pad_index] * (to_length - len(pad))\n",
        "    mask = [w != pad_index for w in padded]\n",
        "    return padded, mask"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "591lVK1BFLqk"
      },
      "source": [
        "# Building blocks of a Transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekya1G4qFLqm"
      },
      "source": [
        "**TODO**:\n",
        "\n",
        "Implement the 5 blocks of the Transformer. In order to finish this section, you should get very small error <1e-7 on each of the 5 checks in this section.\n",
        "\n",
        "\n",
        "The Transformer is split into 3 files: transformer_attention.py, transformer_utils.py and transformer.py\n",
        "\n",
        "Each section below gives you directions and a way to verify your code works properly.\n",
        "\n",
        "You do not need to modify the rest of the code provided, but should read it to understand overall architecture.\n",
        "\n",
        "Our Transformer is built as a Pytorch model, a standard that is good for you to get accustomed to.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG8RDjwKFLqo"
      },
      "source": [
        "## (1) Implementing the Query-Key-Value Attention (AttentionQKV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WurZuO8eFLqp"
      },
      "source": [
        "This part is located in AttentionQKV in transformer_attention.py. You must implement the call function of the class.\n",
        "You will need to implement the mathematical procedure of AttentionQKV that is described in the [Attention is all you need paper](https://arxiv.org/pdf/1706.03762.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjrFRxw3FLqr",
        "outputId": "2fcd0d12-cec5-4c63-9561-8a28407e8096"
      },
      "source": [
        "from transformer_attention import AttentionQKV\n",
        "\n",
        "batch_size = 2;\n",
        "n_queries = 3;\n",
        "n_keyval = 5;\n",
        "depth_k = 2;\n",
        "depth_v = 2\n",
        "\n",
        "with open(root_folder+\"transformer_checks/attention_qkv_io.json\", \"r\") as f:\n",
        "    io = json.load(f)\n",
        "    queries = th.tensor(io['queries'])\n",
        "    keys = th.tensor(io['keys'])\n",
        "    values = th.tensor(io['values'])\n",
        "    expected_output  = th.tensor(io['output'])\n",
        "    expected_weights = th.tensor(io['weights'])\n",
        "\n",
        "attn_qkv = AttentionQKV()\n",
        "queries, keys, values, expected_output, expected_weights = list_to_device((queries, keys, values, expected_output, expected_weights))\n",
        "output, weights = attn_qkv(queries, keys, values)\n",
        "validate_to_array(model_out_to_list,((queries,keys,values),attn_qkv),'attentionqkv', root_folder)\n",
        "print(\"Total error on the output:\",th.sum(th.abs(expected_output-output)).item(), \"(should be 0.0 or close to 0.0)\")\n",
        "print(\"Total error on the weights:\",th.sum(th.abs(expected_weights-weights)).item(), \"(should be 0.0 or close to 0.0)\")"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total error on the output: 2.4586915969848633e-07 (should be 0.0 or close to 0.0)\n",
            "Total error on the weights: 2.8032809495925903e-07 (should be 0.0 or close to 0.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvgZcum5FLqu"
      },
      "source": [
        "## (2) Implementing Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puN9fYvyFLqu"
      },
      "source": [
        "This part is located in the class MultiHeadProjection in transformer_attention.py.\n",
        "You must implement the call, \\_split_heads, and \\_combine_heads functions.\n",
        "\n",
        "**Procedure**\n",
        "\n",
        "The objective is to leverage the AttentionQKV class you already wrote.\n",
        "\n",
        "Your input are the queries, keys, values as 3-d tensors (batch_size, sequence_length, feature_size).\n",
        "\n",
        "Split them into 4-d tensors (batch_size, n_heads, sequence_length, new_feature_size). Where:\n",
        "$$feature\\_size = n\\_heads * new_{feature\\_size}.$$\n",
        "\n",
        "You can then feed the split qkv to your implemented AttentionQKV, which will treat each head as an independent attention function.\n",
        "\n",
        "Then the output must be combined back into a 3-d tensor.\n",
        "You can test the validity of your implementation in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZPz8ux1FLqw",
        "outputId": "43f1d9ff-0b18-41da-9f1a-e4b1590f2f9b"
      },
      "source": [
        "from transformer_attention import MultiHeadProjection\n",
        "\n",
        "batch_size = 2;\n",
        "n_queries = 3;\n",
        "n_heads = 4\n",
        "n_keyval = 5;\n",
        "depth_k = 8;\n",
        "depth_v = 8;\n",
        "\n",
        "with open(root_folder+\"transformer_checks/multihead_io.json\", \"r\") as f:\n",
        "    io = json.load(f)\n",
        "    queries = th.tensor(io['queries'])\n",
        "    keys = th.tensor(io['keys'])\n",
        "    values = th.tensor(io['values'])\n",
        "    expected_output  = th.tensor(io['output'])\n",
        "\n",
        "mhp = MultiHeadProjection(n_heads, (depth_k,depth_v))\n",
        "queries, keys, values, expected_output = list_to_device((queries, keys, values, expected_output))\n",
        "multihead_output = mhp((queries, keys, values))\n",
        "validate_to_array(model_out_to_list,(((queries,keys,values),),mhp),'multihead', root_folder)\n",
        "print(\"Total error on the output:\",th.sum(th.abs(expected_output-multihead_output)).item(), \"(should be 0.0 or close to 0.0)\")"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total error on the output: 8.763745427131653e-07 (should be 0.0 or close to 0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGU6SQ2RFLqz"
      },
      "source": [
        "## (3) Position Embedding "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_MEJtfTFLq0"
      },
      "source": [
        "You must implement the FeedForward and PositionEmbedding classes in transformer.py.\n",
        "\n",
        "\n",
        "The cell below helps you verify the validity of your implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyKCvdAnFLq1",
        "outputId": "d9896c1e-8994-406c-cfd1-12ede1b34129"
      },
      "source": [
        "from transformer import PositionEmbedding\n",
        "\n",
        "batch_size = 2;\n",
        "sequence_length = 3;\n",
        "dim = 4;\n",
        "\n",
        "with open(root_folder+\"transformer_checks/position_embedding_io.json\", \"r\") as f:\n",
        "    io = json.load(f)\n",
        "    inputs = th.tensor(io['inputs'])\n",
        "    expected_output  = th.tensor(io['output'])\n",
        "\n",
        "pos_emb = PositionEmbedding(dim)\n",
        "(inputs,expected_output,pos_emb) = list_to_device((inputs,expected_output,pos_emb))\n",
        "output_t = pos_emb(inputs)\n",
        "validate_to_array(model_out_to_list,((inputs,),pos_emb),'position_embedding', root_folder)\n",
        "print(\"Total error on the output:\",th.sum(th.abs(expected_output-output_t)).item(), \"(should be 0.0 or close to 0.0)\")"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total error on the output: 2.980232238769531e-07 (should be 0.0 or close to 0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpmSj3lKFLq3"
      },
      "source": [
        "## (4) Transformer Encoder / Transformer Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_OPPPkQFLq4"
      },
      "source": [
        "You now have all the blocks needed to implement the Transformer.\n",
        "For this part, you have to fill in 2 classes in the transformer.py file: TransformerEncoderBlock, TransformerDecoderBlock.\n",
        "\n",
        "The code below will verify the accuracy of each block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eTL20vGFLq5",
        "outputId": "1fc21847-8064-4693-fe97-17ed5a4f6fb9"
      },
      "source": [
        "from transformer import TransformerEncoderBlock\n",
        "\n",
        "batch_size = 2\n",
        "sequence_length = 5\n",
        "hidden_size = 6\n",
        "filter_size = 12\n",
        "n_heads = 2\n",
        "\n",
        "with open(root_folder+\"transformer_checks/transformer_encoder_block_io_new.json\", \"r\") as f:\n",
        "    io = json.load(f)\n",
        "    inputs = th.tensor(io['inputs'])\n",
        "    expected_output = th.tensor(io['output'])\n",
        "enc_block = TransformerEncoderBlock(input_size=6, n_heads=n_heads, filter_size=filter_size, hidden_size=hidden_size)\n",
        "# th.save(enc_block.state_dict(),root_folder+\"transformer_checks/transformer_encoder_block\")\n",
        "enc_block.load_state_dict(th.load(root_folder+\"transformer_checks/transformer_encoder_block\"))\n",
        "(inputs,expected_output,enc_block) = list_to_device((inputs,expected_output,enc_block))\n",
        "output_t = enc_block(inputs)\n",
        "validate_to_array(model_out_to_list,((inputs,),enc_block),'encoder_block', root_folder)\n",
        "print(\"Total error on the output:\",th.sum(th.abs(expected_output-output_t)).item(), \"(should be 0.0 or close to 0.0)\")"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total error on the output: 4.336237907409668e-06 (should be 0.0 or close to 0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdyhiZtsFLq7",
        "outputId": "c118678f-36e6-4b3f-ffa3-673022146d5c"
      },
      "source": [
        "from transformer import TransformerDecoderBlock\n",
        "batch_size = 2\n",
        "encoder_length = 5\n",
        "decoder_length = 3\n",
        "hidden_size = 6\n",
        "filter_size = 12\n",
        "n_heads = 2\n",
        "\n",
        "with open(root_folder+\"transformer_checks/transformer_decoder_block_io_new.json\", \"r\") as f:\n",
        "    io = json.load(f)\n",
        "    decoder_inputs = th.tensor(io['decoder_inputs'])\n",
        "    encoder_output = th.tensor(io['encoder_output'])\n",
        "    expected_output = th.tensor(io['output'])\n",
        "\n",
        "dec_block = TransformerDecoderBlock(input_size=6, n_heads=n_heads, filter_size=filter_size, hidden_size=hidden_size)\n",
        "dec_block.load_state_dict(th.load(root_folder+\"transformer_checks/transformer_decoder_block\"))\n",
        "(decoder_inputs,encoder_output,expected_output,dec_block) = list_to_device((decoder_inputs,encoder_output,expected_output,dec_block))\n",
        "output_t = dec_block(decoder_inputs, encoder_output)\n",
        "validate_to_array(model_out_to_list,((decoder_inputs, encoder_output),dec_block),'decoder_block', root_folder)\n",
        "print(\"Total error on the output:\",th.sum(th.abs(expected_output-output_t)).item(), \"(should be 0.0 or close to 0.0)\")\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total error on the output: 3.814697265625e-06 (should be 0.0 or close to 0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzuLYetAFLq9"
      },
      "source": [
        "## (5) Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4yTgzFzFLq_"
      },
      "source": [
        "This is the final high-level function that pieces it all together.\n",
        "\n",
        "You have to implement the call function of the Transformer class in the `transformer.py` file.\n",
        "\n",
        "The block below verifies your implementation is correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QHb6nyeFLrB",
        "outputId": "61cb75b6-667e-486e-bd50-c3af051b7e6c"
      },
      "source": [
        "from transformer import Transformer\n",
        "\n",
        "batch_size = 2\n",
        "vocab_size = 11\n",
        "n_layers = 3\n",
        "n_heads = 4\n",
        "d_model = 8\n",
        "d_filter = 16\n",
        "input_length = 5\n",
        "output_length = 3\n",
        "\n",
        "with open(root_folder+\"transformer_checks/transformer_io_new.json\", \"r\") as f:\n",
        "    io = json.load(f)\n",
        "    enc_input = th.tensor(io['enc_input'])\n",
        "    dec_input = th.tensor(io['dec_input'])\n",
        "    enc_mask = th.tensor(io['enc_mask'])\n",
        "    dec_mask = th.tensor(io['dec_mask'])\n",
        "    expected_output = th.tensor(io['output'])\n",
        "transformer = Transformer(vocab_size=vocab_size, n_layers=n_layers, n_heads=n_heads, d_model=d_model, d_filter=d_filter)\n",
        "transformer.load_state_dict(th.load(root_folder+\"transformer_checks/transformer\"))\n",
        "(enc_input,dec_input,enc_mask,dec_mask,expected_output,transformer) \\\n",
        "    = list_to_device((enc_input,dec_input,enc_mask,dec_mask,expected_output,transformer))\n",
        "output_t = transformer(enc_input, target_sequence=dec_input, encoder_mask=enc_mask, decoder_mask=dec_mask)\n",
        "validate_to_array(model_out_to_list, ((enc_input, dec_input, enc_mask, dec_mask),transformer),'transformer', root_folder)\n",
        "print(\"Total error on the output:\",th.sum(th.abs(expected_output-output_t)).item(), \"(should be 0.0 or close to 0.0)\")"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total error on the output: 4.369020462036133e-05 (should be 0.0 or close to 0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6usohlnXFLrD"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pukC3qFFLrF"
      },
      "source": [
        "Your objective is to train the Language on the dataset you are provided to reach a **validation loss <= 6.50**\n",
        "\n",
        "Careful: we will be testing this loss on an unreleased test set, so make sure to evaluate properly on a validation set and not overfit.\n",
        "\n",
        "You must save the model you want us to test under: models/final_transformer_summarization (the .index, .meta and .data files)\n",
        "\n",
        "**Advice**:\n",
        "- It should be possible to attain validation loss <= 6.50 with the model dimensions we've specified (n_layers=6, d_model=104, d_filter=416), but you can tune these hyperparameters. Increasing d_model will yield better model, at the cost of longer training time.\n",
        "- You should try tuning the learning rate, as well as what optimizer you use.\n",
        "- You might need to train for a few (up to 2 hours) to obtain our expected loss. Remember to tune your hyperparameters first, once you find ones that work well, let it train for longer.\n",
        "\n",
        "**Dataset**: as in the previous notebook, make sure the dataset files are in the `dataset` folder. These can be found on the Google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60AayLbxFLrH",
        "outputId": "3a433031-24e2-48a3-8104-e2bd501b93cb"
      },
      "source": [
        "with open(dataset_folder+\"summarization_dataset_preprocessed.json\", \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# We load the dataset, and split it into 2 sub-datasets based on if they are training or validation.\n",
        "# Feel free to split this dataset another way, but remember, a validation set is important, to have an idea of \n",
        "# the amount of overfitting that has occurred!\n",
        "\n",
        "d_train = [d for d in dataset if d['cut'] == 'training']\n",
        "d_valid = [d for d in dataset if d['cut'] == 'evaluation']\n",
        "\n",
        "len(d_train), len(d_valid)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(61055, 1558)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSNmSXaxFLrI",
        "outputId": "7eab076b-8301-48ff-a88c-4654f91fc87a"
      },
      "source": [
        "# An example (article, summary) pair in the training data:\n",
        "\n",
        "print(d_train[145]['story'])\n",
        "print(\"=======================\\n=======================\")\n",
        "print(d_train[145]['summary'])"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tbilisi, Georgia (CNN)Police have shot and killed a white tiger that killed a man Wednesday in Tbilisi, Georgia, a Ministry of Internal Affairs representative said, after severe flooding allowed hundreds of wild animals to escape the city zoo. \n",
            "The tiger attack happened at a warehouse in the city center. The animal had been unaccounted for since the weekend floods destroyed the zoo premises.\n",
            "The man killed, who was 43, worked in a company based in the warehouse, the Ministry of Internal Affairs said. Doctors said he was attacked in the throat and died before reaching the hospital. \n",
            "Experts are still searching the warehouse, the ministry said, adding that earlier reports that the tiger had injured a second man were unfounded. \n",
            "The zoo administration said Wednesday that another tiger was still missing. It was unable to confirm if the creature was dead or had escaped alive.\n",
            "Georgian Prime Minister Irakli Garibashvili apologized to the public, saying he had been misinformed by the zoo's management when he'd previously said there were no more dangerous animals on the run.\n",
            "City residents were urged to stay indoors for their own safety in the immediate aftermath of the floods. Volunteers have since been helping city workers with the cleanup operation.\n",
            "At least 19 people died in the flooding, according to Civil Georgia, a news website run by the nongovernmental organization United Nations Association of Georgia. Six more remained missing, it said Tuesday, citing the State Security and Crisis Management Council.\n",
            "Meanwhile, the zoo lost about half of its 600 animals, including lions, tigers, bears and wolves, in the natural disaster. \n",
            "Some animals have since been recaptured, Civil Georgia reported. Others died in the floods or have been killed by police as they scour the streets for escapees.\n",
            "Russian state news outlet RT.com  that an African penguin had made it 60 kilometers (37 miles) downriver from Tbilisi before being caught alive in a dragnet on the border with Azerbaijan. \n",
            "Video from the city showed a large crocodile being restrained by rescuers, as well as a hippopotamus standing in floodwaters, looking confused.\n",
            "The latter was eventually cornered in a city square before being tranquilized and recaptured.\n",
            "One terrified bear escaped the flood by perching on a window ledge.\n",
            "Video footage also showed devastation across swaths of the Georgian capital, where flash floods swept away roads, at least one house and many trees. The corpses of dead animals could be seen amid the wreckage.\n",
            "The problems began before midnight Saturday when heavy rainfall turned the Vere River, usually little more than a stream through the center of Tbilisi, into a raging torrent, according to Civil Georgia.\n",
            "Images on Tbilisi City Hall's Facebook page showed roads washed out, hillsides collapsed and vehicles tossed about like toys. Rescue workers carried people on their shoulders through waist-high water.\n",
            "Garibashvili extended his condolences Tuesday to the families of those killed in the flooding.\n",
            "He also proposed the creation of a park in the zoo premises to honor those lost. \"It will be a park of solidarity, a symbol of our unity, selflessness, and mutual support,\" he said in a statement on his website.\n",
            "President Georgi Margvelashvili earlier said the capital's mayoral office would help those who had lost out financially as a result of the floods.\n",
            "\"The situation is difficult, but it can be handled except for the fact that we cannot bring back those who died,\" he said.\n",
            "According to the World Wildlife Fund, as few as 3,200 tigers exist in the wild today.\n",
            "\n",
            "Journalist Eka Kadagishvili reported from Tbilisi, and Laura Smith-Spark wrote from London. CNN's Kimberly Hutcherson contributed to this report.\n",
            "=======================\n",
            "=======================\n",
            "Police have shot dead a tiger that killed a man in Tbilisi, Georgia, a government official says, after zoo animals escaped in weekend flooding.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRRN1hJfFLrJ"
      },
      "source": [
        "Similarly to the previous assignment, we create a function to get a random batch to train on, given a dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OLRb5bWFLrK"
      },
      "source": [
        "def build_batch(dataset, batch_size):\n",
        "    indices = list(np.random.randint(0, len(dataset), size=batch_size))\n",
        "    \n",
        "    batch = [dataset[i] for i in indices]\n",
        "    batch_input = np.array([a['input'] for a in batch])\n",
        "    batch_input_mask = np.array([a['input_mask'] for a in batch])\n",
        "    batch_output = np.array([a['output'] for a in batch])\n",
        "    batch_output_mask = np.array([a['output_mask'] for a in batch])\n",
        "    \n",
        "    return batch_input, batch_input_mask, batch_output, batch_output_mask"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4aVeGB-FLrL"
      },
      "source": [
        "We now instantiate the Transformer with our sets of hyperparameters specific to the task of summarization.\n",
        "In summarization, we are going to go from documents with up to 400 words, to documents with up to 100 words.\n",
        "The vocabulary size is set for you, and is of 10,000 words (we are using WordPieces, [here is a paper about subword encoding](http://aclweb.org/anthology/P18-1007), if you are interested)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61JXS8xNFLrL"
      },
      "source": [
        "# Use this trainer to train a Transformer model\n",
        "\n",
        "class TransformerTrainer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, input_length, output_length, n_layers, d_filter, dropout=0, learning_rate=1e-3):\n",
        "        super().__init__()\n",
        "        self.model = Transformer(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, d_filter=d_filter)\n",
        "\n",
        "        # Summarization loss\n",
        "        criterion = nn.CrossEntropyLoss(reduce='none')\n",
        "        self.loss_fn = lambda pred,target,mask: (criterion(pred.permute(0,2,1),target)*mask).sum()/mask.sum()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "    def forward(self,batch,optimize=True):\n",
        "        pred_logits = self.model(**batch)\n",
        "        target,mask = batch['target_sequence'],batch['decoder_mask']\n",
        "        loss = self.loss_fn(pred_logits,target,mask)\n",
        "        accuracy = (th.eq(pred_logits.argmax(dim=2,keepdim=False),target).float()*mask).sum()/mask.sum()\n",
        "        \n",
        "        if optimize:\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "                \n",
        "        return loss, accuracy"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtAVhYXXFLrN",
        "outputId": "48764c6f-ba75-4954-93c8-ba7bd411e1a1"
      },
      "source": [
        "# Dataset related parameters\n",
        "vocab_size = len(vocab)\n",
        "ilength = 400 # Length of the article\n",
        "olength  = 100 # Length of the summaries\n",
        "\n",
        "# Model related parameters, feel free to modify these.\n",
        "n_layers = 6\n",
        "d_model  = 160\n",
        "d_filter = 4*d_model\n",
        "batch_size = 16\n",
        "\n",
        "dropout = 0\n",
        "learning_rate = 1e-3\n",
        "trainer = TransformerTrainer(vocab_size, d_model, ilength, olength, n_layers, d_filter, dropout)\n",
        "model_id = 'test1'\n",
        "os.makedirs(root_folder+'models/part2/',exist_ok=True)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jEzZqG0FLrO",
        "outputId": "3b6d9468-a313-425b-9d5a-08b85e1e8737"
      },
      "source": [
        "# Skeleton code, as in the previous notebook.\n",
        "# Write code training code and save your best performing model on the\n",
        "# validation set. We will be testing the loss on a held-out test dataset.\n",
        "from tqdm import tqdm\n",
        "gc.collect()\n",
        "trainer.model.to(device)\n",
        "trainer.model.train()\n",
        "losses,accuracies = [],[]\n",
        "t = tqdm(range(int(1e3)+1))\n",
        "for i in t:\n",
        "    # Create a random mini-batch from the training dataset\n",
        "    batch = build_batch(d_train, batch_size)\n",
        "    # Build the feed-dict connecting placeholders and mini-batch\n",
        "    batch_input, batch_input_mask, batch_output, batch_output_mask = [th.tensor(tensor) for tensor in batch]\n",
        "    batch_input, batch_input_mask, batch_output, batch_output_mask \\\n",
        "                = list_to_device([batch_input, batch_input_mask, batch_output, batch_output_mask])\n",
        "    batch = {'source_sequence': batch_input, 'target_sequence': batch_output,\n",
        "            'encoder_mask': batch_input_mask, 'decoder_mask': batch_output_mask}\n",
        "    # Obtain the loss. Be careful when you use the train_op and not, as previously.\n",
        "    train_loss, accuracy = trainer(batch)\n",
        "    losses.append(train_loss.item()),accuracies.append(accuracy.item())\n",
        "    if i % 10 == 0:\n",
        "        t.set_description(f\"Iteration: {i} Loss: {np.mean(losses[-10:])} Accuracy: {np.mean(accuracies[-10:])}\")\n",
        "    if i % 100 == 0:\n",
        "        save_dict = dict(\n",
        "            kwargs = dict(\n",
        "                vocab_size=vocab_size,\n",
        "                d_model=d_model,\n",
        "                n_layers=n_layers, \n",
        "                d_filter=d_filter\n",
        "            ),\n",
        "            model_state_dict = trainer.model.state_dict(),\n",
        "            notes = \"\"\n",
        "        )\n",
        "        th.save(save_dict, root_folder+f'models/part2/model_{model_id}.pt')"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1001 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration: 0 Loss: 110.54257202148438 Accuracy: 0.0:   0%|          | 0/1001 [00:03<?, ?it/s]\u001b[A\n",
            "Iteration: 0 Loss: 110.54257202148438 Accuracy: 0.0:   0%|          | 1/1001 [00:03<1:00:05,  3.61s/it]\u001b[A\n",
            "Iteration: 0 Loss: 110.54257202148438 Accuracy: 0.0:   0%|          | 2/1001 [00:07<59:28,  3.57s/it]  \u001b[A\n",
            "Iteration: 0 Loss: 110.54257202148438 Accuracy: 0.0:   0%|          | 3/1001 [00:10<58:50,  3.54s/it]\u001b[A\n",
            "Iteration: 0 Loss: 110.54257202148438 Accuracy: 0.0:   0%|          | 4/1001 [00:14<58:41,  3.53s/it]\u001b[A\n",
            "Iteration: 0 Loss: 110.54257202148438 Accuracy: 0.0:   0%|          | 5/1001 [00:17<58:21,  3.52s/it]\u001b[A\n",
            "Iteration: 0 Loss: 110.54257202148438 Accuracy: 0.0:   1%|          | 6/1001 [00:21<58:08,  3.51s/it]\u001b[A\n",
            "Iteration: 0 Loss: 110.54257202148438 Accuracy: 0.0:   1%|          | 7/1001 [00:24<57:56,  3.50s/it]\u001b[A\n",
            "Iteration: 0 Loss: 110.54257202148438 Accuracy: 0.0:   1%|          | 8/1001 [00:27<57:43,  3.49s/it]\u001b[A\n",
            "Iteration: 0 Loss: 110.54257202148438 Accuracy: 0.0:   1%|          | 9/1001 [00:31<57:33,  3.48s/it]\u001b[A\n",
            "Iteration: 0 Loss: 110.54257202148438 Accuracy: 0.0:   1%|          | 10/1001 [00:34<57:25,  3.48s/it]\u001b[A\n",
            "Iteration: 10 Loss: 54.912686347961426 Accuracy: 0.0871725739329122:   1%|          | 10/1001 [00:38<57:25,  3.48s/it]\u001b[A\n",
            "Iteration: 10 Loss: 54.912686347961426 Accuracy: 0.0871725739329122:   1%|          | 11/1001 [00:38<57:19,  3.47s/it]\u001b[A\n",
            "Iteration: 10 Loss: 54.912686347961426 Accuracy: 0.0871725739329122:   1%|          | 12/1001 [00:41<57:12,  3.47s/it]\u001b[A\n",
            "Iteration: 10 Loss: 54.912686347961426 Accuracy: 0.0871725739329122:   1%|▏         | 13/1001 [00:45<57:14,  3.48s/it]\u001b[A\n",
            "Iteration: 10 Loss: 54.912686347961426 Accuracy: 0.0871725739329122:   1%|▏         | 14/1001 [00:48<57:06,  3.47s/it]\u001b[A\n",
            "Iteration: 10 Loss: 54.912686347961426 Accuracy: 0.0871725739329122:   1%|▏         | 15/1001 [00:52<56:58,  3.47s/it]\u001b[A\n",
            "Iteration: 10 Loss: 54.912686347961426 Accuracy: 0.0871725739329122:   2%|▏         | 16/1001 [00:55<56:52,  3.46s/it]\u001b[A\n",
            "Iteration: 10 Loss: 54.912686347961426 Accuracy: 0.0871725739329122:   2%|▏         | 17/1001 [00:59<56:51,  3.47s/it]\u001b[A\n",
            "Iteration: 10 Loss: 54.912686347961426 Accuracy: 0.0871725739329122:   2%|▏         | 18/1001 [01:02<56:42,  3.46s/it]\u001b[A\n",
            "Iteration: 10 Loss: 54.912686347961426 Accuracy: 0.0871725739329122:   2%|▏         | 19/1001 [01:06<56:42,  3.46s/it]\u001b[A\n",
            "Iteration: 10 Loss: 54.912686347961426 Accuracy: 0.0871725739329122:   2%|▏         | 20/1001 [01:09<56:38,  3.46s/it]\u001b[A\n",
            "Iteration: 20 Loss: 25.481699180603027 Accuracy: 0.12024556249380111:   2%|▏         | 20/1001 [01:13<56:38,  3.46s/it]\u001b[A\n",
            "Iteration: 20 Loss: 25.481699180603027 Accuracy: 0.12024556249380111:   2%|▏         | 21/1001 [01:13<56:37,  3.47s/it]\u001b[A\n",
            "Iteration: 20 Loss: 25.481699180603027 Accuracy: 0.12024556249380111:   2%|▏         | 22/1001 [01:16<56:38,  3.47s/it]\u001b[A\n",
            "Iteration: 20 Loss: 25.481699180603027 Accuracy: 0.12024556249380111:   2%|▏         | 23/1001 [01:20<56:49,  3.49s/it]\u001b[A\n",
            "Iteration: 20 Loss: 25.481699180603027 Accuracy: 0.12024556249380111:   2%|▏         | 24/1001 [01:23<56:40,  3.48s/it]\u001b[A\n",
            "Iteration: 20 Loss: 25.481699180603027 Accuracy: 0.12024556249380111:   2%|▏         | 25/1001 [01:26<56:34,  3.48s/it]\u001b[A\n",
            "Iteration: 20 Loss: 25.481699180603027 Accuracy: 0.12024556249380111:   3%|▎         | 26/1001 [01:30<56:26,  3.47s/it]\u001b[A\n",
            "Iteration: 20 Loss: 25.481699180603027 Accuracy: 0.12024556249380111:   3%|▎         | 27/1001 [01:33<56:42,  3.49s/it]\u001b[A\n",
            "Iteration: 20 Loss: 25.481699180603027 Accuracy: 0.12024556249380111:   3%|▎         | 28/1001 [01:37<56:29,  3.48s/it]\u001b[A\n",
            "Iteration: 20 Loss: 25.481699180603027 Accuracy: 0.12024556249380111:   3%|▎         | 29/1001 [01:40<56:24,  3.48s/it]\u001b[A\n",
            "Iteration: 20 Loss: 25.481699180603027 Accuracy: 0.12024556249380111:   3%|▎         | 30/1001 [01:44<56:20,  3.48s/it]\u001b[A\n",
            "Iteration: 30 Loss: 19.3405650138855 Accuracy: 0.12978971265256406:   3%|▎         | 30/1001 [01:47<56:20,  3.48s/it]  \u001b[A\n",
            "Iteration: 30 Loss: 19.3405650138855 Accuracy: 0.12978971265256406:   3%|▎         | 31/1001 [01:47<56:17,  3.48s/it]\u001b[A\n",
            "Iteration: 30 Loss: 19.3405650138855 Accuracy: 0.12978971265256406:   3%|▎         | 32/1001 [01:51<56:16,  3.48s/it]\u001b[A\n",
            "Iteration: 30 Loss: 19.3405650138855 Accuracy: 0.12978971265256406:   3%|▎         | 33/1001 [01:54<56:12,  3.48s/it]\u001b[A\n",
            "Iteration: 30 Loss: 19.3405650138855 Accuracy: 0.12978971265256406:   3%|▎         | 34/1001 [01:58<56:03,  3.48s/it]\u001b[A\n",
            "Iteration: 30 Loss: 19.3405650138855 Accuracy: 0.12978971265256406:   3%|▎         | 35/1001 [02:01<55:58,  3.48s/it]\u001b[A\n",
            "Iteration: 30 Loss: 19.3405650138855 Accuracy: 0.12978971265256406:   4%|▎         | 36/1001 [02:05<55:51,  3.47s/it]\u001b[A\n",
            "Iteration: 30 Loss: 19.3405650138855 Accuracy: 0.12978971265256406:   4%|▎         | 37/1001 [02:08<55:49,  3.47s/it]\u001b[A\n",
            "Iteration: 30 Loss: 19.3405650138855 Accuracy: 0.12978971265256406:   4%|▍         | 38/1001 [02:12<55:41,  3.47s/it]\u001b[A\n",
            "Iteration: 30 Loss: 19.3405650138855 Accuracy: 0.12978971265256406:   4%|▍         | 39/1001 [02:15<55:35,  3.47s/it]\u001b[A\n",
            "Iteration: 30 Loss: 19.3405650138855 Accuracy: 0.12978971265256406:   4%|▍         | 40/1001 [02:19<55:37,  3.47s/it]\u001b[A\n",
            "Iteration: 40 Loss: 13.374247932434082 Accuracy: 0.13709171935915948:   4%|▍         | 40/1001 [02:22<55:37,  3.47s/it]\u001b[A\n",
            "Iteration: 40 Loss: 13.374247932434082 Accuracy: 0.13709171935915948:   4%|▍         | 41/1001 [02:22<55:32,  3.47s/it]\u001b[A\n",
            "Iteration: 40 Loss: 13.374247932434082 Accuracy: 0.13709171935915948:   4%|▍         | 42/1001 [02:26<55:42,  3.49s/it]\u001b[A\n",
            "Iteration: 40 Loss: 13.374247932434082 Accuracy: 0.13709171935915948:   4%|▍         | 43/1001 [02:29<55:31,  3.48s/it]\u001b[A\n",
            "Iteration: 40 Loss: 13.374247932434082 Accuracy: 0.13709171935915948:   4%|▍         | 44/1001 [02:33<55:28,  3.48s/it]\u001b[A\n",
            "Iteration: 40 Loss: 13.374247932434082 Accuracy: 0.13709171935915948:   4%|▍         | 45/1001 [02:36<55:17,  3.47s/it]\u001b[A\n",
            "Iteration: 40 Loss: 13.374247932434082 Accuracy: 0.13709171935915948:   5%|▍         | 46/1001 [02:40<55:17,  3.47s/it]\u001b[A\n",
            "Iteration: 40 Loss: 13.374247932434082 Accuracy: 0.13709171935915948:   5%|▍         | 47/1001 [02:43<55:13,  3.47s/it]\u001b[A\n",
            "Iteration: 40 Loss: 13.374247932434082 Accuracy: 0.13709171935915948:   5%|▍         | 48/1001 [02:46<55:08,  3.47s/it]\u001b[A\n",
            "Iteration: 40 Loss: 13.374247932434082 Accuracy: 0.13709171935915948:   5%|▍         | 49/1001 [02:50<55:01,  3.47s/it]\u001b[A\n",
            "Iteration: 40 Loss: 13.374247932434082 Accuracy: 0.13709171935915948:   5%|▍         | 50/1001 [02:53<55:05,  3.48s/it]\u001b[A\n",
            "Iteration: 50 Loss: 10.363817691802979 Accuracy: 0.13131313025951385:   5%|▍         | 50/1001 [02:57<55:05,  3.48s/it]\u001b[A\n",
            "Iteration: 50 Loss: 10.363817691802979 Accuracy: 0.13131313025951385:   5%|▌         | 51/1001 [02:57<55:10,  3.48s/it]\u001b[A\n",
            "Iteration: 50 Loss: 10.363817691802979 Accuracy: 0.13131313025951385:   5%|▌         | 52/1001 [03:00<55:05,  3.48s/it]\u001b[A\n",
            "Iteration: 50 Loss: 10.363817691802979 Accuracy: 0.13131313025951385:   5%|▌         | 53/1001 [03:04<55:00,  3.48s/it]\u001b[A\n",
            "Iteration: 50 Loss: 10.363817691802979 Accuracy: 0.13131313025951385:   5%|▌         | 54/1001 [03:07<54:56,  3.48s/it]\u001b[A\n",
            "Iteration: 50 Loss: 10.363817691802979 Accuracy: 0.13131313025951385:   5%|▌         | 55/1001 [03:11<54:54,  3.48s/it]\u001b[A\n",
            "Iteration: 50 Loss: 10.363817691802979 Accuracy: 0.13131313025951385:   6%|▌         | 56/1001 [03:14<54:48,  3.48s/it]\u001b[A\n",
            "Iteration: 50 Loss: 10.363817691802979 Accuracy: 0.13131313025951385:   6%|▌         | 57/1001 [03:18<54:41,  3.48s/it]\u001b[A\n",
            "Iteration: 50 Loss: 10.363817691802979 Accuracy: 0.13131313025951385:   6%|▌         | 58/1001 [03:21<54:38,  3.48s/it]\u001b[A\n",
            "Iteration: 50 Loss: 10.363817691802979 Accuracy: 0.13131313025951385:   6%|▌         | 59/1001 [03:25<54:39,  3.48s/it]\u001b[A\n",
            "Iteration: 50 Loss: 10.363817691802979 Accuracy: 0.13131313025951385:   6%|▌         | 60/1001 [03:28<54:37,  3.48s/it]\u001b[A\n",
            "Iteration: 60 Loss: 9.014957332611084 Accuracy: 0.13214858174324035:   6%|▌         | 60/1001 [03:32<54:37,  3.48s/it] \u001b[A\n",
            "Iteration: 60 Loss: 9.014957332611084 Accuracy: 0.13214858174324035:   6%|▌         | 61/1001 [03:32<54:43,  3.49s/it]\u001b[A\n",
            "Iteration: 60 Loss: 9.014957332611084 Accuracy: 0.13214858174324035:   6%|▌         | 62/1001 [03:35<54:34,  3.49s/it]\u001b[A\n",
            "Iteration: 60 Loss: 9.014957332611084 Accuracy: 0.13214858174324035:   6%|▋         | 63/1001 [03:39<54:25,  3.48s/it]\u001b[A\n",
            "Iteration: 60 Loss: 9.014957332611084 Accuracy: 0.13214858174324035:   6%|▋         | 64/1001 [03:42<54:13,  3.47s/it]\u001b[A\n",
            "Iteration: 60 Loss: 9.014957332611084 Accuracy: 0.13214858174324035:   6%|▋         | 65/1001 [03:46<54:08,  3.47s/it]\u001b[A\n",
            "Iteration: 60 Loss: 9.014957332611084 Accuracy: 0.13214858174324035:   7%|▋         | 66/1001 [03:49<54:06,  3.47s/it]\u001b[A\n",
            "Iteration: 60 Loss: 9.014957332611084 Accuracy: 0.13214858174324035:   7%|▋         | 67/1001 [03:53<54:02,  3.47s/it]\u001b[A\n",
            "Iteration: 60 Loss: 9.014957332611084 Accuracy: 0.13214858174324035:   7%|▋         | 68/1001 [03:56<53:57,  3.47s/it]\u001b[A\n",
            "Iteration: 60 Loss: 9.014957332611084 Accuracy: 0.13214858174324035:   7%|▋         | 69/1001 [03:59<53:57,  3.47s/it]\u001b[A\n",
            "Iteration: 60 Loss: 9.014957332611084 Accuracy: 0.13214858174324035:   7%|▋         | 70/1001 [04:03<53:55,  3.48s/it]\u001b[A\n",
            "Iteration: 70 Loss: 7.7687116146087645 Accuracy: 0.14471695497632026:   7%|▋         | 70/1001 [04:06<53:55,  3.48s/it]\u001b[A\n",
            "Iteration: 70 Loss: 7.7687116146087645 Accuracy: 0.14471695497632026:   7%|▋         | 71/1001 [04:06<53:53,  3.48s/it]\u001b[A\n",
            "Iteration: 70 Loss: 7.7687116146087645 Accuracy: 0.14471695497632026:   7%|▋         | 72/1001 [04:10<53:45,  3.47s/it]\u001b[A\n",
            "Iteration: 70 Loss: 7.7687116146087645 Accuracy: 0.14471695497632026:   7%|▋         | 73/1001 [04:13<53:43,  3.47s/it]\u001b[A\n",
            "Iteration: 70 Loss: 7.7687116146087645 Accuracy: 0.14471695497632026:   7%|▋         | 74/1001 [04:17<53:39,  3.47s/it]\u001b[A\n",
            "Iteration: 70 Loss: 7.7687116146087645 Accuracy: 0.14471695497632026:   7%|▋         | 75/1001 [04:20<53:37,  3.47s/it]\u001b[A\n",
            "Iteration: 70 Loss: 7.7687116146087645 Accuracy: 0.14471695497632026:   8%|▊         | 76/1001 [04:24<53:38,  3.48s/it]\u001b[A\n",
            "Iteration: 70 Loss: 7.7687116146087645 Accuracy: 0.14471695497632026:   8%|▊         | 77/1001 [04:27<53:36,  3.48s/it]\u001b[A\n",
            "Iteration: 70 Loss: 7.7687116146087645 Accuracy: 0.14471695497632026:   8%|▊         | 78/1001 [04:31<53:30,  3.48s/it]\u001b[A\n",
            "Iteration: 70 Loss: 7.7687116146087645 Accuracy: 0.14471695497632026:   8%|▊         | 79/1001 [04:34<53:25,  3.48s/it]\u001b[A\n",
            "Iteration: 70 Loss: 7.7687116146087645 Accuracy: 0.14471695497632026:   8%|▊         | 80/1001 [04:38<53:22,  3.48s/it]\u001b[A\n",
            "Iteration: 80 Loss: 7.83791036605835 Accuracy: 0.15110958218574524:   8%|▊         | 80/1001 [04:41<53:22,  3.48s/it]  \u001b[A\n",
            "Iteration: 80 Loss: 7.83791036605835 Accuracy: 0.15110958218574524:   8%|▊         | 81/1001 [04:41<53:17,  3.48s/it]\u001b[A\n",
            "Iteration: 80 Loss: 7.83791036605835 Accuracy: 0.15110958218574524:   8%|▊         | 82/1001 [04:45<53:08,  3.47s/it]\u001b[A\n",
            "Iteration: 80 Loss: 7.83791036605835 Accuracy: 0.15110958218574524:   8%|▊         | 83/1001 [04:48<53:07,  3.47s/it]\u001b[A\n",
            "Iteration: 80 Loss: 7.83791036605835 Accuracy: 0.15110958218574524:   8%|▊         | 84/1001 [04:52<53:01,  3.47s/it]\u001b[A\n",
            "Iteration: 80 Loss: 7.83791036605835 Accuracy: 0.15110958218574524:   8%|▊         | 85/1001 [04:55<52:57,  3.47s/it]\u001b[A\n",
            "Iteration: 80 Loss: 7.83791036605835 Accuracy: 0.15110958218574524:   9%|▊         | 86/1001 [04:59<52:53,  3.47s/it]\u001b[A\n",
            "Iteration: 80 Loss: 7.83791036605835 Accuracy: 0.15110958218574524:   9%|▊         | 87/1001 [05:02<52:52,  3.47s/it]\u001b[A\n",
            "Iteration: 80 Loss: 7.83791036605835 Accuracy: 0.15110958218574524:   9%|▉         | 88/1001 [05:05<52:47,  3.47s/it]\u001b[A\n",
            "Iteration: 80 Loss: 7.83791036605835 Accuracy: 0.15110958218574524:   9%|▉         | 89/1001 [05:09<52:49,  3.48s/it]\u001b[A\n",
            "Iteration: 80 Loss: 7.83791036605835 Accuracy: 0.15110958218574524:   9%|▉         | 90/1001 [05:12<52:45,  3.47s/it]\u001b[A\n",
            "Iteration: 90 Loss: 7.0975117683410645 Accuracy: 0.15665351003408431:   9%|▉         | 90/1001 [05:16<52:45,  3.47s/it]\u001b[A\n",
            "Iteration: 90 Loss: 7.0975117683410645 Accuracy: 0.15665351003408431:   9%|▉         | 91/1001 [05:16<52:41,  3.47s/it]\u001b[A\n",
            "Iteration: 90 Loss: 7.0975117683410645 Accuracy: 0.15665351003408431:   9%|▉         | 92/1001 [05:19<52:38,  3.47s/it]\u001b[A\n",
            "Iteration: 90 Loss: 7.0975117683410645 Accuracy: 0.15665351003408431:   9%|▉         | 93/1001 [05:23<52:35,  3.47s/it]\u001b[A\n",
            "Iteration: 90 Loss: 7.0975117683410645 Accuracy: 0.15665351003408431:   9%|▉         | 94/1001 [05:26<52:30,  3.47s/it]\u001b[A\n",
            "Iteration: 90 Loss: 7.0975117683410645 Accuracy: 0.15665351003408431:   9%|▉         | 95/1001 [05:30<52:26,  3.47s/it]\u001b[A\n",
            "Iteration: 90 Loss: 7.0975117683410645 Accuracy: 0.15665351003408431:  10%|▉         | 96/1001 [05:33<52:22,  3.47s/it]\u001b[A\n",
            "Iteration: 90 Loss: 7.0975117683410645 Accuracy: 0.15665351003408431:  10%|▉         | 97/1001 [05:37<52:19,  3.47s/it]\u001b[A\n",
            "Iteration: 90 Loss: 7.0975117683410645 Accuracy: 0.15665351003408431:  10%|▉         | 98/1001 [05:40<52:12,  3.47s/it]\u001b[A\n",
            "Iteration: 90 Loss: 7.0975117683410645 Accuracy: 0.15665351003408431:  10%|▉         | 99/1001 [05:44<52:18,  3.48s/it]\u001b[A\n",
            "Iteration: 90 Loss: 7.0975117683410645 Accuracy: 0.15665351003408431:  10%|▉         | 100/1001 [05:47<52:12,  3.48s/it]\u001b[A\n",
            "Iteration: 100 Loss: 6.381995773315429 Accuracy: 0.15067970231175423:  10%|▉         | 100/1001 [05:51<52:12,  3.48s/it]\u001b[A\n",
            "Iteration: 100 Loss: 6.381995773315429 Accuracy: 0.15067970231175423:  10%|█         | 101/1001 [05:51<52:41,  3.51s/it]\u001b[A\n",
            "Iteration: 100 Loss: 6.381995773315429 Accuracy: 0.15067970231175423:  10%|█         | 102/1001 [05:54<52:28,  3.50s/it]\u001b[A\n",
            "Iteration: 100 Loss: 6.381995773315429 Accuracy: 0.15067970231175423:  10%|█         | 103/1001 [05:58<52:16,  3.49s/it]\u001b[A\n",
            "Iteration: 100 Loss: 6.381995773315429 Accuracy: 0.15067970231175423:  10%|█         | 104/1001 [06:01<52:04,  3.48s/it]\u001b[A\n",
            "Iteration: 100 Loss: 6.381995773315429 Accuracy: 0.15067970231175423:  10%|█         | 105/1001 [06:05<51:57,  3.48s/it]\u001b[A\n",
            "Iteration: 100 Loss: 6.381995773315429 Accuracy: 0.15067970231175423:  11%|█         | 106/1001 [06:08<51:48,  3.47s/it]\u001b[A\n",
            "Iteration: 100 Loss: 6.381995773315429 Accuracy: 0.15067970231175423:  11%|█         | 107/1001 [06:12<51:45,  3.47s/it]\u001b[A\n",
            "Iteration: 100 Loss: 6.381995773315429 Accuracy: 0.15067970231175423:  11%|█         | 108/1001 [06:15<51:49,  3.48s/it]\u001b[A\n",
            "Iteration: 100 Loss: 6.381995773315429 Accuracy: 0.15067970231175423:  11%|█         | 109/1001 [06:19<51:43,  3.48s/it]\u001b[A\n",
            "Iteration: 100 Loss: 6.381995773315429 Accuracy: 0.15067970231175423:  11%|█         | 110/1001 [06:22<51:36,  3.48s/it]\u001b[A\n",
            "Iteration: 110 Loss: 6.472989177703857 Accuracy: 0.1590665251016617:  11%|█         | 110/1001 [06:26<51:36,  3.48s/it] \u001b[A\n",
            "Iteration: 110 Loss: 6.472989177703857 Accuracy: 0.1590665251016617:  11%|█         | 111/1001 [06:26<51:36,  3.48s/it]\u001b[A\n",
            "Iteration: 110 Loss: 6.472989177703857 Accuracy: 0.1590665251016617:  11%|█         | 112/1001 [06:29<51:27,  3.47s/it]\u001b[A\n",
            "Iteration: 110 Loss: 6.472989177703857 Accuracy: 0.1590665251016617:  11%|█▏        | 113/1001 [06:32<51:26,  3.48s/it]\u001b[A\n",
            "Iteration: 110 Loss: 6.472989177703857 Accuracy: 0.1590665251016617:  11%|█▏        | 114/1001 [06:36<51:18,  3.47s/it]\u001b[A\n",
            "Iteration: 110 Loss: 6.472989177703857 Accuracy: 0.1590665251016617:  11%|█▏        | 115/1001 [06:39<51:15,  3.47s/it]\u001b[A\n",
            "Iteration: 110 Loss: 6.472989177703857 Accuracy: 0.1590665251016617:  12%|█▏        | 116/1001 [06:43<51:09,  3.47s/it]\u001b[A\n",
            "Iteration: 110 Loss: 6.472989177703857 Accuracy: 0.1590665251016617:  12%|█▏        | 117/1001 [06:46<51:02,  3.46s/it]\u001b[A\n",
            "Iteration: 110 Loss: 6.472989177703857 Accuracy: 0.1590665251016617:  12%|█▏        | 118/1001 [06:50<51:04,  3.47s/it]\u001b[A\n",
            "Iteration: 110 Loss: 6.472989177703857 Accuracy: 0.1590665251016617:  12%|█▏        | 119/1001 [06:53<50:57,  3.47s/it]\u001b[A\n",
            "Iteration: 110 Loss: 6.472989177703857 Accuracy: 0.1590665251016617:  12%|█▏        | 120/1001 [06:57<50:58,  3.47s/it]\u001b[A\n",
            "Iteration: 120 Loss: 5.055814456939697 Accuracy: 0.17709144502878188:  12%|█▏        | 120/1001 [07:00<50:58,  3.47s/it]\u001b[A\n",
            "Iteration: 120 Loss: 5.055814456939697 Accuracy: 0.17709144502878188:  12%|█▏        | 121/1001 [07:00<50:58,  3.48s/it]\u001b[A\n",
            "Iteration: 120 Loss: 5.055814456939697 Accuracy: 0.17709144502878188:  12%|█▏        | 122/1001 [07:04<50:53,  3.47s/it]\u001b[A\n",
            "Iteration: 120 Loss: 5.055814456939697 Accuracy: 0.17709144502878188:  12%|█▏        | 123/1001 [07:07<50:51,  3.48s/it]\u001b[A\n",
            "Iteration: 120 Loss: 5.055814456939697 Accuracy: 0.17709144502878188:  12%|█▏        | 124/1001 [07:11<50:51,  3.48s/it]\u001b[A\n",
            "Iteration: 120 Loss: 5.055814456939697 Accuracy: 0.17709144502878188:  12%|█▏        | 125/1001 [07:14<50:50,  3.48s/it]\u001b[A\n",
            "Iteration: 120 Loss: 5.055814456939697 Accuracy: 0.17709144502878188:  13%|█▎        | 126/1001 [07:18<50:43,  3.48s/it]\u001b[A\n",
            "Iteration: 120 Loss: 5.055814456939697 Accuracy: 0.17709144502878188:  13%|█▎        | 127/1001 [07:21<50:42,  3.48s/it]\u001b[A\n",
            "Iteration: 120 Loss: 5.055814456939697 Accuracy: 0.17709144502878188:  13%|█▎        | 128/1001 [07:25<50:46,  3.49s/it]\u001b[A\n",
            "Iteration: 120 Loss: 5.055814456939697 Accuracy: 0.17709144502878188:  13%|█▎        | 129/1001 [07:28<50:35,  3.48s/it]\u001b[A\n",
            "Iteration: 120 Loss: 5.055814456939697 Accuracy: 0.17709144502878188:  13%|█▎        | 130/1001 [07:32<50:28,  3.48s/it]\u001b[A\n",
            "Iteration: 130 Loss: 5.2792640209198 Accuracy: 0.18654869347810746:  13%|█▎        | 130/1001 [07:35<50:28,  3.48s/it]  \u001b[A\n",
            "Iteration: 130 Loss: 5.2792640209198 Accuracy: 0.18654869347810746:  13%|█▎        | 131/1001 [07:35<50:24,  3.48s/it]\u001b[A\n",
            "Iteration: 130 Loss: 5.2792640209198 Accuracy: 0.18654869347810746:  13%|█▎        | 132/1001 [07:39<50:19,  3.48s/it]\u001b[A\n",
            "Iteration: 130 Loss: 5.2792640209198 Accuracy: 0.18654869347810746:  13%|█▎        | 133/1001 [07:42<50:09,  3.47s/it]\u001b[A\n",
            "Iteration: 130 Loss: 5.2792640209198 Accuracy: 0.18654869347810746:  13%|█▎        | 134/1001 [07:45<50:08,  3.47s/it]\u001b[A\n",
            "Iteration: 130 Loss: 5.2792640209198 Accuracy: 0.18654869347810746:  13%|█▎        | 135/1001 [07:49<50:04,  3.47s/it]\u001b[A\n",
            "Iteration: 130 Loss: 5.2792640209198 Accuracy: 0.18654869347810746:  14%|█▎        | 136/1001 [07:52<50:02,  3.47s/it]\u001b[A\n",
            "Iteration: 130 Loss: 5.2792640209198 Accuracy: 0.18654869347810746:  14%|█▎        | 137/1001 [07:56<50:02,  3.48s/it]\u001b[A\n",
            "Iteration: 130 Loss: 5.2792640209198 Accuracy: 0.18654869347810746:  14%|█▍        | 138/1001 [07:59<49:57,  3.47s/it]\u001b[A\n",
            "Iteration: 130 Loss: 5.2792640209198 Accuracy: 0.18654869347810746:  14%|█▍        | 139/1001 [08:03<49:55,  3.48s/it]\u001b[A\n",
            "Iteration: 130 Loss: 5.2792640209198 Accuracy: 0.18654869347810746:  14%|█▍        | 140/1001 [08:06<49:56,  3.48s/it]\u001b[A\n",
            "Iteration: 140 Loss: 4.770091032981872 Accuracy: 0.19603166580200196:  14%|█▍        | 140/1001 [08:10<49:56,  3.48s/it]\u001b[A\n",
            "Iteration: 140 Loss: 4.770091032981872 Accuracy: 0.19603166580200196:  14%|█▍        | 141/1001 [08:10<49:50,  3.48s/it]\u001b[A\n",
            "Iteration: 140 Loss: 4.770091032981872 Accuracy: 0.19603166580200196:  14%|█▍        | 142/1001 [08:13<49:47,  3.48s/it]\u001b[A\n",
            "Iteration: 140 Loss: 4.770091032981872 Accuracy: 0.19603166580200196:  14%|█▍        | 143/1001 [08:17<49:45,  3.48s/it]\u001b[A\n",
            "Iteration: 140 Loss: 4.770091032981872 Accuracy: 0.19603166580200196:  14%|█▍        | 144/1001 [08:20<49:40,  3.48s/it]\u001b[A\n",
            "Iteration: 140 Loss: 4.770091032981872 Accuracy: 0.19603166580200196:  14%|█▍        | 145/1001 [08:24<49:40,  3.48s/it]\u001b[A\n",
            "Iteration: 140 Loss: 4.770091032981872 Accuracy: 0.19603166580200196:  15%|█▍        | 146/1001 [08:27<49:40,  3.49s/it]\u001b[A\n",
            "Iteration: 140 Loss: 4.770091032981872 Accuracy: 0.19603166580200196:  15%|█▍        | 147/1001 [08:31<49:35,  3.48s/it]\u001b[A\n",
            "Iteration: 140 Loss: 4.770091032981872 Accuracy: 0.19603166580200196:  15%|█▍        | 148/1001 [08:34<49:28,  3.48s/it]\u001b[A\n",
            "Iteration: 140 Loss: 4.770091032981872 Accuracy: 0.19603166580200196:  15%|█▍        | 149/1001 [08:38<49:22,  3.48s/it]\u001b[A\n",
            "Iteration: 140 Loss: 4.770091032981872 Accuracy: 0.19603166580200196:  15%|█▍        | 150/1001 [08:41<49:16,  3.47s/it]\u001b[A\n",
            "Iteration: 150 Loss: 4.868417024612427 Accuracy: 0.1926603466272354:  15%|█▍        | 150/1001 [08:45<49:16,  3.47s/it] \u001b[A\n",
            "Iteration: 150 Loss: 4.868417024612427 Accuracy: 0.1926603466272354:  15%|█▌        | 151/1001 [08:45<49:16,  3.48s/it]\u001b[A\n",
            "Iteration: 150 Loss: 4.868417024612427 Accuracy: 0.1926603466272354:  15%|█▌        | 152/1001 [08:48<49:06,  3.47s/it]\u001b[A\n",
            "Iteration: 150 Loss: 4.868417024612427 Accuracy: 0.1926603466272354:  15%|█▌        | 153/1001 [08:52<49:06,  3.47s/it]\u001b[A\n",
            "Iteration: 150 Loss: 4.868417024612427 Accuracy: 0.1926603466272354:  15%|█▌        | 154/1001 [08:55<49:01,  3.47s/it]\u001b[A\n",
            "Iteration: 150 Loss: 4.868417024612427 Accuracy: 0.1926603466272354:  15%|█▌        | 155/1001 [08:58<49:01,  3.48s/it]\u001b[A\n",
            "Iteration: 150 Loss: 4.868417024612427 Accuracy: 0.1926603466272354:  16%|█▌        | 156/1001 [09:02<48:57,  3.48s/it]\u001b[A\n",
            "Iteration: 150 Loss: 4.868417024612427 Accuracy: 0.1926603466272354:  16%|█▌        | 157/1001 [09:05<49:01,  3.48s/it]\u001b[A\n",
            "Iteration: 150 Loss: 4.868417024612427 Accuracy: 0.1926603466272354:  16%|█▌        | 158/1001 [09:09<48:52,  3.48s/it]\u001b[A\n",
            "Iteration: 150 Loss: 4.868417024612427 Accuracy: 0.1926603466272354:  16%|█▌        | 159/1001 [09:12<48:48,  3.48s/it]\u001b[A\n",
            "Iteration: 150 Loss: 4.868417024612427 Accuracy: 0.1926603466272354:  16%|█▌        | 160/1001 [09:16<48:41,  3.47s/it]\u001b[A\n",
            "Iteration: 160 Loss: 4.851880836486816 Accuracy: 0.2036236271262169:  16%|█▌        | 160/1001 [09:19<48:41,  3.47s/it]\u001b[A\n",
            "Iteration: 160 Loss: 4.851880836486816 Accuracy: 0.2036236271262169:  16%|█▌        | 161/1001 [09:19<48:39,  3.48s/it]\u001b[A\n",
            "Iteration: 160 Loss: 4.851880836486816 Accuracy: 0.2036236271262169:  16%|█▌        | 162/1001 [09:23<48:34,  3.47s/it]\u001b[A\n",
            "Iteration: 160 Loss: 4.851880836486816 Accuracy: 0.2036236271262169:  16%|█▋        | 163/1001 [09:26<48:30,  3.47s/it]\u001b[A\n",
            "Iteration: 160 Loss: 4.851880836486816 Accuracy: 0.2036236271262169:  16%|█▋        | 164/1001 [09:30<48:28,  3.48s/it]\u001b[A\n",
            "Iteration: 160 Loss: 4.851880836486816 Accuracy: 0.2036236271262169:  16%|█▋        | 165/1001 [09:33<48:30,  3.48s/it]\u001b[A\n",
            "Iteration: 160 Loss: 4.851880836486816 Accuracy: 0.2036236271262169:  17%|█▋        | 166/1001 [09:37<48:28,  3.48s/it]\u001b[A\n",
            "Iteration: 160 Loss: 4.851880836486816 Accuracy: 0.2036236271262169:  17%|█▋        | 167/1001 [09:40<48:19,  3.48s/it]\u001b[A\n",
            "Iteration: 160 Loss: 4.851880836486816 Accuracy: 0.2036236271262169:  17%|█▋        | 168/1001 [09:44<48:26,  3.49s/it]\u001b[A\n",
            "Iteration: 160 Loss: 4.851880836486816 Accuracy: 0.2036236271262169:  17%|█▋        | 169/1001 [09:47<48:16,  3.48s/it]\u001b[A\n",
            "Iteration: 160 Loss: 4.851880836486816 Accuracy: 0.2036236271262169:  17%|█▋        | 170/1001 [09:51<48:14,  3.48s/it]\u001b[A\n",
            "Iteration: 170 Loss: 4.523522138595581 Accuracy: 0.20676923245191575:  17%|█▋        | 170/1001 [09:54<48:14,  3.48s/it]\u001b[A\n",
            "Iteration: 170 Loss: 4.523522138595581 Accuracy: 0.20676923245191575:  17%|█▋        | 171/1001 [09:54<48:12,  3.49s/it]\u001b[A\n",
            "Iteration: 170 Loss: 4.523522138595581 Accuracy: 0.20676923245191575:  17%|█▋        | 172/1001 [09:58<48:07,  3.48s/it]\u001b[A\n",
            "Iteration: 170 Loss: 4.523522138595581 Accuracy: 0.20676923245191575:  17%|█▋        | 173/1001 [10:01<48:01,  3.48s/it]\u001b[A\n",
            "Iteration: 170 Loss: 4.523522138595581 Accuracy: 0.20676923245191575:  17%|█▋        | 174/1001 [10:05<47:55,  3.48s/it]\u001b[A\n",
            "Iteration: 170 Loss: 4.523522138595581 Accuracy: 0.20676923245191575:  17%|█▋        | 175/1001 [10:08<47:56,  3.48s/it]\u001b[A\n",
            "Iteration: 170 Loss: 4.523522138595581 Accuracy: 0.20676923245191575:  18%|█▊        | 176/1001 [10:12<47:50,  3.48s/it]\u001b[A\n",
            "Iteration: 170 Loss: 4.523522138595581 Accuracy: 0.20676923245191575:  18%|█▊        | 177/1001 [10:15<47:45,  3.48s/it]\u001b[A\n",
            "Iteration: 170 Loss: 4.523522138595581 Accuracy: 0.20676923245191575:  18%|█▊        | 178/1001 [10:18<47:43,  3.48s/it]\u001b[A\n",
            "Iteration: 170 Loss: 4.523522138595581 Accuracy: 0.20676923245191575:  18%|█▊        | 179/1001 [10:22<47:36,  3.48s/it]\u001b[A\n",
            "Iteration: 170 Loss: 4.523522138595581 Accuracy: 0.20676923245191575:  18%|█▊        | 180/1001 [10:25<47:40,  3.48s/it]\u001b[A\n",
            "Iteration: 180 Loss: 5.049162483215332 Accuracy: 0.1985994130373001:  18%|█▊        | 180/1001 [10:29<47:40,  3.48s/it] \u001b[A\n",
            "Iteration: 180 Loss: 5.049162483215332 Accuracy: 0.1985994130373001:  18%|█▊        | 181/1001 [10:29<47:34,  3.48s/it]\u001b[A\n",
            "Iteration: 180 Loss: 5.049162483215332 Accuracy: 0.1985994130373001:  18%|█▊        | 182/1001 [10:32<47:30,  3.48s/it]\u001b[A\n",
            "Iteration: 180 Loss: 5.049162483215332 Accuracy: 0.1985994130373001:  18%|█▊        | 183/1001 [10:36<47:19,  3.47s/it]\u001b[A\n",
            "Iteration: 180 Loss: 5.049162483215332 Accuracy: 0.1985994130373001:  18%|█▊        | 184/1001 [10:39<47:18,  3.47s/it]\u001b[A\n",
            "Iteration: 180 Loss: 5.049162483215332 Accuracy: 0.1985994130373001:  18%|█▊        | 185/1001 [10:43<47:13,  3.47s/it]\u001b[A\n",
            "Iteration: 180 Loss: 5.049162483215332 Accuracy: 0.1985994130373001:  19%|█▊        | 186/1001 [10:46<47:07,  3.47s/it]\u001b[A\n",
            "Iteration: 180 Loss: 5.049162483215332 Accuracy: 0.1985994130373001:  19%|█▊        | 187/1001 [10:50<47:03,  3.47s/it]\u001b[A\n",
            "Iteration: 180 Loss: 5.049162483215332 Accuracy: 0.1985994130373001:  19%|█▉        | 188/1001 [10:53<47:04,  3.47s/it]\u001b[A\n",
            "Iteration: 180 Loss: 5.049162483215332 Accuracy: 0.1985994130373001:  19%|█▉        | 189/1001 [10:57<47:00,  3.47s/it]\u001b[A\n",
            "Iteration: 180 Loss: 5.049162483215332 Accuracy: 0.1985994130373001:  19%|█▉        | 190/1001 [11:00<46:53,  3.47s/it]\u001b[A\n",
            "Iteration: 190 Loss: 4.965390157699585 Accuracy: 0.19058665484189988:  19%|█▉        | 190/1001 [11:04<46:53,  3.47s/it]\u001b[A\n",
            "Iteration: 190 Loss: 4.965390157699585 Accuracy: 0.19058665484189988:  19%|█▉        | 191/1001 [11:04<46:56,  3.48s/it]\u001b[A\n",
            "Iteration: 190 Loss: 4.965390157699585 Accuracy: 0.19058665484189988:  19%|█▉        | 192/1001 [11:07<46:54,  3.48s/it]\u001b[A\n",
            "Iteration: 190 Loss: 4.965390157699585 Accuracy: 0.19058665484189988:  19%|█▉        | 193/1001 [11:11<46:49,  3.48s/it]\u001b[A\n",
            "Iteration: 190 Loss: 4.965390157699585 Accuracy: 0.19058665484189988:  19%|█▉        | 194/1001 [11:14<46:53,  3.49s/it]\u001b[A\n",
            "Iteration: 190 Loss: 4.965390157699585 Accuracy: 0.19058665484189988:  19%|█▉        | 195/1001 [11:18<46:45,  3.48s/it]\u001b[A\n",
            "Iteration: 190 Loss: 4.965390157699585 Accuracy: 0.19058665484189988:  20%|█▉        | 196/1001 [11:21<46:42,  3.48s/it]\u001b[A\n",
            "Iteration: 190 Loss: 4.965390157699585 Accuracy: 0.19058665484189988:  20%|█▉        | 197/1001 [11:25<46:43,  3.49s/it]\u001b[A\n",
            "Iteration: 190 Loss: 4.965390157699585 Accuracy: 0.19058665484189988:  20%|█▉        | 198/1001 [11:28<46:35,  3.48s/it]\u001b[A\n",
            "Iteration: 190 Loss: 4.965390157699585 Accuracy: 0.19058665484189988:  20%|█▉        | 199/1001 [11:32<46:27,  3.48s/it]\u001b[A\n",
            "Iteration: 190 Loss: 4.965390157699585 Accuracy: 0.19058665484189988:  20%|█▉        | 200/1001 [11:35<46:21,  3.47s/it]\u001b[A\n",
            "Iteration: 200 Loss: 4.892909097671509 Accuracy: 0.18275233432650567:  20%|█▉        | 200/1001 [11:38<46:21,  3.47s/it]\u001b[A\n",
            "Iteration: 200 Loss: 4.892909097671509 Accuracy: 0.18275233432650567:  20%|██        | 201/1001 [11:39<46:49,  3.51s/it]\u001b[A\n",
            "Iteration: 200 Loss: 4.892909097671509 Accuracy: 0.18275233432650567:  20%|██        | 202/1001 [11:42<46:36,  3.50s/it]\u001b[A\n",
            "Iteration: 200 Loss: 4.892909097671509 Accuracy: 0.18275233432650567:  20%|██        | 203/1001 [11:46<46:23,  3.49s/it]\u001b[A\n",
            "Iteration: 200 Loss: 4.892909097671509 Accuracy: 0.18275233432650567:  20%|██        | 204/1001 [11:49<46:13,  3.48s/it]\u001b[A\n",
            "Iteration: 200 Loss: 4.892909097671509 Accuracy: 0.18275233432650567:  20%|██        | 205/1001 [11:52<46:09,  3.48s/it]\u001b[A\n",
            "Iteration: 200 Loss: 4.892909097671509 Accuracy: 0.18275233432650567:  21%|██        | 206/1001 [11:56<46:03,  3.48s/it]\u001b[A\n",
            "Iteration: 200 Loss: 4.892909097671509 Accuracy: 0.18275233432650567:  21%|██        | 207/1001 [11:59<45:58,  3.47s/it]\u001b[A\n",
            "Iteration: 200 Loss: 4.892909097671509 Accuracy: 0.18275233432650567:  21%|██        | 208/1001 [12:03<45:53,  3.47s/it]\u001b[A\n",
            "Iteration: 200 Loss: 4.892909097671509 Accuracy: 0.18275233432650567:  21%|██        | 209/1001 [12:06<45:49,  3.47s/it]\u001b[A\n",
            "Iteration: 200 Loss: 4.892909097671509 Accuracy: 0.18275233432650567:  21%|██        | 210/1001 [12:10<45:46,  3.47s/it]\u001b[A\n",
            "Iteration: 210 Loss: 4.478391456604004 Accuracy: 0.2083630934357643:  21%|██        | 210/1001 [12:13<45:46,  3.47s/it] \u001b[A\n",
            "Iteration: 210 Loss: 4.478391456604004 Accuracy: 0.2083630934357643:  21%|██        | 211/1001 [12:13<45:45,  3.48s/it]\u001b[A\n",
            "Iteration: 210 Loss: 4.478391456604004 Accuracy: 0.2083630934357643:  21%|██        | 212/1001 [12:17<45:37,  3.47s/it]\u001b[A\n",
            "Iteration: 210 Loss: 4.478391456604004 Accuracy: 0.2083630934357643:  21%|██▏       | 213/1001 [12:20<45:32,  3.47s/it]\u001b[A\n",
            "Iteration: 210 Loss: 4.478391456604004 Accuracy: 0.2083630934357643:  21%|██▏       | 214/1001 [12:24<45:30,  3.47s/it]\u001b[A\n",
            "Iteration: 210 Loss: 4.478391456604004 Accuracy: 0.2083630934357643:  21%|██▏       | 215/1001 [12:27<45:26,  3.47s/it]\u001b[A\n",
            "Iteration: 210 Loss: 4.478391456604004 Accuracy: 0.2083630934357643:  22%|██▏       | 216/1001 [12:31<45:25,  3.47s/it]\u001b[A\n",
            "Iteration: 210 Loss: 4.478391456604004 Accuracy: 0.2083630934357643:  22%|██▏       | 217/1001 [12:34<45:26,  3.48s/it]\u001b[A\n",
            "Iteration: 210 Loss: 4.478391456604004 Accuracy: 0.2083630934357643:  22%|██▏       | 218/1001 [12:38<45:21,  3.48s/it]\u001b[A\n",
            "Iteration: 210 Loss: 4.478391456604004 Accuracy: 0.2083630934357643:  22%|██▏       | 219/1001 [12:41<45:16,  3.47s/it]\u001b[A\n",
            "Iteration: 210 Loss: 4.478391456604004 Accuracy: 0.2083630934357643:  22%|██▏       | 220/1001 [12:45<45:10,  3.47s/it]\u001b[A\n",
            "Iteration: 220 Loss: 4.443618702888489 Accuracy: 0.20154016315937043:  22%|██▏       | 220/1001 [12:48<45:10,  3.47s/it]\u001b[A\n",
            "Iteration: 220 Loss: 4.443618702888489 Accuracy: 0.20154016315937043:  22%|██▏       | 221/1001 [12:48<45:06,  3.47s/it]\u001b[A\n",
            "Iteration: 220 Loss: 4.443618702888489 Accuracy: 0.20154016315937043:  22%|██▏       | 222/1001 [12:51<45:07,  3.48s/it]\u001b[A\n",
            "Iteration: 220 Loss: 4.443618702888489 Accuracy: 0.20154016315937043:  22%|██▏       | 223/1001 [12:55<45:03,  3.47s/it]\u001b[A\n",
            "Iteration: 220 Loss: 4.443618702888489 Accuracy: 0.20154016315937043:  22%|██▏       | 224/1001 [12:58<45:02,  3.48s/it]\u001b[A\n",
            "Iteration: 220 Loss: 4.443618702888489 Accuracy: 0.20154016315937043:  22%|██▏       | 225/1001 [13:02<44:55,  3.47s/it]\u001b[A\n",
            "Iteration: 220 Loss: 4.443618702888489 Accuracy: 0.20154016315937043:  23%|██▎       | 226/1001 [13:05<44:51,  3.47s/it]\u001b[A\n",
            "Iteration: 220 Loss: 4.443618702888489 Accuracy: 0.20154016315937043:  23%|██▎       | 227/1001 [13:09<44:46,  3.47s/it]\u001b[A\n",
            "Iteration: 220 Loss: 4.443618702888489 Accuracy: 0.20154016315937043:  23%|██▎       | 228/1001 [13:12<44:40,  3.47s/it]\u001b[A\n",
            "Iteration: 220 Loss: 4.443618702888489 Accuracy: 0.20154016315937043:  23%|██▎       | 229/1001 [13:16<44:41,  3.47s/it]\u001b[A\n",
            "Iteration: 220 Loss: 4.443618702888489 Accuracy: 0.20154016315937043:  23%|██▎       | 230/1001 [13:19<44:38,  3.47s/it]\u001b[A\n",
            "Iteration: 230 Loss: 4.602666568756104 Accuracy: 0.21023098826408387:  23%|██▎       | 230/1001 [13:23<44:38,  3.47s/it]\u001b[A\n",
            "Iteration: 230 Loss: 4.602666568756104 Accuracy: 0.21023098826408387:  23%|██▎       | 231/1001 [13:23<44:37,  3.48s/it]\u001b[A\n",
            "Iteration: 230 Loss: 4.602666568756104 Accuracy: 0.21023098826408387:  23%|██▎       | 232/1001 [13:26<44:36,  3.48s/it]\u001b[A\n",
            "Iteration: 230 Loss: 4.602666568756104 Accuracy: 0.21023098826408387:  23%|██▎       | 233/1001 [13:30<44:30,  3.48s/it]\u001b[A\n",
            "Iteration: 230 Loss: 4.602666568756104 Accuracy: 0.21023098826408387:  23%|██▎       | 234/1001 [13:33<44:26,  3.48s/it]\u001b[A\n",
            "Iteration: 230 Loss: 4.602666568756104 Accuracy: 0.21023098826408387:  23%|██▎       | 235/1001 [13:37<44:21,  3.47s/it]\u001b[A\n",
            "Iteration: 230 Loss: 4.602666568756104 Accuracy: 0.21023098826408387:  24%|██▎       | 236/1001 [13:40<44:16,  3.47s/it]\u001b[A\n",
            "Iteration: 230 Loss: 4.602666568756104 Accuracy: 0.21023098826408387:  24%|██▎       | 237/1001 [13:44<44:12,  3.47s/it]\u001b[A\n",
            "Iteration: 230 Loss: 4.602666568756104 Accuracy: 0.21023098826408387:  24%|██▍       | 238/1001 [13:47<44:10,  3.47s/it]\u001b[A\n",
            "Iteration: 230 Loss: 4.602666568756104 Accuracy: 0.21023098826408387:  24%|██▍       | 239/1001 [13:51<44:07,  3.47s/it]\u001b[A\n",
            "Iteration: 230 Loss: 4.602666568756104 Accuracy: 0.21023098826408387:  24%|██▍       | 240/1001 [13:54<44:05,  3.48s/it]\u001b[A\n",
            "Iteration: 240 Loss: 4.600282025337219 Accuracy: 0.19545745700597764:  24%|██▍       | 240/1001 [13:57<44:05,  3.48s/it]\u001b[A\n",
            "Iteration: 240 Loss: 4.600282025337219 Accuracy: 0.19545745700597764:  24%|██▍       | 241/1001 [13:58<44:05,  3.48s/it]\u001b[A\n",
            "Iteration: 240 Loss: 4.600282025337219 Accuracy: 0.19545745700597764:  24%|██▍       | 242/1001 [14:01<43:59,  3.48s/it]\u001b[A\n",
            "Iteration: 240 Loss: 4.600282025337219 Accuracy: 0.19545745700597764:  24%|██▍       | 243/1001 [14:04<43:53,  3.47s/it]\u001b[A\n",
            "Iteration: 240 Loss: 4.600282025337219 Accuracy: 0.19545745700597764:  24%|██▍       | 244/1001 [14:08<43:45,  3.47s/it]\u001b[A\n",
            "Iteration: 240 Loss: 4.600282025337219 Accuracy: 0.19545745700597764:  24%|██▍       | 245/1001 [14:11<43:44,  3.47s/it]\u001b[A\n",
            "Iteration: 240 Loss: 4.600282025337219 Accuracy: 0.19545745700597764:  25%|██▍       | 246/1001 [14:15<43:41,  3.47s/it]\u001b[A\n",
            "Iteration: 240 Loss: 4.600282025337219 Accuracy: 0.19545745700597764:  25%|██▍       | 247/1001 [14:18<43:38,  3.47s/it]\u001b[A\n",
            "Iteration: 240 Loss: 4.600282025337219 Accuracy: 0.19545745700597764:  25%|██▍       | 248/1001 [14:22<43:35,  3.47s/it]\u001b[A\n",
            "Iteration: 240 Loss: 4.600282025337219 Accuracy: 0.19545745700597764:  25%|██▍       | 249/1001 [14:25<43:40,  3.48s/it]\u001b[A\n",
            "Iteration: 240 Loss: 4.600282025337219 Accuracy: 0.19545745700597764:  25%|██▍       | 250/1001 [14:29<43:33,  3.48s/it]\u001b[A\n",
            "Iteration: 250 Loss: 4.5131930112838745 Accuracy: 0.20408205986022948:  25%|██▍       | 250/1001 [14:32<43:33,  3.48s/it]\u001b[A\n",
            "Iteration: 250 Loss: 4.5131930112838745 Accuracy: 0.20408205986022948:  25%|██▌       | 251/1001 [14:32<43:36,  3.49s/it]\u001b[A\n",
            "Iteration: 250 Loss: 4.5131930112838745 Accuracy: 0.20408205986022948:  25%|██▌       | 252/1001 [14:36<43:37,  3.50s/it]\u001b[A\n",
            "Iteration: 250 Loss: 4.5131930112838745 Accuracy: 0.20408205986022948:  25%|██▌       | 253/1001 [14:39<43:27,  3.49s/it]\u001b[A\n",
            "Iteration: 250 Loss: 4.5131930112838745 Accuracy: 0.20408205986022948:  25%|██▌       | 254/1001 [14:43<43:22,  3.48s/it]\u001b[A\n",
            "Iteration: 250 Loss: 4.5131930112838745 Accuracy: 0.20408205986022948:  25%|██▌       | 255/1001 [14:46<43:18,  3.48s/it]\u001b[A\n",
            "Iteration: 250 Loss: 4.5131930112838745 Accuracy: 0.20408205986022948:  26%|██▌       | 256/1001 [14:50<43:12,  3.48s/it]\u001b[A\n",
            "Iteration: 250 Loss: 4.5131930112838745 Accuracy: 0.20408205986022948:  26%|██▌       | 257/1001 [14:53<43:06,  3.48s/it]\u001b[A\n",
            "Iteration: 250 Loss: 4.5131930112838745 Accuracy: 0.20408205986022948:  26%|██▌       | 258/1001 [14:57<43:00,  3.47s/it]\u001b[A\n",
            "Iteration: 250 Loss: 4.5131930112838745 Accuracy: 0.20408205986022948:  26%|██▌       | 259/1001 [15:00<42:55,  3.47s/it]\u001b[A\n",
            "Iteration: 250 Loss: 4.5131930112838745 Accuracy: 0.20408205986022948:  26%|██▌       | 260/1001 [15:04<43:01,  3.48s/it]\u001b[A\n",
            "Iteration: 260 Loss: 4.4817160844802855 Accuracy: 0.19801264703273774:  26%|██▌       | 260/1001 [15:07<43:01,  3.48s/it]\u001b[A\n",
            "Iteration: 260 Loss: 4.4817160844802855 Accuracy: 0.19801264703273774:  26%|██▌       | 261/1001 [15:07<42:54,  3.48s/it]\u001b[A\n",
            "Iteration: 260 Loss: 4.4817160844802855 Accuracy: 0.19801264703273774:  26%|██▌       | 262/1001 [15:11<42:52,  3.48s/it]\u001b[A\n",
            "Iteration: 260 Loss: 4.4817160844802855 Accuracy: 0.19801264703273774:  26%|██▋       | 263/1001 [15:14<42:46,  3.48s/it]\u001b[A\n",
            "Iteration: 260 Loss: 4.4817160844802855 Accuracy: 0.19801264703273774:  26%|██▋       | 264/1001 [15:18<42:41,  3.48s/it]\u001b[A\n",
            "Iteration: 260 Loss: 4.4817160844802855 Accuracy: 0.19801264703273774:  26%|██▋       | 265/1001 [15:21<42:38,  3.48s/it]\u001b[A\n",
            "Iteration: 260 Loss: 4.4817160844802855 Accuracy: 0.19801264703273774:  27%|██▋       | 266/1001 [15:24<42:40,  3.48s/it]\u001b[A\n",
            "Iteration: 260 Loss: 4.4817160844802855 Accuracy: 0.19801264703273774:  27%|██▋       | 267/1001 [15:28<42:45,  3.50s/it]\u001b[A\n",
            "Iteration: 260 Loss: 4.4817160844802855 Accuracy: 0.19801264703273774:  27%|██▋       | 268/1001 [15:31<42:40,  3.49s/it]\u001b[A\n",
            "Iteration: 260 Loss: 4.4817160844802855 Accuracy: 0.19801264703273774:  27%|██▋       | 269/1001 [15:35<42:32,  3.49s/it]\u001b[A\n",
            "Iteration: 260 Loss: 4.4817160844802855 Accuracy: 0.19801264703273774:  27%|██▋       | 270/1001 [15:38<42:35,  3.50s/it]\u001b[A\n",
            "Iteration: 270 Loss: 4.476298475265503 Accuracy: 0.2186615765094757:  27%|██▋       | 270/1001 [15:42<42:35,  3.50s/it]  \u001b[A\n",
            "Iteration: 270 Loss: 4.476298475265503 Accuracy: 0.2186615765094757:  27%|██▋       | 271/1001 [15:42<42:30,  3.49s/it]\u001b[A\n",
            "Iteration: 270 Loss: 4.476298475265503 Accuracy: 0.2186615765094757:  27%|██▋       | 272/1001 [15:45<42:26,  3.49s/it]\u001b[A\n",
            "Iteration: 270 Loss: 4.476298475265503 Accuracy: 0.2186615765094757:  27%|██▋       | 273/1001 [15:49<42:17,  3.49s/it]\u001b[A\n",
            "Iteration: 270 Loss: 4.476298475265503 Accuracy: 0.2186615765094757:  27%|██▋       | 274/1001 [15:52<42:09,  3.48s/it]\u001b[A\n",
            "Iteration: 270 Loss: 4.476298475265503 Accuracy: 0.2186615765094757:  27%|██▋       | 275/1001 [15:56<42:05,  3.48s/it]\u001b[A\n",
            "Iteration: 270 Loss: 4.476298475265503 Accuracy: 0.2186615765094757:  28%|██▊       | 276/1001 [15:59<42:00,  3.48s/it]\u001b[A\n",
            "Iteration: 270 Loss: 4.476298475265503 Accuracy: 0.2186615765094757:  28%|██▊       | 277/1001 [16:03<41:57,  3.48s/it]\u001b[A\n",
            "Iteration: 270 Loss: 4.476298475265503 Accuracy: 0.2186615765094757:  28%|██▊       | 278/1001 [16:06<41:51,  3.47s/it]\u001b[A\n",
            "Iteration: 270 Loss: 4.476298475265503 Accuracy: 0.2186615765094757:  28%|██▊       | 279/1001 [16:10<41:45,  3.47s/it]\u001b[A\n",
            "Iteration: 270 Loss: 4.476298475265503 Accuracy: 0.2186615765094757:  28%|██▊       | 280/1001 [16:13<41:42,  3.47s/it]\u001b[A\n",
            "Iteration: 280 Loss: 4.098959517478943 Accuracy: 0.2127893313765526:  28%|██▊       | 280/1001 [16:17<41:42,  3.47s/it]\u001b[A\n",
            "Iteration: 280 Loss: 4.098959517478943 Accuracy: 0.2127893313765526:  28%|██▊       | 281/1001 [16:17<41:50,  3.49s/it]\u001b[A\n",
            "Iteration: 280 Loss: 4.098959517478943 Accuracy: 0.2127893313765526:  28%|██▊       | 282/1001 [16:20<41:40,  3.48s/it]\u001b[A\n",
            "Iteration: 280 Loss: 4.098959517478943 Accuracy: 0.2127893313765526:  28%|██▊       | 283/1001 [16:24<41:38,  3.48s/it]\u001b[A\n",
            "Iteration: 280 Loss: 4.098959517478943 Accuracy: 0.2127893313765526:  28%|██▊       | 284/1001 [16:27<41:31,  3.48s/it]\u001b[A\n",
            "Iteration: 280 Loss: 4.098959517478943 Accuracy: 0.2127893313765526:  28%|██▊       | 285/1001 [16:31<41:28,  3.48s/it]\u001b[A\n",
            "Iteration: 280 Loss: 4.098959517478943 Accuracy: 0.2127893313765526:  29%|██▊       | 286/1001 [16:34<41:27,  3.48s/it]\u001b[A\n",
            "Iteration: 280 Loss: 4.098959517478943 Accuracy: 0.2127893313765526:  29%|██▊       | 287/1001 [16:38<41:28,  3.49s/it]\u001b[A\n",
            "Iteration: 280 Loss: 4.098959517478943 Accuracy: 0.2127893313765526:  29%|██▉       | 288/1001 [16:41<41:21,  3.48s/it]\u001b[A\n",
            "Iteration: 280 Loss: 4.098959517478943 Accuracy: 0.2127893313765526:  29%|██▉       | 289/1001 [16:45<41:21,  3.49s/it]\u001b[A\n",
            "Iteration: 280 Loss: 4.098959517478943 Accuracy: 0.2127893313765526:  29%|██▉       | 290/1001 [16:48<41:17,  3.48s/it]\u001b[A\n",
            "Iteration: 290 Loss: 4.069300246238709 Accuracy: 0.2278112694621086:  29%|██▉       | 290/1001 [16:52<41:17,  3.48s/it]\u001b[A\n",
            "Iteration: 290 Loss: 4.069300246238709 Accuracy: 0.2278112694621086:  29%|██▉       | 291/1001 [16:52<41:14,  3.49s/it]\u001b[A\n",
            "Iteration: 290 Loss: 4.069300246238709 Accuracy: 0.2278112694621086:  29%|██▉       | 292/1001 [16:55<41:05,  3.48s/it]\u001b[A\n",
            "Iteration: 290 Loss: 4.069300246238709 Accuracy: 0.2278112694621086:  29%|██▉       | 293/1001 [16:59<41:04,  3.48s/it]\u001b[A\n",
            "Iteration: 290 Loss: 4.069300246238709 Accuracy: 0.2278112694621086:  29%|██▉       | 294/1001 [17:02<40:56,  3.47s/it]\u001b[A\n",
            "Iteration: 290 Loss: 4.069300246238709 Accuracy: 0.2278112694621086:  29%|██▉       | 295/1001 [17:05<40:51,  3.47s/it]\u001b[A\n",
            "Iteration: 290 Loss: 4.069300246238709 Accuracy: 0.2278112694621086:  30%|██▉       | 296/1001 [17:09<40:45,  3.47s/it]\u001b[A\n",
            "Iteration: 290 Loss: 4.069300246238709 Accuracy: 0.2278112694621086:  30%|██▉       | 297/1001 [17:12<40:43,  3.47s/it]\u001b[A\n",
            "Iteration: 290 Loss: 4.069300246238709 Accuracy: 0.2278112694621086:  30%|██▉       | 298/1001 [17:16<40:44,  3.48s/it]\u001b[A\n",
            "Iteration: 290 Loss: 4.069300246238709 Accuracy: 0.2278112694621086:  30%|██▉       | 299/1001 [17:19<40:38,  3.47s/it]\u001b[A\n",
            "Iteration: 290 Loss: 4.069300246238709 Accuracy: 0.2278112694621086:  30%|██▉       | 300/1001 [17:23<40:36,  3.48s/it]\u001b[A\n",
            "Iteration: 300 Loss: 4.383233070373535 Accuracy: 0.21882846504449843:  30%|██▉       | 300/1001 [17:26<40:36,  3.48s/it]\u001b[A\n",
            "Iteration: 300 Loss: 4.383233070373535 Accuracy: 0.21882846504449843:  30%|███       | 301/1001 [17:26<41:04,  3.52s/it]\u001b[A\n",
            "Iteration: 300 Loss: 4.383233070373535 Accuracy: 0.21882846504449843:  30%|███       | 302/1001 [17:30<40:51,  3.51s/it]\u001b[A\n",
            "Iteration: 300 Loss: 4.383233070373535 Accuracy: 0.21882846504449843:  30%|███       | 303/1001 [17:33<40:41,  3.50s/it]\u001b[A\n",
            "Iteration: 300 Loss: 4.383233070373535 Accuracy: 0.21882846504449843:  30%|███       | 304/1001 [17:37<40:33,  3.49s/it]\u001b[A\n",
            "Iteration: 300 Loss: 4.383233070373535 Accuracy: 0.21882846504449843:  30%|███       | 305/1001 [17:40<40:26,  3.49s/it]\u001b[A\n",
            "Iteration: 300 Loss: 4.383233070373535 Accuracy: 0.21882846504449843:  31%|███       | 306/1001 [17:44<40:22,  3.49s/it]\u001b[A\n",
            "Iteration: 300 Loss: 4.383233070373535 Accuracy: 0.21882846504449843:  31%|███       | 307/1001 [17:47<40:15,  3.48s/it]\u001b[A\n",
            "Iteration: 300 Loss: 4.383233070373535 Accuracy: 0.21882846504449843:  31%|███       | 308/1001 [17:51<40:09,  3.48s/it]\u001b[A\n",
            "Iteration: 300 Loss: 4.383233070373535 Accuracy: 0.21882846504449843:  31%|███       | 309/1001 [17:54<40:06,  3.48s/it]\u001b[A\n",
            "Iteration: 300 Loss: 4.383233070373535 Accuracy: 0.21882846504449843:  31%|███       | 310/1001 [17:58<40:03,  3.48s/it]\u001b[A\n",
            "Iteration: 310 Loss: 4.553071165084839 Accuracy: 0.21886873245239258:  31%|███       | 310/1001 [18:01<40:03,  3.48s/it]\u001b[A\n",
            "Iteration: 310 Loss: 4.553071165084839 Accuracy: 0.21886873245239258:  31%|███       | 311/1001 [18:01<40:00,  3.48s/it]\u001b[A\n",
            "Iteration: 310 Loss: 4.553071165084839 Accuracy: 0.21886873245239258:  31%|███       | 312/1001 [18:05<40:01,  3.48s/it]\u001b[A\n",
            "Iteration: 310 Loss: 4.553071165084839 Accuracy: 0.21886873245239258:  31%|███▏      | 313/1001 [18:08<39:50,  3.47s/it]\u001b[A\n",
            "Iteration: 310 Loss: 4.553071165084839 Accuracy: 0.21886873245239258:  31%|███▏      | 314/1001 [18:12<39:45,  3.47s/it]\u001b[A\n",
            "Iteration: 310 Loss: 4.553071165084839 Accuracy: 0.21886873245239258:  31%|███▏      | 315/1001 [18:15<39:41,  3.47s/it]\u001b[A\n",
            "Iteration: 310 Loss: 4.553071165084839 Accuracy: 0.21886873245239258:  32%|███▏      | 316/1001 [18:19<39:40,  3.48s/it]\u001b[A\n",
            "Iteration: 310 Loss: 4.553071165084839 Accuracy: 0.21886873245239258:  32%|███▏      | 317/1001 [18:22<39:35,  3.47s/it]\u001b[A\n",
            "Iteration: 310 Loss: 4.553071165084839 Accuracy: 0.21886873245239258:  32%|███▏      | 318/1001 [18:26<39:42,  3.49s/it]\u001b[A\n",
            "Iteration: 310 Loss: 4.553071165084839 Accuracy: 0.21886873245239258:  32%|███▏      | 319/1001 [18:29<39:33,  3.48s/it]\u001b[A\n",
            "Iteration: 310 Loss: 4.553071165084839 Accuracy: 0.21886873245239258:  32%|███▏      | 320/1001 [18:32<39:27,  3.48s/it]\u001b[A\n",
            "Iteration: 320 Loss: 4.533885788917542 Accuracy: 0.22895292639732362:  32%|███▏      | 320/1001 [18:36<39:27,  3.48s/it]\u001b[A\n",
            "Iteration: 320 Loss: 4.533885788917542 Accuracy: 0.22895292639732362:  32%|███▏      | 321/1001 [18:36<39:24,  3.48s/it]\u001b[A\n",
            "Iteration: 320 Loss: 4.533885788917542 Accuracy: 0.22895292639732362:  32%|███▏      | 322/1001 [18:39<39:18,  3.47s/it]\u001b[A\n",
            "Iteration: 320 Loss: 4.533885788917542 Accuracy: 0.22895292639732362:  32%|███▏      | 323/1001 [18:43<39:11,  3.47s/it]\u001b[A\n",
            "Iteration: 320 Loss: 4.533885788917542 Accuracy: 0.22895292639732362:  32%|███▏      | 324/1001 [18:46<39:08,  3.47s/it]\u001b[A\n",
            "Iteration: 320 Loss: 4.533885788917542 Accuracy: 0.22895292639732362:  32%|███▏      | 325/1001 [18:50<39:03,  3.47s/it]\u001b[A\n",
            "Iteration: 320 Loss: 4.533885788917542 Accuracy: 0.22895292639732362:  33%|███▎      | 326/1001 [18:53<39:01,  3.47s/it]\u001b[A\n",
            "Iteration: 320 Loss: 4.533885788917542 Accuracy: 0.22895292639732362:  33%|███▎      | 327/1001 [18:57<39:07,  3.48s/it]\u001b[A\n",
            "Iteration: 320 Loss: 4.533885788917542 Accuracy: 0.22895292639732362:  33%|███▎      | 328/1001 [19:00<39:01,  3.48s/it]\u001b[A\n",
            "Iteration: 320 Loss: 4.533885788917542 Accuracy: 0.22895292639732362:  33%|███▎      | 329/1001 [19:04<38:56,  3.48s/it]\u001b[A\n",
            "Iteration: 320 Loss: 4.533885788917542 Accuracy: 0.22895292639732362:  33%|███▎      | 330/1001 [19:07<38:56,  3.48s/it]\u001b[A\n",
            "Iteration: 330 Loss: 4.124560356140137 Accuracy: 0.23396940678358077:  33%|███▎      | 330/1001 [19:11<38:56,  3.48s/it]\u001b[A\n",
            "Iteration: 330 Loss: 4.124560356140137 Accuracy: 0.23396940678358077:  33%|███▎      | 331/1001 [19:11<38:52,  3.48s/it]\u001b[A\n",
            "Iteration: 330 Loss: 4.124560356140137 Accuracy: 0.23396940678358077:  33%|███▎      | 332/1001 [19:14<38:47,  3.48s/it]\u001b[A\n",
            "Iteration: 330 Loss: 4.124560356140137 Accuracy: 0.23396940678358077:  33%|███▎      | 333/1001 [19:18<38:47,  3.48s/it]\u001b[A\n",
            "Iteration: 330 Loss: 4.124560356140137 Accuracy: 0.23396940678358077:  33%|███▎      | 334/1001 [19:21<38:41,  3.48s/it]\u001b[A\n",
            "Iteration: 330 Loss: 4.124560356140137 Accuracy: 0.23396940678358077:  33%|███▎      | 335/1001 [19:25<38:38,  3.48s/it]\u001b[A\n",
            "Iteration: 330 Loss: 4.124560356140137 Accuracy: 0.23396940678358077:  34%|███▎      | 336/1001 [19:28<38:43,  3.49s/it]\u001b[A\n",
            "Iteration: 330 Loss: 4.124560356140137 Accuracy: 0.23396940678358077:  34%|███▎      | 337/1001 [19:32<38:37,  3.49s/it]\u001b[A\n",
            "Iteration: 330 Loss: 4.124560356140137 Accuracy: 0.23396940678358077:  34%|███▍      | 338/1001 [19:35<38:27,  3.48s/it]\u001b[A\n",
            "Iteration: 330 Loss: 4.124560356140137 Accuracy: 0.23396940678358077:  34%|███▍      | 339/1001 [19:39<38:24,  3.48s/it]\u001b[A\n",
            "Iteration: 330 Loss: 4.124560356140137 Accuracy: 0.23396940678358077:  34%|███▍      | 340/1001 [19:42<38:14,  3.47s/it]\u001b[A\n",
            "Iteration: 340 Loss: 4.244497132301331 Accuracy: 0.23578131943941116:  34%|███▍      | 340/1001 [19:46<38:14,  3.47s/it]\u001b[A\n",
            "Iteration: 340 Loss: 4.244497132301331 Accuracy: 0.23578131943941116:  34%|███▍      | 341/1001 [19:46<38:14,  3.48s/it]\u001b[A\n",
            "Iteration: 340 Loss: 4.244497132301331 Accuracy: 0.23578131943941116:  34%|███▍      | 342/1001 [19:49<38:10,  3.48s/it]\u001b[A\n",
            "Iteration: 340 Loss: 4.244497132301331 Accuracy: 0.23578131943941116:  34%|███▍      | 343/1001 [19:52<38:05,  3.47s/it]\u001b[A\n",
            "Iteration: 340 Loss: 4.244497132301331 Accuracy: 0.23578131943941116:  34%|███▍      | 344/1001 [19:56<38:04,  3.48s/it]\u001b[A\n",
            "Iteration: 340 Loss: 4.244497132301331 Accuracy: 0.23578131943941116:  34%|███▍      | 345/1001 [19:59<38:00,  3.48s/it]\u001b[A\n",
            "Iteration: 340 Loss: 4.244497132301331 Accuracy: 0.23578131943941116:  35%|███▍      | 346/1001 [20:03<38:04,  3.49s/it]\u001b[A\n",
            "Iteration: 340 Loss: 4.244497132301331 Accuracy: 0.23578131943941116:  35%|███▍      | 347/1001 [20:06<38:09,  3.50s/it]\u001b[A\n",
            "Iteration: 340 Loss: 4.244497132301331 Accuracy: 0.23578131943941116:  35%|███▍      | 348/1001 [20:10<37:59,  3.49s/it]\u001b[A\n",
            "Iteration: 340 Loss: 4.244497132301331 Accuracy: 0.23578131943941116:  35%|███▍      | 349/1001 [20:13<37:52,  3.48s/it]\u001b[A\n",
            "Iteration: 340 Loss: 4.244497132301331 Accuracy: 0.23578131943941116:  35%|███▍      | 350/1001 [20:17<37:44,  3.48s/it]\u001b[A\n",
            "Iteration: 350 Loss: 3.904725766181946 Accuracy: 0.24300988465547563:  35%|███▍      | 350/1001 [20:20<37:44,  3.48s/it]\u001b[A\n",
            "Iteration: 350 Loss: 3.904725766181946 Accuracy: 0.24300988465547563:  35%|███▌      | 351/1001 [20:20<37:45,  3.49s/it]\u001b[A\n",
            "Iteration: 350 Loss: 3.904725766181946 Accuracy: 0.24300988465547563:  35%|███▌      | 352/1001 [20:24<37:37,  3.48s/it]\u001b[A\n",
            "Iteration: 350 Loss: 3.904725766181946 Accuracy: 0.24300988465547563:  35%|███▌      | 353/1001 [20:27<37:34,  3.48s/it]\u001b[A\n",
            "Iteration: 350 Loss: 3.904725766181946 Accuracy: 0.24300988465547563:  35%|███▌      | 354/1001 [20:31<37:28,  3.47s/it]\u001b[A\n",
            "Iteration: 350 Loss: 3.904725766181946 Accuracy: 0.24300988465547563:  35%|███▌      | 355/1001 [20:34<37:31,  3.49s/it]\u001b[A\n",
            "Iteration: 350 Loss: 3.904725766181946 Accuracy: 0.24300988465547563:  36%|███▌      | 356/1001 [20:38<37:25,  3.48s/it]\u001b[A\n",
            "Iteration: 350 Loss: 3.904725766181946 Accuracy: 0.24300988465547563:  36%|███▌      | 357/1001 [20:41<37:21,  3.48s/it]\u001b[A\n",
            "Iteration: 350 Loss: 3.904725766181946 Accuracy: 0.24300988465547563:  36%|███▌      | 358/1001 [20:45<37:16,  3.48s/it]\u001b[A\n",
            "Iteration: 350 Loss: 3.904725766181946 Accuracy: 0.24300988465547563:  36%|███▌      | 359/1001 [20:48<37:09,  3.47s/it]\u001b[A\n",
            "Iteration: 350 Loss: 3.904725766181946 Accuracy: 0.24300988465547563:  36%|███▌      | 360/1001 [20:52<37:05,  3.47s/it]\u001b[A\n",
            "Iteration: 360 Loss: 4.337698006629944 Accuracy: 0.23363816887140273:  36%|███▌      | 360/1001 [20:55<37:05,  3.47s/it]\u001b[A\n",
            "Iteration: 360 Loss: 4.337698006629944 Accuracy: 0.23363816887140273:  36%|███▌      | 361/1001 [20:55<37:02,  3.47s/it]\u001b[A\n",
            "Iteration: 360 Loss: 4.337698006629944 Accuracy: 0.23363816887140273:  36%|███▌      | 362/1001 [20:59<37:01,  3.48s/it]\u001b[A\n",
            "Iteration: 360 Loss: 4.337698006629944 Accuracy: 0.23363816887140273:  36%|███▋      | 363/1001 [21:02<36:54,  3.47s/it]\u001b[A\n",
            "Iteration: 360 Loss: 4.337698006629944 Accuracy: 0.23363816887140273:  36%|███▋      | 364/1001 [21:06<36:50,  3.47s/it]\u001b[A\n",
            "Iteration: 360 Loss: 4.337698006629944 Accuracy: 0.23363816887140273:  36%|███▋      | 365/1001 [21:09<36:57,  3.49s/it]\u001b[A\n",
            "Iteration: 360 Loss: 4.337698006629944 Accuracy: 0.23363816887140273:  37%|███▋      | 366/1001 [21:13<36:52,  3.48s/it]\u001b[A\n",
            "Iteration: 360 Loss: 4.337698006629944 Accuracy: 0.23363816887140273:  37%|███▋      | 367/1001 [21:16<36:56,  3.50s/it]\u001b[A\n",
            "Iteration: 360 Loss: 4.337698006629944 Accuracy: 0.23363816887140273:  37%|███▋      | 368/1001 [21:20<36:50,  3.49s/it]\u001b[A\n",
            "Iteration: 360 Loss: 4.337698006629944 Accuracy: 0.23363816887140273:  37%|███▋      | 369/1001 [21:23<36:43,  3.49s/it]\u001b[A\n",
            "Iteration: 360 Loss: 4.337698006629944 Accuracy: 0.23363816887140273:  37%|███▋      | 370/1001 [21:27<36:39,  3.49s/it]\u001b[A\n",
            "Iteration: 370 Loss: 4.109075331687928 Accuracy: 0.2262994945049286:  37%|███▋      | 370/1001 [21:30<36:39,  3.49s/it] \u001b[A\n",
            "Iteration: 370 Loss: 4.109075331687928 Accuracy: 0.2262994945049286:  37%|███▋      | 371/1001 [21:30<36:39,  3.49s/it]\u001b[A\n",
            "Iteration: 370 Loss: 4.109075331687928 Accuracy: 0.2262994945049286:  37%|███▋      | 372/1001 [21:34<36:36,  3.49s/it]\u001b[A\n",
            "Iteration: 370 Loss: 4.109075331687928 Accuracy: 0.2262994945049286:  37%|███▋      | 373/1001 [21:37<36:30,  3.49s/it]\u001b[A\n",
            "Iteration: 370 Loss: 4.109075331687928 Accuracy: 0.2262994945049286:  37%|███▋      | 374/1001 [21:41<36:34,  3.50s/it]\u001b[A\n",
            "Iteration: 370 Loss: 4.109075331687928 Accuracy: 0.2262994945049286:  37%|███▋      | 375/1001 [21:44<36:23,  3.49s/it]\u001b[A\n",
            "Iteration: 370 Loss: 4.109075331687928 Accuracy: 0.2262994945049286:  38%|███▊      | 376/1001 [21:47<36:18,  3.49s/it]\u001b[A\n",
            "Iteration: 370 Loss: 4.109075331687928 Accuracy: 0.2262994945049286:  38%|███▊      | 377/1001 [21:51<36:11,  3.48s/it]\u001b[A\n",
            "Iteration: 370 Loss: 4.109075331687928 Accuracy: 0.2262994945049286:  38%|███▊      | 378/1001 [21:54<36:10,  3.48s/it]\u001b[A\n",
            "Iteration: 370 Loss: 4.109075331687928 Accuracy: 0.2262994945049286:  38%|███▊      | 379/1001 [21:58<36:07,  3.49s/it]\u001b[A\n",
            "Iteration: 370 Loss: 4.109075331687928 Accuracy: 0.2262994945049286:  38%|███▊      | 380/1001 [22:01<36:06,  3.49s/it]\u001b[A\n",
            "Iteration: 380 Loss: 4.052724814414978 Accuracy: 0.23996693193912505:  38%|███▊      | 380/1001 [22:05<36:06,  3.49s/it]\u001b[A\n",
            "Iteration: 380 Loss: 4.052724814414978 Accuracy: 0.23996693193912505:  38%|███▊      | 381/1001 [22:05<36:02,  3.49s/it]\u001b[A\n",
            "Iteration: 380 Loss: 4.052724814414978 Accuracy: 0.23996693193912505:  38%|███▊      | 382/1001 [22:08<35:58,  3.49s/it]\u001b[A\n",
            "Iteration: 380 Loss: 4.052724814414978 Accuracy: 0.23996693193912505:  38%|███▊      | 383/1001 [22:12<35:53,  3.48s/it]\u001b[A\n",
            "Iteration: 380 Loss: 4.052724814414978 Accuracy: 0.23996693193912505:  38%|███▊      | 384/1001 [22:15<35:47,  3.48s/it]\u001b[A\n",
            "Iteration: 380 Loss: 4.052724814414978 Accuracy: 0.23996693193912505:  38%|███▊      | 385/1001 [22:19<35:44,  3.48s/it]\u001b[A\n",
            "Iteration: 380 Loss: 4.052724814414978 Accuracy: 0.23996693193912505:  39%|███▊      | 386/1001 [22:22<35:41,  3.48s/it]\u001b[A\n",
            "Iteration: 380 Loss: 4.052724814414978 Accuracy: 0.23996693193912505:  39%|███▊      | 387/1001 [22:26<35:48,  3.50s/it]\u001b[A\n",
            "Iteration: 380 Loss: 4.052724814414978 Accuracy: 0.23996693193912505:  39%|███▉      | 388/1001 [22:29<35:43,  3.50s/it]\u001b[A\n",
            "Iteration: 380 Loss: 4.052724814414978 Accuracy: 0.23996693193912505:  39%|███▉      | 389/1001 [22:33<35:35,  3.49s/it]\u001b[A\n",
            "Iteration: 380 Loss: 4.052724814414978 Accuracy: 0.23996693193912505:  39%|███▉      | 390/1001 [22:36<35:30,  3.49s/it]\u001b[A\n",
            "Iteration: 390 Loss: 4.235977077484131 Accuracy: 0.23117624819278718:  39%|███▉      | 390/1001 [22:40<35:30,  3.49s/it]\u001b[A\n",
            "Iteration: 390 Loss: 4.235977077484131 Accuracy: 0.23117624819278718:  39%|███▉      | 391/1001 [22:40<35:26,  3.49s/it]\u001b[A\n",
            "Iteration: 390 Loss: 4.235977077484131 Accuracy: 0.23117624819278718:  39%|███▉      | 392/1001 [22:43<35:21,  3.48s/it]\u001b[A\n",
            "Iteration: 390 Loss: 4.235977077484131 Accuracy: 0.23117624819278718:  39%|███▉      | 393/1001 [22:47<35:25,  3.50s/it]\u001b[A\n",
            "Iteration: 390 Loss: 4.235977077484131 Accuracy: 0.23117624819278718:  39%|███▉      | 394/1001 [22:50<35:18,  3.49s/it]\u001b[A\n",
            "Iteration: 390 Loss: 4.235977077484131 Accuracy: 0.23117624819278718:  39%|███▉      | 395/1001 [22:54<35:08,  3.48s/it]\u001b[A\n",
            "Iteration: 390 Loss: 4.235977077484131 Accuracy: 0.23117624819278718:  40%|███▉      | 396/1001 [22:57<35:05,  3.48s/it]\u001b[A\n",
            "Iteration: 390 Loss: 4.235977077484131 Accuracy: 0.23117624819278718:  40%|███▉      | 397/1001 [23:01<34:59,  3.48s/it]\u001b[A\n",
            "Iteration: 390 Loss: 4.235977077484131 Accuracy: 0.23117624819278718:  40%|███▉      | 398/1001 [23:04<34:55,  3.48s/it]\u001b[A\n",
            "Iteration: 390 Loss: 4.235977077484131 Accuracy: 0.23117624819278718:  40%|███▉      | 399/1001 [23:08<34:51,  3.47s/it]\u001b[A\n",
            "Iteration: 390 Loss: 4.235977077484131 Accuracy: 0.23117624819278718:  40%|███▉      | 400/1001 [23:11<34:46,  3.47s/it]\u001b[A\n",
            "Iteration: 400 Loss: 3.955145978927612 Accuracy: 0.23762545138597488:  40%|███▉      | 400/1001 [23:15<34:46,  3.47s/it]\u001b[A\n",
            "Iteration: 400 Loss: 3.955145978927612 Accuracy: 0.23762545138597488:  40%|████      | 401/1001 [23:15<35:03,  3.51s/it]\u001b[A\n",
            "Iteration: 400 Loss: 3.955145978927612 Accuracy: 0.23762545138597488:  40%|████      | 402/1001 [23:18<34:55,  3.50s/it]\u001b[A\n",
            "Iteration: 400 Loss: 3.955145978927612 Accuracy: 0.23762545138597488:  40%|████      | 403/1001 [23:22<34:49,  3.49s/it]\u001b[A\n",
            "Iteration: 400 Loss: 3.955145978927612 Accuracy: 0.23762545138597488:  40%|████      | 404/1001 [23:25<34:56,  3.51s/it]\u001b[A\n",
            "Iteration: 400 Loss: 3.955145978927612 Accuracy: 0.23762545138597488:  40%|████      | 405/1001 [23:29<34:49,  3.51s/it]\u001b[A\n",
            "Iteration: 400 Loss: 3.955145978927612 Accuracy: 0.23762545138597488:  41%|████      | 406/1001 [23:32<34:42,  3.50s/it]\u001b[A\n",
            "Iteration: 400 Loss: 3.955145978927612 Accuracy: 0.23762545138597488:  41%|████      | 407/1001 [23:36<34:34,  3.49s/it]\u001b[A\n",
            "Iteration: 400 Loss: 3.955145978927612 Accuracy: 0.23762545138597488:  41%|████      | 408/1001 [23:39<34:27,  3.49s/it]\u001b[A\n",
            "Iteration: 400 Loss: 3.955145978927612 Accuracy: 0.23762545138597488:  41%|████      | 409/1001 [23:43<34:22,  3.48s/it]\u001b[A\n",
            "Iteration: 400 Loss: 3.955145978927612 Accuracy: 0.23762545138597488:  41%|████      | 410/1001 [23:46<34:18,  3.48s/it]\u001b[A\n",
            "Iteration: 410 Loss: 3.840591311454773 Accuracy: 0.24415099769830703:  41%|████      | 410/1001 [23:50<34:18,  3.48s/it]\u001b[A\n",
            "Iteration: 410 Loss: 3.840591311454773 Accuracy: 0.24415099769830703:  41%|████      | 411/1001 [23:50<34:16,  3.49s/it]\u001b[A\n",
            "Iteration: 410 Loss: 3.840591311454773 Accuracy: 0.24415099769830703:  41%|████      | 412/1001 [23:53<34:20,  3.50s/it]\u001b[A\n",
            "Iteration: 410 Loss: 3.840591311454773 Accuracy: 0.24415099769830703:  41%|████▏     | 413/1001 [23:57<34:14,  3.49s/it]\u001b[A\n",
            "Iteration: 410 Loss: 3.840591311454773 Accuracy: 0.24415099769830703:  41%|████▏     | 414/1001 [24:00<34:07,  3.49s/it]\u001b[A\n",
            "Iteration: 410 Loss: 3.840591311454773 Accuracy: 0.24415099769830703:  41%|████▏     | 415/1001 [24:03<34:01,  3.48s/it]\u001b[A\n",
            "Iteration: 410 Loss: 3.840591311454773 Accuracy: 0.24415099769830703:  42%|████▏     | 416/1001 [24:07<33:57,  3.48s/it]\u001b[A\n",
            "Iteration: 410 Loss: 3.840591311454773 Accuracy: 0.24415099769830703:  42%|████▏     | 417/1001 [24:10<33:51,  3.48s/it]\u001b[A\n",
            "Iteration: 410 Loss: 3.840591311454773 Accuracy: 0.24415099769830703:  42%|████▏     | 418/1001 [24:14<33:45,  3.47s/it]\u001b[A\n",
            "Iteration: 410 Loss: 3.840591311454773 Accuracy: 0.24415099769830703:  42%|████▏     | 419/1001 [24:17<33:42,  3.48s/it]\u001b[A\n",
            "Iteration: 410 Loss: 3.840591311454773 Accuracy: 0.24415099769830703:  42%|████▏     | 420/1001 [24:21<33:37,  3.47s/it]\u001b[A\n",
            "Iteration: 420 Loss: 4.254425358772278 Accuracy: 0.24989555180072784:  42%|████▏     | 420/1001 [24:24<33:37,  3.47s/it]\u001b[A\n",
            "Iteration: 420 Loss: 4.254425358772278 Accuracy: 0.24989555180072784:  42%|████▏     | 421/1001 [24:24<33:37,  3.48s/it]\u001b[A\n",
            "Iteration: 420 Loss: 4.254425358772278 Accuracy: 0.24989555180072784:  42%|████▏     | 422/1001 [24:28<33:36,  3.48s/it]\u001b[A\n",
            "Iteration: 420 Loss: 4.254425358772278 Accuracy: 0.24989555180072784:  42%|████▏     | 423/1001 [24:31<33:32,  3.48s/it]\u001b[A\n",
            "Iteration: 420 Loss: 4.254425358772278 Accuracy: 0.24989555180072784:  42%|████▏     | 424/1001 [24:35<33:31,  3.49s/it]\u001b[A\n",
            "Iteration: 420 Loss: 4.254425358772278 Accuracy: 0.24989555180072784:  42%|████▏     | 425/1001 [24:38<33:24,  3.48s/it]\u001b[A\n",
            "Iteration: 420 Loss: 4.254425358772278 Accuracy: 0.24989555180072784:  43%|████▎     | 426/1001 [24:42<33:19,  3.48s/it]\u001b[A\n",
            "Iteration: 420 Loss: 4.254425358772278 Accuracy: 0.24989555180072784:  43%|████▎     | 427/1001 [24:45<33:16,  3.48s/it]\u001b[A\n",
            "Iteration: 420 Loss: 4.254425358772278 Accuracy: 0.24989555180072784:  43%|████▎     | 428/1001 [24:49<33:14,  3.48s/it]\u001b[A\n",
            "Iteration: 420 Loss: 4.254425358772278 Accuracy: 0.24989555180072784:  43%|████▎     | 429/1001 [24:52<33:11,  3.48s/it]\u001b[A\n",
            "Iteration: 420 Loss: 4.254425358772278 Accuracy: 0.24989555180072784:  43%|████▎     | 430/1001 [24:56<33:07,  3.48s/it]\u001b[A\n",
            "Iteration: 430 Loss: 3.829853630065918 Accuracy: 0.23978805989027024:  43%|████▎     | 430/1001 [24:59<33:07,  3.48s/it]\u001b[A\n",
            "Iteration: 430 Loss: 3.829853630065918 Accuracy: 0.23978805989027024:  43%|████▎     | 431/1001 [24:59<33:08,  3.49s/it]\u001b[A\n",
            "Iteration: 430 Loss: 3.829853630065918 Accuracy: 0.23978805989027024:  43%|████▎     | 432/1001 [25:03<33:02,  3.48s/it]\u001b[A\n",
            "Iteration: 430 Loss: 3.829853630065918 Accuracy: 0.23978805989027024:  43%|████▎     | 433/1001 [25:06<32:56,  3.48s/it]\u001b[A\n",
            "Iteration: 430 Loss: 3.829853630065918 Accuracy: 0.23978805989027024:  43%|████▎     | 434/1001 [25:10<32:51,  3.48s/it]\u001b[A\n",
            "Iteration: 430 Loss: 3.829853630065918 Accuracy: 0.23978805989027024:  43%|████▎     | 435/1001 [25:13<32:49,  3.48s/it]\u001b[A\n",
            "Iteration: 430 Loss: 3.829853630065918 Accuracy: 0.23978805989027024:  44%|████▎     | 436/1001 [25:17<32:46,  3.48s/it]\u001b[A\n",
            "Iteration: 430 Loss: 3.829853630065918 Accuracy: 0.23978805989027024:  44%|████▎     | 437/1001 [25:20<32:43,  3.48s/it]\u001b[A\n",
            "Iteration: 430 Loss: 3.829853630065918 Accuracy: 0.23978805989027024:  44%|████▍     | 438/1001 [25:24<32:40,  3.48s/it]\u001b[A\n",
            "Iteration: 430 Loss: 3.829853630065918 Accuracy: 0.23978805989027024:  44%|████▍     | 439/1001 [25:27<32:35,  3.48s/it]\u001b[A\n",
            "Iteration: 430 Loss: 3.829853630065918 Accuracy: 0.23978805989027024:  44%|████▍     | 440/1001 [25:30<32:33,  3.48s/it]\u001b[A\n",
            "Iteration: 440 Loss: 4.255763459205627 Accuracy: 0.2521519437432289:  44%|████▍     | 440/1001 [25:34<32:33,  3.48s/it] \u001b[A\n",
            "Iteration: 440 Loss: 4.255763459205627 Accuracy: 0.2521519437432289:  44%|████▍     | 441/1001 [25:34<32:40,  3.50s/it]\u001b[A\n",
            "Iteration: 440 Loss: 4.255763459205627 Accuracy: 0.2521519437432289:  44%|████▍     | 442/1001 [25:37<32:31,  3.49s/it]\u001b[A\n",
            "Iteration: 440 Loss: 4.255763459205627 Accuracy: 0.2521519437432289:  44%|████▍     | 443/1001 [25:41<32:24,  3.49s/it]\u001b[A\n",
            "Iteration: 440 Loss: 4.255763459205627 Accuracy: 0.2521519437432289:  44%|████▍     | 444/1001 [25:44<32:19,  3.48s/it]\u001b[A\n",
            "Iteration: 440 Loss: 4.255763459205627 Accuracy: 0.2521519437432289:  44%|████▍     | 445/1001 [25:48<32:14,  3.48s/it]\u001b[A\n",
            "Iteration: 440 Loss: 4.255763459205627 Accuracy: 0.2521519437432289:  45%|████▍     | 446/1001 [25:51<32:10,  3.48s/it]\u001b[A\n",
            "Iteration: 440 Loss: 4.255763459205627 Accuracy: 0.2521519437432289:  45%|████▍     | 447/1001 [25:55<32:06,  3.48s/it]\u001b[A\n",
            "Iteration: 440 Loss: 4.255763459205627 Accuracy: 0.2521519437432289:  45%|████▍     | 448/1001 [25:58<32:04,  3.48s/it]\u001b[A\n",
            "Iteration: 440 Loss: 4.255763459205627 Accuracy: 0.2521519437432289:  45%|████▍     | 449/1001 [26:02<32:02,  3.48s/it]\u001b[A\n",
            "Iteration: 440 Loss: 4.255763459205627 Accuracy: 0.2521519437432289:  45%|████▍     | 450/1001 [26:05<32:00,  3.49s/it]\u001b[A\n",
            "Iteration: 450 Loss: 3.793180966377258 Accuracy: 0.23056416511535643:  45%|████▍     | 450/1001 [26:09<32:00,  3.49s/it]\u001b[A\n",
            "Iteration: 450 Loss: 3.793180966377258 Accuracy: 0.23056416511535643:  45%|████▌     | 451/1001 [26:09<31:57,  3.49s/it]\u001b[A\n",
            "Iteration: 450 Loss: 3.793180966377258 Accuracy: 0.23056416511535643:  45%|████▌     | 452/1001 [26:12<31:52,  3.48s/it]\u001b[A\n",
            "Iteration: 450 Loss: 3.793180966377258 Accuracy: 0.23056416511535643:  45%|████▌     | 453/1001 [26:16<31:51,  3.49s/it]\u001b[A\n",
            "Iteration: 450 Loss: 3.793180966377258 Accuracy: 0.23056416511535643:  45%|████▌     | 454/1001 [26:19<31:45,  3.48s/it]\u001b[A\n",
            "Iteration: 450 Loss: 3.793180966377258 Accuracy: 0.23056416511535643:  45%|████▌     | 455/1001 [26:23<31:40,  3.48s/it]\u001b[A\n",
            "Iteration: 450 Loss: 3.793180966377258 Accuracy: 0.23056416511535643:  46%|████▌     | 456/1001 [26:26<31:43,  3.49s/it]\u001b[A\n",
            "Iteration: 450 Loss: 3.793180966377258 Accuracy: 0.23056416511535643:  46%|████▌     | 457/1001 [26:30<31:37,  3.49s/it]\u001b[A\n",
            "Iteration: 450 Loss: 3.793180966377258 Accuracy: 0.23056416511535643:  46%|████▌     | 458/1001 [26:33<31:33,  3.49s/it]\u001b[A\n",
            "Iteration: 450 Loss: 3.793180966377258 Accuracy: 0.23056416511535643:  46%|████▌     | 459/1001 [26:37<31:27,  3.48s/it]\u001b[A\n",
            "Iteration: 450 Loss: 3.793180966377258 Accuracy: 0.23056416511535643:  46%|████▌     | 460/1001 [26:40<31:25,  3.49s/it]\u001b[A\n",
            "Iteration: 460 Loss: 3.972901940345764 Accuracy: 0.2473945438861847:  46%|████▌     | 460/1001 [26:44<31:25,  3.49s/it] \u001b[A\n",
            "Iteration: 460 Loss: 3.972901940345764 Accuracy: 0.2473945438861847:  46%|████▌     | 461/1001 [26:44<31:21,  3.49s/it]\u001b[A\n",
            "Iteration: 460 Loss: 3.972901940345764 Accuracy: 0.2473945438861847:  46%|████▌     | 462/1001 [26:47<31:15,  3.48s/it]\u001b[A\n",
            "Iteration: 460 Loss: 3.972901940345764 Accuracy: 0.2473945438861847:  46%|████▋     | 463/1001 [26:51<31:12,  3.48s/it]\u001b[A\n",
            "Iteration: 460 Loss: 3.972901940345764 Accuracy: 0.2473945438861847:  46%|████▋     | 464/1001 [26:54<31:05,  3.47s/it]\u001b[A\n",
            "Iteration: 460 Loss: 3.972901940345764 Accuracy: 0.2473945438861847:  46%|████▋     | 465/1001 [26:58<31:03,  3.48s/it]\u001b[A\n",
            "Iteration: 460 Loss: 3.972901940345764 Accuracy: 0.2473945438861847:  47%|████▋     | 466/1001 [27:01<31:09,  3.49s/it]\u001b[A\n",
            "Iteration: 460 Loss: 3.972901940345764 Accuracy: 0.2473945438861847:  47%|████▋     | 467/1001 [27:05<31:04,  3.49s/it]\u001b[A\n",
            "Iteration: 460 Loss: 3.972901940345764 Accuracy: 0.2473945438861847:  47%|████▋     | 468/1001 [27:08<30:58,  3.49s/it]\u001b[A\n",
            "Iteration: 460 Loss: 3.972901940345764 Accuracy: 0.2473945438861847:  47%|████▋     | 469/1001 [27:12<30:56,  3.49s/it]\u001b[A\n",
            "Iteration: 460 Loss: 3.972901940345764 Accuracy: 0.2473945438861847:  47%|████▋     | 470/1001 [27:15<30:51,  3.49s/it]\u001b[A\n",
            "Iteration: 470 Loss: 4.108743286132812 Accuracy: 0.24814308285713196:  47%|████▋     | 470/1001 [27:19<30:51,  3.49s/it]\u001b[A\n",
            "Iteration: 470 Loss: 4.108743286132812 Accuracy: 0.24814308285713196:  47%|████▋     | 471/1001 [27:19<30:48,  3.49s/it]\u001b[A\n",
            "Iteration: 470 Loss: 4.108743286132812 Accuracy: 0.24814308285713196:  47%|████▋     | 472/1001 [27:22<30:43,  3.48s/it]\u001b[A\n",
            "Iteration: 470 Loss: 4.108743286132812 Accuracy: 0.24814308285713196:  47%|████▋     | 473/1001 [27:25<30:40,  3.49s/it]\u001b[A\n",
            "Iteration: 470 Loss: 4.108743286132812 Accuracy: 0.24814308285713196:  47%|████▋     | 474/1001 [27:29<30:36,  3.49s/it]\u001b[A\n",
            "Iteration: 470 Loss: 4.108743286132812 Accuracy: 0.24814308285713196:  47%|████▋     | 475/1001 [27:32<30:30,  3.48s/it]\u001b[A\n",
            "Iteration: 470 Loss: 4.108743286132812 Accuracy: 0.24814308285713196:  48%|████▊     | 476/1001 [27:36<30:26,  3.48s/it]\u001b[A\n",
            "Iteration: 470 Loss: 4.108743286132812 Accuracy: 0.24814308285713196:  48%|████▊     | 477/1001 [27:39<30:20,  3.47s/it]\u001b[A\n",
            "Iteration: 470 Loss: 4.108743286132812 Accuracy: 0.24814308285713196:  48%|████▊     | 478/1001 [27:43<30:14,  3.47s/it]\u001b[A\n",
            "Iteration: 470 Loss: 4.108743286132812 Accuracy: 0.24814308285713196:  48%|████▊     | 479/1001 [27:46<30:09,  3.47s/it]\u001b[A\n",
            "Iteration: 470 Loss: 4.108743286132812 Accuracy: 0.24814308285713196:  48%|████▊     | 480/1001 [27:50<30:07,  3.47s/it]\u001b[A\n",
            "Iteration: 480 Loss: 4.150676560401917 Accuracy: 0.24455031901597976:  48%|████▊     | 480/1001 [27:53<30:07,  3.47s/it]\u001b[A\n",
            "Iteration: 480 Loss: 4.150676560401917 Accuracy: 0.24455031901597976:  48%|████▊     | 481/1001 [27:53<30:06,  3.47s/it]\u001b[A\n",
            "Iteration: 480 Loss: 4.150676560401917 Accuracy: 0.24455031901597976:  48%|████▊     | 482/1001 [27:57<30:03,  3.47s/it]\u001b[A\n",
            "Iteration: 480 Loss: 4.150676560401917 Accuracy: 0.24455031901597976:  48%|████▊     | 483/1001 [28:00<30:00,  3.48s/it]\u001b[A\n",
            "Iteration: 480 Loss: 4.150676560401917 Accuracy: 0.24455031901597976:  48%|████▊     | 484/1001 [28:04<29:59,  3.48s/it]\u001b[A\n",
            "Iteration: 480 Loss: 4.150676560401917 Accuracy: 0.24455031901597976:  48%|████▊     | 485/1001 [28:07<29:56,  3.48s/it]\u001b[A\n",
            "Iteration: 480 Loss: 4.150676560401917 Accuracy: 0.24455031901597976:  49%|████▊     | 486/1001 [28:11<29:54,  3.48s/it]\u001b[A\n",
            "Iteration: 480 Loss: 4.150676560401917 Accuracy: 0.24455031901597976:  49%|████▊     | 487/1001 [28:14<29:50,  3.48s/it]\u001b[A\n",
            "Iteration: 480 Loss: 4.150676560401917 Accuracy: 0.24455031901597976:  49%|████▉     | 488/1001 [28:18<29:53,  3.50s/it]\u001b[A\n",
            "Iteration: 480 Loss: 4.150676560401917 Accuracy: 0.24455031901597976:  49%|████▉     | 489/1001 [28:21<29:45,  3.49s/it]\u001b[A\n",
            "Iteration: 480 Loss: 4.150676560401917 Accuracy: 0.24455031901597976:  49%|████▉     | 490/1001 [28:25<29:38,  3.48s/it]\u001b[A\n",
            "Iteration: 490 Loss: 3.8338560819625855 Accuracy: 0.25193019658327104:  49%|████▉     | 490/1001 [28:28<29:38,  3.48s/it]\u001b[A\n",
            "Iteration: 490 Loss: 3.8338560819625855 Accuracy: 0.25193019658327104:  49%|████▉     | 491/1001 [28:28<29:34,  3.48s/it]\u001b[A\n",
            "Iteration: 490 Loss: 3.8338560819625855 Accuracy: 0.25193019658327104:  49%|████▉     | 492/1001 [28:32<29:30,  3.48s/it]\u001b[A\n",
            "Iteration: 490 Loss: 3.8338560819625855 Accuracy: 0.25193019658327104:  49%|████▉     | 493/1001 [28:35<29:25,  3.48s/it]\u001b[A\n",
            "Iteration: 490 Loss: 3.8338560819625855 Accuracy: 0.25193019658327104:  49%|████▉     | 494/1001 [28:39<29:23,  3.48s/it]\u001b[A\n",
            "Iteration: 490 Loss: 3.8338560819625855 Accuracy: 0.25193019658327104:  49%|████▉     | 495/1001 [28:42<29:17,  3.47s/it]\u001b[A\n",
            "Iteration: 490 Loss: 3.8338560819625855 Accuracy: 0.25193019658327104:  50%|████▉     | 496/1001 [28:45<29:14,  3.47s/it]\u001b[A\n",
            "Iteration: 490 Loss: 3.8338560819625855 Accuracy: 0.25193019658327104:  50%|████▉     | 497/1001 [28:49<29:10,  3.47s/it]\u001b[A\n",
            "Iteration: 490 Loss: 3.8338560819625855 Accuracy: 0.25193019658327104:  50%|████▉     | 498/1001 [28:52<29:11,  3.48s/it]\u001b[A\n",
            "Iteration: 490 Loss: 3.8338560819625855 Accuracy: 0.25193019658327104:  50%|████▉     | 499/1001 [28:56<29:04,  3.48s/it]\u001b[A\n",
            "Iteration: 490 Loss: 3.8338560819625855 Accuracy: 0.25193019658327104:  50%|████▉     | 500/1001 [28:59<29:00,  3.47s/it]\u001b[A\n",
            "Iteration: 500 Loss: 3.828133797645569 Accuracy: 0.261874794960022:  50%|████▉     | 500/1001 [29:03<29:00,  3.47s/it]   \u001b[A\n",
            "Iteration: 500 Loss: 3.828133797645569 Accuracy: 0.261874794960022:  50%|█████     | 501/1001 [29:03<29:17,  3.52s/it]\u001b[A\n",
            "Iteration: 500 Loss: 3.828133797645569 Accuracy: 0.261874794960022:  50%|█████     | 502/1001 [29:06<29:08,  3.50s/it]\u001b[A\n",
            "Iteration: 500 Loss: 3.828133797645569 Accuracy: 0.261874794960022:  50%|█████     | 503/1001 [29:10<28:59,  3.49s/it]\u001b[A\n",
            "Iteration: 500 Loss: 3.828133797645569 Accuracy: 0.261874794960022:  50%|█████     | 504/1001 [29:13<28:52,  3.49s/it]\u001b[A\n",
            "Iteration: 500 Loss: 3.828133797645569 Accuracy: 0.261874794960022:  50%|█████     | 505/1001 [29:17<28:51,  3.49s/it]\u001b[A\n",
            "Iteration: 500 Loss: 3.828133797645569 Accuracy: 0.261874794960022:  51%|█████     | 506/1001 [29:20<28:46,  3.49s/it]\u001b[A\n",
            "Iteration: 500 Loss: 3.828133797645569 Accuracy: 0.261874794960022:  51%|█████     | 507/1001 [29:24<28:39,  3.48s/it]\u001b[A\n",
            "Iteration: 500 Loss: 3.828133797645569 Accuracy: 0.261874794960022:  51%|█████     | 508/1001 [29:27<28:35,  3.48s/it]\u001b[A\n",
            "Iteration: 500 Loss: 3.828133797645569 Accuracy: 0.261874794960022:  51%|█████     | 509/1001 [29:31<28:31,  3.48s/it]\u001b[A\n",
            "Iteration: 500 Loss: 3.828133797645569 Accuracy: 0.261874794960022:  51%|█████     | 510/1001 [29:34<28:25,  3.47s/it]\u001b[A\n",
            "Iteration: 510 Loss: 3.7679431438446045 Accuracy: 0.24776447117328643:  51%|█████     | 510/1001 [29:38<28:25,  3.47s/it]\u001b[A\n",
            "Iteration: 510 Loss: 3.7679431438446045 Accuracy: 0.24776447117328643:  51%|█████     | 511/1001 [29:38<28:24,  3.48s/it]\u001b[A\n",
            "Iteration: 510 Loss: 3.7679431438446045 Accuracy: 0.24776447117328643:  51%|█████     | 512/1001 [29:41<28:19,  3.48s/it]\u001b[A\n",
            "Iteration: 510 Loss: 3.7679431438446045 Accuracy: 0.24776447117328643:  51%|█████     | 513/1001 [29:45<28:14,  3.47s/it]\u001b[A\n",
            "Iteration: 510 Loss: 3.7679431438446045 Accuracy: 0.24776447117328643:  51%|█████▏    | 514/1001 [29:48<28:08,  3.47s/it]\u001b[A\n",
            "Iteration: 510 Loss: 3.7679431438446045 Accuracy: 0.24776447117328643:  51%|█████▏    | 515/1001 [29:52<28:07,  3.47s/it]\u001b[A\n",
            "Iteration: 510 Loss: 3.7679431438446045 Accuracy: 0.24776447117328643:  52%|█████▏    | 516/1001 [29:55<28:02,  3.47s/it]\u001b[A\n",
            "Iteration: 510 Loss: 3.7679431438446045 Accuracy: 0.24776447117328643:  52%|█████▏    | 517/1001 [29:59<28:03,  3.48s/it]\u001b[A\n",
            "Iteration: 510 Loss: 3.7679431438446045 Accuracy: 0.24776447117328643:  52%|█████▏    | 518/1001 [30:02<27:57,  3.47s/it]\u001b[A\n",
            "Iteration: 510 Loss: 3.7679431438446045 Accuracy: 0.24776447117328643:  52%|█████▏    | 519/1001 [30:06<27:56,  3.48s/it]\u001b[A\n",
            "Iteration: 510 Loss: 3.7679431438446045 Accuracy: 0.24776447117328643:  52%|█████▏    | 520/1001 [30:09<27:53,  3.48s/it]\u001b[A\n",
            "Iteration: 520 Loss: 3.8532965660095213 Accuracy: 0.24543310552835465:  52%|█████▏    | 520/1001 [30:13<27:53,  3.48s/it]\u001b[A\n",
            "Iteration: 520 Loss: 3.8532965660095213 Accuracy: 0.24543310552835465:  52%|█████▏    | 521/1001 [30:13<27:51,  3.48s/it]\u001b[A\n",
            "Iteration: 520 Loss: 3.8532965660095213 Accuracy: 0.24543310552835465:  52%|█████▏    | 522/1001 [30:16<27:47,  3.48s/it]\u001b[A\n",
            "Iteration: 520 Loss: 3.8532965660095213 Accuracy: 0.24543310552835465:  52%|█████▏    | 523/1001 [30:20<27:49,  3.49s/it]\u001b[A\n",
            "Iteration: 520 Loss: 3.8532965660095213 Accuracy: 0.24543310552835465:  52%|█████▏    | 524/1001 [30:23<27:41,  3.48s/it]\u001b[A\n",
            "Iteration: 520 Loss: 3.8532965660095213 Accuracy: 0.24543310552835465:  52%|█████▏    | 525/1001 [30:27<27:44,  3.50s/it]\u001b[A\n",
            "Iteration: 520 Loss: 3.8532965660095213 Accuracy: 0.24543310552835465:  53%|█████▎    | 526/1001 [30:30<27:42,  3.50s/it]\u001b[A\n",
            "Iteration: 520 Loss: 3.8532965660095213 Accuracy: 0.24543310552835465:  53%|█████▎    | 527/1001 [30:33<27:33,  3.49s/it]\u001b[A\n",
            "Iteration: 520 Loss: 3.8532965660095213 Accuracy: 0.24543310552835465:  53%|█████▎    | 528/1001 [30:37<27:27,  3.48s/it]\u001b[A\n",
            "Iteration: 520 Loss: 3.8532965660095213 Accuracy: 0.24543310552835465:  53%|█████▎    | 529/1001 [30:40<27:22,  3.48s/it]\u001b[A\n",
            "Iteration: 520 Loss: 3.8532965660095213 Accuracy: 0.24543310552835465:  53%|█████▎    | 530/1001 [30:44<27:15,  3.47s/it]\u001b[A\n",
            "Iteration: 530 Loss: 3.916736531257629 Accuracy: 0.2528696671128273:  53%|█████▎    | 530/1001 [30:47<27:15,  3.47s/it]  \u001b[A\n",
            "Iteration: 530 Loss: 3.916736531257629 Accuracy: 0.2528696671128273:  53%|█████▎    | 531/1001 [30:47<27:12,  3.47s/it]\u001b[A\n",
            "Iteration: 530 Loss: 3.916736531257629 Accuracy: 0.2528696671128273:  53%|█████▎    | 532/1001 [30:51<27:08,  3.47s/it]\u001b[A\n",
            "Iteration: 530 Loss: 3.916736531257629 Accuracy: 0.2528696671128273:  53%|█████▎    | 533/1001 [30:54<27:06,  3.48s/it]\u001b[A\n",
            "Iteration: 530 Loss: 3.916736531257629 Accuracy: 0.2528696671128273:  53%|█████▎    | 534/1001 [30:58<27:01,  3.47s/it]\u001b[A\n",
            "Iteration: 530 Loss: 3.916736531257629 Accuracy: 0.2528696671128273:  53%|█████▎    | 535/1001 [31:01<27:00,  3.48s/it]\u001b[A\n",
            "Iteration: 530 Loss: 3.916736531257629 Accuracy: 0.2528696671128273:  54%|█████▎    | 536/1001 [31:05<26:56,  3.48s/it]\u001b[A\n",
            "Iteration: 530 Loss: 3.916736531257629 Accuracy: 0.2528696671128273:  54%|█████▎    | 537/1001 [31:08<26:52,  3.48s/it]\u001b[A\n",
            "Iteration: 530 Loss: 3.916736531257629 Accuracy: 0.2528696671128273:  54%|█████▎    | 538/1001 [31:12<26:49,  3.48s/it]\u001b[A\n",
            "Iteration: 530 Loss: 3.916736531257629 Accuracy: 0.2528696671128273:  54%|█████▍    | 539/1001 [31:15<26:43,  3.47s/it]\u001b[A\n",
            "Iteration: 530 Loss: 3.916736531257629 Accuracy: 0.2528696671128273:  54%|█████▍    | 540/1001 [31:19<26:41,  3.47s/it]\u001b[A\n",
            "Iteration: 540 Loss: 3.874751257896423 Accuracy: 0.25872505754232406:  54%|█████▍    | 540/1001 [31:22<26:41,  3.47s/it]\u001b[A\n",
            "Iteration: 540 Loss: 3.874751257896423 Accuracy: 0.25872505754232406:  54%|█████▍    | 541/1001 [31:22<26:37,  3.47s/it]\u001b[A\n",
            "Iteration: 540 Loss: 3.874751257896423 Accuracy: 0.25872505754232406:  54%|█████▍    | 542/1001 [31:26<26:36,  3.48s/it]\u001b[A\n",
            "Iteration: 540 Loss: 3.874751257896423 Accuracy: 0.25872505754232406:  54%|█████▍    | 543/1001 [31:29<26:34,  3.48s/it]\u001b[A\n",
            "Iteration: 540 Loss: 3.874751257896423 Accuracy: 0.25872505754232406:  54%|█████▍    | 544/1001 [31:33<26:31,  3.48s/it]\u001b[A\n",
            "Iteration: 540 Loss: 3.874751257896423 Accuracy: 0.25872505754232406:  54%|█████▍    | 545/1001 [31:36<26:25,  3.48s/it]\u001b[A\n",
            "Iteration: 540 Loss: 3.874751257896423 Accuracy: 0.25872505754232406:  55%|█████▍    | 546/1001 [31:39<26:22,  3.48s/it]\u001b[A\n",
            "Iteration: 540 Loss: 3.874751257896423 Accuracy: 0.25872505754232406:  55%|█████▍    | 547/1001 [31:43<26:19,  3.48s/it]\u001b[A\n",
            "Iteration: 540 Loss: 3.874751257896423 Accuracy: 0.25872505754232406:  55%|█████▍    | 548/1001 [31:46<26:15,  3.48s/it]\u001b[A\n",
            "Iteration: 540 Loss: 3.874751257896423 Accuracy: 0.25872505754232406:  55%|█████▍    | 549/1001 [31:50<26:11,  3.48s/it]\u001b[A\n",
            "Iteration: 540 Loss: 3.874751257896423 Accuracy: 0.25872505754232406:  55%|█████▍    | 550/1001 [31:53<26:09,  3.48s/it]\u001b[A\n",
            "Iteration: 550 Loss: 3.9747300863265993 Accuracy: 0.2460083931684494:  55%|█████▍    | 550/1001 [31:57<26:09,  3.48s/it]\u001b[A\n",
            "Iteration: 550 Loss: 3.9747300863265993 Accuracy: 0.2460083931684494:  55%|█████▌    | 551/1001 [31:57<26:04,  3.48s/it]\u001b[A\n",
            "Iteration: 550 Loss: 3.9747300863265993 Accuracy: 0.2460083931684494:  55%|█████▌    | 552/1001 [32:00<26:04,  3.49s/it]\u001b[A\n",
            "Iteration: 550 Loss: 3.9747300863265993 Accuracy: 0.2460083931684494:  55%|█████▌    | 553/1001 [32:04<25:59,  3.48s/it]\u001b[A\n",
            "Iteration: 550 Loss: 3.9747300863265993 Accuracy: 0.2460083931684494:  55%|█████▌    | 554/1001 [32:07<25:56,  3.48s/it]\u001b[A\n",
            "Iteration: 550 Loss: 3.9747300863265993 Accuracy: 0.2460083931684494:  55%|█████▌    | 555/1001 [32:11<25:53,  3.48s/it]\u001b[A\n",
            "Iteration: 550 Loss: 3.9747300863265993 Accuracy: 0.2460083931684494:  56%|█████▌    | 556/1001 [32:14<25:49,  3.48s/it]\u001b[A\n",
            "Iteration: 550 Loss: 3.9747300863265993 Accuracy: 0.2460083931684494:  56%|█████▌    | 557/1001 [32:18<25:46,  3.48s/it]\u001b[A\n",
            "Iteration: 550 Loss: 3.9747300863265993 Accuracy: 0.2460083931684494:  56%|█████▌    | 558/1001 [32:21<25:46,  3.49s/it]\u001b[A\n",
            "Iteration: 550 Loss: 3.9747300863265993 Accuracy: 0.2460083931684494:  56%|█████▌    | 559/1001 [32:25<25:39,  3.48s/it]\u001b[A\n",
            "Iteration: 550 Loss: 3.9747300863265993 Accuracy: 0.2460083931684494:  56%|█████▌    | 560/1001 [32:28<25:33,  3.48s/it]\u001b[A\n",
            "Iteration: 560 Loss: 3.8515533685684202 Accuracy: 0.25683002471923827:  56%|█████▌    | 560/1001 [32:32<25:33,  3.48s/it]\u001b[A\n",
            "Iteration: 560 Loss: 3.8515533685684202 Accuracy: 0.25683002471923827:  56%|█████▌    | 561/1001 [32:32<25:29,  3.48s/it]\u001b[A\n",
            "Iteration: 560 Loss: 3.8515533685684202 Accuracy: 0.25683002471923827:  56%|█████▌    | 562/1001 [32:35<25:25,  3.47s/it]\u001b[A\n",
            "Iteration: 560 Loss: 3.8515533685684202 Accuracy: 0.25683002471923827:  56%|█████▌    | 563/1001 [32:39<25:22,  3.48s/it]\u001b[A\n",
            "Iteration: 560 Loss: 3.8515533685684202 Accuracy: 0.25683002471923827:  56%|█████▋    | 564/1001 [32:42<25:22,  3.48s/it]\u001b[A\n",
            "Iteration: 560 Loss: 3.8515533685684202 Accuracy: 0.25683002471923827:  56%|█████▋    | 565/1001 [32:46<25:17,  3.48s/it]\u001b[A\n",
            "Iteration: 560 Loss: 3.8515533685684202 Accuracy: 0.25683002471923827:  57%|█████▋    | 566/1001 [32:49<25:15,  3.48s/it]\u001b[A\n",
            "Iteration: 560 Loss: 3.8515533685684202 Accuracy: 0.25683002471923827:  57%|█████▋    | 567/1001 [32:53<25:10,  3.48s/it]\u001b[A\n",
            "Iteration: 560 Loss: 3.8515533685684202 Accuracy: 0.25683002471923827:  57%|█████▋    | 568/1001 [32:56<25:06,  3.48s/it]\u001b[A\n",
            "Iteration: 560 Loss: 3.8515533685684202 Accuracy: 0.25683002471923827:  57%|█████▋    | 569/1001 [33:00<25:00,  3.47s/it]\u001b[A\n",
            "Iteration: 560 Loss: 3.8515533685684202 Accuracy: 0.25683002471923827:  57%|█████▋    | 570/1001 [33:03<24:56,  3.47s/it]\u001b[A\n",
            "Iteration: 570 Loss: 4.019475102424622 Accuracy: 0.25774614214897157:  57%|█████▋    | 570/1001 [33:06<24:56,  3.47s/it] \u001b[A\n",
            "Iteration: 570 Loss: 4.019475102424622 Accuracy: 0.25774614214897157:  57%|█████▋    | 571/1001 [33:06<24:52,  3.47s/it]\u001b[A\n",
            "Iteration: 570 Loss: 4.019475102424622 Accuracy: 0.25774614214897157:  57%|█████▋    | 572/1001 [33:10<24:49,  3.47s/it]\u001b[A\n",
            "Iteration: 570 Loss: 4.019475102424622 Accuracy: 0.25774614214897157:  57%|█████▋    | 573/1001 [33:13<24:49,  3.48s/it]\u001b[A\n",
            "Iteration: 570 Loss: 4.019475102424622 Accuracy: 0.25774614214897157:  57%|█████▋    | 574/1001 [33:17<24:44,  3.48s/it]\u001b[A\n",
            "Iteration: 570 Loss: 4.019475102424622 Accuracy: 0.25774614214897157:  57%|█████▋    | 575/1001 [33:20<24:41,  3.48s/it]\u001b[A\n",
            "Iteration: 570 Loss: 4.019475102424622 Accuracy: 0.25774614214897157:  58%|█████▊    | 576/1001 [33:24<24:37,  3.48s/it]\u001b[A\n",
            "Iteration: 570 Loss: 4.019475102424622 Accuracy: 0.25774614214897157:  58%|█████▊    | 577/1001 [33:27<24:41,  3.49s/it]\u001b[A\n",
            "Iteration: 570 Loss: 4.019475102424622 Accuracy: 0.25774614214897157:  58%|█████▊    | 578/1001 [33:31<24:35,  3.49s/it]\u001b[A\n",
            "Iteration: 570 Loss: 4.019475102424622 Accuracy: 0.25774614214897157:  58%|█████▊    | 579/1001 [33:34<24:29,  3.48s/it]\u001b[A\n",
            "Iteration: 570 Loss: 4.019475102424622 Accuracy: 0.25774614214897157:  58%|█████▊    | 580/1001 [33:38<24:24,  3.48s/it]\u001b[A\n",
            "Iteration: 580 Loss: 4.100518727302552 Accuracy: 0.2658597379922867:  58%|█████▊    | 580/1001 [33:41<24:24,  3.48s/it] \u001b[A\n",
            "Iteration: 580 Loss: 4.100518727302552 Accuracy: 0.2658597379922867:  58%|█████▊    | 581/1001 [33:41<24:23,  3.49s/it]\u001b[A\n",
            "Iteration: 580 Loss: 4.100518727302552 Accuracy: 0.2658597379922867:  58%|█████▊    | 582/1001 [33:45<24:19,  3.48s/it]\u001b[A\n",
            "Iteration: 580 Loss: 4.100518727302552 Accuracy: 0.2658597379922867:  58%|█████▊    | 583/1001 [33:48<24:20,  3.49s/it]\u001b[A\n",
            "Iteration: 580 Loss: 4.100518727302552 Accuracy: 0.2658597379922867:  58%|█████▊    | 584/1001 [33:52<24:16,  3.49s/it]\u001b[A\n",
            "Iteration: 580 Loss: 4.100518727302552 Accuracy: 0.2658597379922867:  58%|█████▊    | 585/1001 [33:55<24:11,  3.49s/it]\u001b[A\n",
            "Iteration: 580 Loss: 4.100518727302552 Accuracy: 0.2658597379922867:  59%|█████▊    | 586/1001 [33:59<24:07,  3.49s/it]\u001b[A\n",
            "Iteration: 580 Loss: 4.100518727302552 Accuracy: 0.2658597379922867:  59%|█████▊    | 587/1001 [34:02<24:04,  3.49s/it]\u001b[A\n",
            "Iteration: 580 Loss: 4.100518727302552 Accuracy: 0.2658597379922867:  59%|█████▊    | 588/1001 [34:06<24:00,  3.49s/it]\u001b[A\n",
            "Iteration: 580 Loss: 4.100518727302552 Accuracy: 0.2658597379922867:  59%|█████▉    | 589/1001 [34:09<23:55,  3.48s/it]\u001b[A\n",
            "Iteration: 580 Loss: 4.100518727302552 Accuracy: 0.2658597379922867:  59%|█████▉    | 590/1001 [34:13<23:52,  3.49s/it]\u001b[A\n",
            "Iteration: 590 Loss: 3.9155643701553347 Accuracy: 0.25256662368774413:  59%|█████▉    | 590/1001 [34:16<23:52,  3.49s/it]\u001b[A\n",
            "Iteration: 590 Loss: 3.9155643701553347 Accuracy: 0.25256662368774413:  59%|█████▉    | 591/1001 [34:16<23:49,  3.49s/it]\u001b[A\n",
            "Iteration: 590 Loss: 3.9155643701553347 Accuracy: 0.25256662368774413:  59%|█████▉    | 592/1001 [34:20<23:44,  3.48s/it]\u001b[A\n",
            "Iteration: 590 Loss: 3.9155643701553347 Accuracy: 0.25256662368774413:  59%|█████▉    | 593/1001 [34:23<23:43,  3.49s/it]\u001b[A\n",
            "Iteration: 590 Loss: 3.9155643701553347 Accuracy: 0.25256662368774413:  59%|█████▉    | 594/1001 [34:27<23:41,  3.49s/it]\u001b[A\n",
            "Iteration: 590 Loss: 3.9155643701553347 Accuracy: 0.25256662368774413:  59%|█████▉    | 595/1001 [34:30<23:35,  3.49s/it]\u001b[A\n",
            "Iteration: 590 Loss: 3.9155643701553347 Accuracy: 0.25256662368774413:  60%|█████▉    | 596/1001 [34:34<23:32,  3.49s/it]\u001b[A\n",
            "Iteration: 590 Loss: 3.9155643701553347 Accuracy: 0.25256662368774413:  60%|█████▉    | 597/1001 [34:37<23:28,  3.49s/it]\u001b[A\n",
            "Iteration: 590 Loss: 3.9155643701553347 Accuracy: 0.25256662368774413:  60%|█████▉    | 598/1001 [34:41<23:22,  3.48s/it]\u001b[A\n",
            "Iteration: 590 Loss: 3.9155643701553347 Accuracy: 0.25256662368774413:  60%|█████▉    | 599/1001 [34:44<23:16,  3.47s/it]\u001b[A\n",
            "Iteration: 590 Loss: 3.9155643701553347 Accuracy: 0.25256662368774413:  60%|█████▉    | 600/1001 [34:48<23:12,  3.47s/it]\u001b[A\n",
            "Iteration: 600 Loss: 3.931679129600525 Accuracy: 0.2591610670089722:  60%|█████▉    | 600/1001 [34:51<23:12,  3.47s/it]  \u001b[A\n",
            "Iteration: 600 Loss: 3.931679129600525 Accuracy: 0.2591610670089722:  60%|██████    | 601/1001 [34:51<23:25,  3.51s/it]\u001b[A\n",
            "Iteration: 600 Loss: 3.931679129600525 Accuracy: 0.2591610670089722:  60%|██████    | 602/1001 [34:55<23:18,  3.50s/it]\u001b[A\n",
            "Iteration: 600 Loss: 3.931679129600525 Accuracy: 0.2591610670089722:  60%|██████    | 603/1001 [34:58<23:10,  3.49s/it]\u001b[A\n",
            "Iteration: 600 Loss: 3.931679129600525 Accuracy: 0.2591610670089722:  60%|██████    | 604/1001 [35:02<23:04,  3.49s/it]\u001b[A\n",
            "Iteration: 600 Loss: 3.931679129600525 Accuracy: 0.2591610670089722:  60%|██████    | 605/1001 [35:05<22:59,  3.48s/it]\u001b[A\n",
            "Iteration: 600 Loss: 3.931679129600525 Accuracy: 0.2591610670089722:  61%|██████    | 606/1001 [35:09<22:56,  3.48s/it]\u001b[A\n",
            "Iteration: 600 Loss: 3.931679129600525 Accuracy: 0.2591610670089722:  61%|██████    | 607/1001 [35:12<22:53,  3.49s/it]\u001b[A\n",
            "Iteration: 600 Loss: 3.931679129600525 Accuracy: 0.2591610670089722:  61%|██████    | 608/1001 [35:15<22:47,  3.48s/it]\u001b[A\n",
            "Iteration: 600 Loss: 3.931679129600525 Accuracy: 0.2591610670089722:  61%|██████    | 609/1001 [35:19<22:43,  3.48s/it]\u001b[A\n",
            "Iteration: 600 Loss: 3.931679129600525 Accuracy: 0.2591610670089722:  61%|██████    | 610/1001 [35:22<22:37,  3.47s/it]\u001b[A\n",
            "Iteration: 610 Loss: 3.8408666849136353 Accuracy: 0.25746970623731613:  61%|██████    | 610/1001 [35:26<22:37,  3.47s/it]\u001b[A\n",
            "Iteration: 610 Loss: 3.8408666849136353 Accuracy: 0.25746970623731613:  61%|██████    | 611/1001 [35:26<22:43,  3.50s/it]\u001b[A\n",
            "Iteration: 610 Loss: 3.8408666849136353 Accuracy: 0.25746970623731613:  61%|██████    | 612/1001 [35:29<22:37,  3.49s/it]\u001b[A\n",
            "Iteration: 610 Loss: 3.8408666849136353 Accuracy: 0.25746970623731613:  61%|██████    | 613/1001 [35:33<22:33,  3.49s/it]\u001b[A\n",
            "Iteration: 610 Loss: 3.8408666849136353 Accuracy: 0.25746970623731613:  61%|██████▏   | 614/1001 [35:36<22:27,  3.48s/it]\u001b[A\n",
            "Iteration: 610 Loss: 3.8408666849136353 Accuracy: 0.25746970623731613:  61%|██████▏   | 615/1001 [35:40<22:25,  3.49s/it]\u001b[A\n",
            "Iteration: 610 Loss: 3.8408666849136353 Accuracy: 0.25746970623731613:  62%|██████▏   | 616/1001 [35:43<22:20,  3.48s/it]\u001b[A\n",
            "Iteration: 610 Loss: 3.8408666849136353 Accuracy: 0.25746970623731613:  62%|██████▏   | 617/1001 [35:47<22:16,  3.48s/it]\u001b[A\n",
            "Iteration: 610 Loss: 3.8408666849136353 Accuracy: 0.25746970623731613:  62%|██████▏   | 618/1001 [35:50<22:10,  3.48s/it]\u001b[A\n",
            "Iteration: 610 Loss: 3.8408666849136353 Accuracy: 0.25746970623731613:  62%|██████▏   | 619/1001 [35:54<22:08,  3.48s/it]\u001b[A\n",
            "Iteration: 610 Loss: 3.8408666849136353 Accuracy: 0.25746970623731613:  62%|██████▏   | 620/1001 [35:57<22:02,  3.47s/it]\u001b[A\n",
            "Iteration: 620 Loss: 3.9910149812698363 Accuracy: 0.25395016074180604:  62%|██████▏   | 620/1001 [36:01<22:02,  3.47s/it]\u001b[A\n",
            "Iteration: 620 Loss: 3.9910149812698363 Accuracy: 0.25395016074180604:  62%|██████▏   | 621/1001 [36:01<22:08,  3.50s/it]\u001b[A\n",
            "Iteration: 620 Loss: 3.9910149812698363 Accuracy: 0.25395016074180604:  62%|██████▏   | 622/1001 [36:04<22:03,  3.49s/it]\u001b[A\n",
            "Iteration: 620 Loss: 3.9910149812698363 Accuracy: 0.25395016074180604:  62%|██████▏   | 623/1001 [36:08<21:58,  3.49s/it]\u001b[A\n",
            "Iteration: 620 Loss: 3.9910149812698363 Accuracy: 0.25395016074180604:  62%|██████▏   | 624/1001 [36:11<21:52,  3.48s/it]\u001b[A\n",
            "Iteration: 620 Loss: 3.9910149812698363 Accuracy: 0.25395016074180604:  62%|██████▏   | 625/1001 [36:15<21:53,  3.49s/it]\u001b[A\n",
            "Iteration: 620 Loss: 3.9910149812698363 Accuracy: 0.25395016074180604:  63%|██████▎   | 626/1001 [36:18<21:47,  3.49s/it]\u001b[A\n",
            "Iteration: 620 Loss: 3.9910149812698363 Accuracy: 0.25395016074180604:  63%|██████▎   | 627/1001 [36:22<21:42,  3.48s/it]\u001b[A\n",
            "Iteration: 620 Loss: 3.9910149812698363 Accuracy: 0.25395016074180604:  63%|██████▎   | 628/1001 [36:25<21:38,  3.48s/it]\u001b[A\n",
            "Iteration: 620 Loss: 3.9910149812698363 Accuracy: 0.25395016074180604:  63%|██████▎   | 629/1001 [36:29<21:36,  3.48s/it]\u001b[A\n",
            "Iteration: 620 Loss: 3.9910149812698363 Accuracy: 0.25395016074180604:  63%|██████▎   | 630/1001 [36:32<21:37,  3.50s/it]\u001b[A\n",
            "Iteration: 630 Loss: 3.847146463394165 Accuracy: 0.25371854156255724:  63%|██████▎   | 630/1001 [36:36<21:37,  3.50s/it] \u001b[A\n",
            "Iteration: 630 Loss: 3.847146463394165 Accuracy: 0.25371854156255724:  63%|██████▎   | 631/1001 [36:36<21:34,  3.50s/it]\u001b[A\n",
            "Iteration: 630 Loss: 3.847146463394165 Accuracy: 0.25371854156255724:  63%|██████▎   | 632/1001 [36:39<21:28,  3.49s/it]\u001b[A\n",
            "Iteration: 630 Loss: 3.847146463394165 Accuracy: 0.25371854156255724:  63%|██████▎   | 633/1001 [36:43<21:22,  3.49s/it]\u001b[A\n",
            "Iteration: 630 Loss: 3.847146463394165 Accuracy: 0.25371854156255724:  63%|██████▎   | 634/1001 [36:46<21:15,  3.47s/it]\u001b[A\n",
            "Iteration: 630 Loss: 3.847146463394165 Accuracy: 0.25371854156255724:  63%|██████▎   | 635/1001 [36:50<21:10,  3.47s/it]\u001b[A\n",
            "Iteration: 630 Loss: 3.847146463394165 Accuracy: 0.25371854156255724:  64%|██████▎   | 636/1001 [36:53<21:07,  3.47s/it]\u001b[A\n",
            "Iteration: 630 Loss: 3.847146463394165 Accuracy: 0.25371854156255724:  64%|██████▎   | 637/1001 [36:56<21:04,  3.47s/it]\u001b[A\n",
            "Iteration: 630 Loss: 3.847146463394165 Accuracy: 0.25371854156255724:  64%|██████▎   | 638/1001 [37:00<21:02,  3.48s/it]\u001b[A\n",
            "Iteration: 630 Loss: 3.847146463394165 Accuracy: 0.25371854156255724:  64%|██████▍   | 639/1001 [37:03<20:58,  3.48s/it]\u001b[A\n",
            "Iteration: 630 Loss: 3.847146463394165 Accuracy: 0.25371854156255724:  64%|██████▍   | 640/1001 [37:07<20:57,  3.48s/it]\u001b[A\n",
            "Iteration: 640 Loss: 3.919131898880005 Accuracy: 0.25193185955286024:  64%|██████▍   | 640/1001 [37:10<20:57,  3.48s/it]\u001b[A\n",
            "Iteration: 640 Loss: 3.919131898880005 Accuracy: 0.25193185955286024:  64%|██████▍   | 641/1001 [37:10<20:53,  3.48s/it]\u001b[A\n",
            "Iteration: 640 Loss: 3.919131898880005 Accuracy: 0.25193185955286024:  64%|██████▍   | 642/1001 [37:14<20:49,  3.48s/it]\u001b[A\n",
            "Iteration: 640 Loss: 3.919131898880005 Accuracy: 0.25193185955286024:  64%|██████▍   | 643/1001 [37:17<20:45,  3.48s/it]\u001b[A\n",
            "Iteration: 640 Loss: 3.919131898880005 Accuracy: 0.25193185955286024:  64%|██████▍   | 644/1001 [37:21<20:42,  3.48s/it]\u001b[A\n",
            "Iteration: 640 Loss: 3.919131898880005 Accuracy: 0.25193185955286024:  64%|██████▍   | 645/1001 [37:24<20:39,  3.48s/it]\u001b[A\n",
            "Iteration: 640 Loss: 3.919131898880005 Accuracy: 0.25193185955286024:  65%|██████▍   | 646/1001 [37:28<20:38,  3.49s/it]\u001b[A\n",
            "Iteration: 640 Loss: 3.919131898880005 Accuracy: 0.25193185955286024:  65%|██████▍   | 647/1001 [37:31<20:34,  3.49s/it]\u001b[A\n",
            "Iteration: 640 Loss: 3.919131898880005 Accuracy: 0.25193185955286024:  65%|██████▍   | 648/1001 [37:35<20:28,  3.48s/it]\u001b[A\n",
            "Iteration: 640 Loss: 3.919131898880005 Accuracy: 0.25193185955286024:  65%|██████▍   | 649/1001 [37:38<20:23,  3.48s/it]\u001b[A\n",
            "Iteration: 640 Loss: 3.919131898880005 Accuracy: 0.25193185955286024:  65%|██████▍   | 650/1001 [37:42<20:20,  3.48s/it]\u001b[A\n",
            "Iteration: 650 Loss: 3.741165208816528 Accuracy: 0.24792874306440355:  65%|██████▍   | 650/1001 [37:45<20:20,  3.48s/it]\u001b[A\n",
            "Iteration: 650 Loss: 3.741165208816528 Accuracy: 0.24792874306440355:  65%|██████▌   | 651/1001 [37:45<20:16,  3.48s/it]\u001b[A\n",
            "Iteration: 650 Loss: 3.741165208816528 Accuracy: 0.24792874306440355:  65%|██████▌   | 652/1001 [37:49<20:14,  3.48s/it]\u001b[A\n",
            "Iteration: 650 Loss: 3.741165208816528 Accuracy: 0.24792874306440355:  65%|██████▌   | 653/1001 [37:52<20:08,  3.47s/it]\u001b[A\n",
            "Iteration: 650 Loss: 3.741165208816528 Accuracy: 0.24792874306440355:  65%|██████▌   | 654/1001 [37:56<20:06,  3.48s/it]\u001b[A\n",
            "Iteration: 650 Loss: 3.741165208816528 Accuracy: 0.24792874306440355:  65%|██████▌   | 655/1001 [37:59<20:01,  3.47s/it]\u001b[A\n",
            "Iteration: 650 Loss: 3.741165208816528 Accuracy: 0.24792874306440355:  66%|██████▌   | 656/1001 [38:03<19:58,  3.47s/it]\u001b[A\n",
            "Iteration: 650 Loss: 3.741165208816528 Accuracy: 0.24792874306440355:  66%|██████▌   | 657/1001 [38:06<19:54,  3.47s/it]\u001b[A\n",
            "Iteration: 650 Loss: 3.741165208816528 Accuracy: 0.24792874306440355:  66%|██████▌   | 658/1001 [38:10<19:51,  3.47s/it]\u001b[A\n",
            "Iteration: 650 Loss: 3.741165208816528 Accuracy: 0.24792874306440355:  66%|██████▌   | 659/1001 [38:13<19:49,  3.48s/it]\u001b[A\n",
            "Iteration: 650 Loss: 3.741165208816528 Accuracy: 0.24792874306440355:  66%|██████▌   | 660/1001 [38:17<19:46,  3.48s/it]\u001b[A\n",
            "Iteration: 660 Loss: 3.7281803607940676 Accuracy: 0.2508624643087387:  66%|██████▌   | 660/1001 [38:20<19:46,  3.48s/it]\u001b[A\n",
            "Iteration: 660 Loss: 3.7281803607940676 Accuracy: 0.2508624643087387:  66%|██████▌   | 661/1001 [38:20<19:42,  3.48s/it]\u001b[A\n",
            "Iteration: 660 Loss: 3.7281803607940676 Accuracy: 0.2508624643087387:  66%|██████▌   | 662/1001 [38:23<19:39,  3.48s/it]\u001b[A\n",
            "Iteration: 660 Loss: 3.7281803607940676 Accuracy: 0.2508624643087387:  66%|██████▌   | 663/1001 [38:27<19:38,  3.49s/it]\u001b[A\n",
            "Iteration: 660 Loss: 3.7281803607940676 Accuracy: 0.2508624643087387:  66%|██████▋   | 664/1001 [38:30<19:33,  3.48s/it]\u001b[A\n",
            "Iteration: 660 Loss: 3.7281803607940676 Accuracy: 0.2508624643087387:  66%|██████▋   | 665/1001 [38:34<19:27,  3.48s/it]\u001b[A\n",
            "Iteration: 660 Loss: 3.7281803607940676 Accuracy: 0.2508624643087387:  67%|██████▋   | 666/1001 [38:37<19:28,  3.49s/it]\u001b[A\n",
            "Iteration: 660 Loss: 3.7281803607940676 Accuracy: 0.2508624643087387:  67%|██████▋   | 667/1001 [38:41<19:22,  3.48s/it]\u001b[A\n",
            "Iteration: 660 Loss: 3.7281803607940676 Accuracy: 0.2508624643087387:  67%|██████▋   | 668/1001 [38:44<19:22,  3.49s/it]\u001b[A\n",
            "Iteration: 660 Loss: 3.7281803607940676 Accuracy: 0.2508624643087387:  67%|██████▋   | 669/1001 [38:48<19:19,  3.49s/it]\u001b[A\n",
            "Iteration: 660 Loss: 3.7281803607940676 Accuracy: 0.2508624643087387:  67%|██████▋   | 670/1001 [38:51<19:13,  3.48s/it]\u001b[A\n",
            "Iteration: 670 Loss: 3.7198355436325072 Accuracy: 0.25912559777498245:  67%|██████▋   | 670/1001 [38:55<19:13,  3.48s/it]\u001b[A\n",
            "Iteration: 670 Loss: 3.7198355436325072 Accuracy: 0.25912559777498245:  67%|██████▋   | 671/1001 [38:55<19:09,  3.48s/it]\u001b[A\n",
            "Iteration: 670 Loss: 3.7198355436325072 Accuracy: 0.25912559777498245:  67%|██████▋   | 672/1001 [38:58<19:06,  3.48s/it]\u001b[A\n",
            "Iteration: 670 Loss: 3.7198355436325072 Accuracy: 0.25912559777498245:  67%|██████▋   | 673/1001 [39:02<19:01,  3.48s/it]\u001b[A\n",
            "Iteration: 670 Loss: 3.7198355436325072 Accuracy: 0.25912559777498245:  67%|██████▋   | 674/1001 [39:05<18:57,  3.48s/it]\u001b[A\n",
            "Iteration: 670 Loss: 3.7198355436325072 Accuracy: 0.25912559777498245:  67%|██████▋   | 675/1001 [39:09<18:54,  3.48s/it]\u001b[A\n",
            "Iteration: 670 Loss: 3.7198355436325072 Accuracy: 0.25912559777498245:  68%|██████▊   | 676/1001 [39:12<18:49,  3.48s/it]\u001b[A\n",
            "Iteration: 670 Loss: 3.7198355436325072 Accuracy: 0.25912559777498245:  68%|██████▊   | 677/1001 [39:16<18:46,  3.48s/it]\u001b[A\n",
            "Iteration: 670 Loss: 3.7198355436325072 Accuracy: 0.25912559777498245:  68%|██████▊   | 678/1001 [39:19<18:41,  3.47s/it]\u001b[A\n",
            "Iteration: 670 Loss: 3.7198355436325072 Accuracy: 0.25912559777498245:  68%|██████▊   | 679/1001 [39:23<18:38,  3.47s/it]\u001b[A\n",
            "Iteration: 670 Loss: 3.7198355436325072 Accuracy: 0.25912559777498245:  68%|██████▊   | 680/1001 [39:26<18:35,  3.48s/it]\u001b[A\n",
            "Iteration: 680 Loss: 4.1515168190002445 Accuracy: 0.26202595680952073:  68%|██████▊   | 680/1001 [39:30<18:35,  3.48s/it]\u001b[A\n",
            "Iteration: 680 Loss: 4.1515168190002445 Accuracy: 0.26202595680952073:  68%|██████▊   | 681/1001 [39:30<18:32,  3.48s/it]\u001b[A\n",
            "Iteration: 680 Loss: 4.1515168190002445 Accuracy: 0.26202595680952073:  68%|██████▊   | 682/1001 [39:33<18:29,  3.48s/it]\u001b[A\n",
            "Iteration: 680 Loss: 4.1515168190002445 Accuracy: 0.26202595680952073:  68%|██████▊   | 683/1001 [39:37<18:26,  3.48s/it]\u001b[A\n",
            "Iteration: 680 Loss: 4.1515168190002445 Accuracy: 0.26202595680952073:  68%|██████▊   | 684/1001 [39:40<18:21,  3.48s/it]\u001b[A\n",
            "Iteration: 680 Loss: 4.1515168190002445 Accuracy: 0.26202595680952073:  68%|██████▊   | 685/1001 [39:44<18:19,  3.48s/it]\u001b[A\n",
            "Iteration: 680 Loss: 4.1515168190002445 Accuracy: 0.26202595680952073:  69%|██████▊   | 686/1001 [39:47<18:14,  3.47s/it]\u001b[A\n",
            "Iteration: 680 Loss: 4.1515168190002445 Accuracy: 0.26202595680952073:  69%|██████▊   | 687/1001 [39:50<18:13,  3.48s/it]\u001b[A\n",
            "Iteration: 680 Loss: 4.1515168190002445 Accuracy: 0.26202595680952073:  69%|██████▊   | 688/1001 [39:54<18:08,  3.48s/it]\u001b[A\n",
            "Iteration: 680 Loss: 4.1515168190002445 Accuracy: 0.26202595680952073:  69%|██████▉   | 689/1001 [39:57<18:05,  3.48s/it]\u001b[A\n",
            "Iteration: 680 Loss: 4.1515168190002445 Accuracy: 0.26202595680952073:  69%|██████▉   | 690/1001 [40:01<18:00,  3.48s/it]\u001b[A\n",
            "Iteration: 690 Loss: 3.9380449771881105 Accuracy: 0.25786924064159394:  69%|██████▉   | 690/1001 [40:04<18:00,  3.48s/it]\u001b[A\n",
            "Iteration: 690 Loss: 3.9380449771881105 Accuracy: 0.25786924064159394:  69%|██████▉   | 691/1001 [40:04<17:59,  3.48s/it]\u001b[A\n",
            "Iteration: 690 Loss: 3.9380449771881105 Accuracy: 0.25786924064159394:  69%|██████▉   | 692/1001 [40:08<17:54,  3.48s/it]\u001b[A\n",
            "Iteration: 690 Loss: 3.9380449771881105 Accuracy: 0.25786924064159394:  69%|██████▉   | 693/1001 [40:11<17:51,  3.48s/it]\u001b[A\n",
            "Iteration: 690 Loss: 3.9380449771881105 Accuracy: 0.25786924064159394:  69%|██████▉   | 694/1001 [40:15<17:46,  3.47s/it]\u001b[A\n",
            "Iteration: 690 Loss: 3.9380449771881105 Accuracy: 0.25786924064159394:  69%|██████▉   | 695/1001 [40:18<17:43,  3.48s/it]\u001b[A\n",
            "Iteration: 690 Loss: 3.9380449771881105 Accuracy: 0.25786924064159394:  70%|██████▉   | 696/1001 [40:22<17:40,  3.48s/it]\u001b[A\n",
            "Iteration: 690 Loss: 3.9380449771881105 Accuracy: 0.25786924064159394:  70%|██████▉   | 697/1001 [40:25<17:37,  3.48s/it]\u001b[A\n",
            "Iteration: 690 Loss: 3.9380449771881105 Accuracy: 0.25786924064159394:  70%|██████▉   | 698/1001 [40:29<17:35,  3.48s/it]\u001b[A\n",
            "Iteration: 690 Loss: 3.9380449771881105 Accuracy: 0.25786924064159394:  70%|██████▉   | 699/1001 [40:32<17:29,  3.48s/it]\u001b[A\n",
            "Iteration: 690 Loss: 3.9380449771881105 Accuracy: 0.25786924064159394:  70%|██████▉   | 700/1001 [40:36<17:25,  3.47s/it]\u001b[A\n",
            "Iteration: 700 Loss: 3.731574273109436 Accuracy: 0.2597551688551903:  70%|██████▉   | 700/1001 [40:39<17:25,  3.47s/it]  \u001b[A\n",
            "Iteration: 700 Loss: 3.731574273109436 Accuracy: 0.2597551688551903:  70%|███████   | 701/1001 [40:39<17:37,  3.52s/it]\u001b[A\n",
            "Iteration: 700 Loss: 3.731574273109436 Accuracy: 0.2597551688551903:  70%|███████   | 702/1001 [40:43<17:29,  3.51s/it]\u001b[A\n",
            "Iteration: 700 Loss: 3.731574273109436 Accuracy: 0.2597551688551903:  70%|███████   | 703/1001 [40:46<17:22,  3.50s/it]\u001b[A\n",
            "Iteration: 700 Loss: 3.731574273109436 Accuracy: 0.2597551688551903:  70%|███████   | 704/1001 [40:50<17:19,  3.50s/it]\u001b[A\n",
            "Iteration: 700 Loss: 3.731574273109436 Accuracy: 0.2597551688551903:  70%|███████   | 705/1001 [40:53<17:14,  3.50s/it]\u001b[A\n",
            "Iteration: 700 Loss: 3.731574273109436 Accuracy: 0.2597551688551903:  71%|███████   | 706/1001 [40:57<17:10,  3.49s/it]\u001b[A\n",
            "Iteration: 700 Loss: 3.731574273109436 Accuracy: 0.2597551688551903:  71%|███████   | 707/1001 [41:00<17:06,  3.49s/it]\u001b[A\n",
            "Iteration: 700 Loss: 3.731574273109436 Accuracy: 0.2597551688551903:  71%|███████   | 708/1001 [41:04<17:01,  3.49s/it]\u001b[A\n",
            "Iteration: 700 Loss: 3.731574273109436 Accuracy: 0.2597551688551903:  71%|███████   | 709/1001 [41:07<16:56,  3.48s/it]\u001b[A\n",
            "Iteration: 700 Loss: 3.731574273109436 Accuracy: 0.2597551688551903:  71%|███████   | 710/1001 [41:11<16:52,  3.48s/it]\u001b[A\n",
            "Iteration: 710 Loss: 4.1648119449615475 Accuracy: 0.24734813272953032:  71%|███████   | 710/1001 [41:14<16:52,  3.48s/it]\u001b[A\n",
            "Iteration: 710 Loss: 4.1648119449615475 Accuracy: 0.24734813272953032:  71%|███████   | 711/1001 [41:14<16:51,  3.49s/it]\u001b[A\n",
            "Iteration: 710 Loss: 4.1648119449615475 Accuracy: 0.24734813272953032:  71%|███████   | 712/1001 [41:18<16:47,  3.48s/it]\u001b[A\n",
            "Iteration: 710 Loss: 4.1648119449615475 Accuracy: 0.24734813272953032:  71%|███████   | 713/1001 [41:21<16:41,  3.48s/it]\u001b[A\n",
            "Iteration: 710 Loss: 4.1648119449615475 Accuracy: 0.24734813272953032:  71%|███████▏  | 714/1001 [41:25<16:37,  3.48s/it]\u001b[A\n",
            "Iteration: 710 Loss: 4.1648119449615475 Accuracy: 0.24734813272953032:  71%|███████▏  | 715/1001 [41:28<16:36,  3.49s/it]\u001b[A\n",
            "Iteration: 710 Loss: 4.1648119449615475 Accuracy: 0.24734813272953032:  72%|███████▏  | 716/1001 [41:32<16:34,  3.49s/it]\u001b[A\n",
            "Iteration: 710 Loss: 4.1648119449615475 Accuracy: 0.24734813272953032:  72%|███████▏  | 717/1001 [41:35<16:29,  3.48s/it]\u001b[A\n",
            "Iteration: 710 Loss: 4.1648119449615475 Accuracy: 0.24734813272953032:  72%|███████▏  | 718/1001 [41:39<16:25,  3.48s/it]\u001b[A\n",
            "Iteration: 710 Loss: 4.1648119449615475 Accuracy: 0.24734813272953032:  72%|███████▏  | 719/1001 [41:42<16:20,  3.48s/it]\u001b[A\n",
            "Iteration: 710 Loss: 4.1648119449615475 Accuracy: 0.24734813272953032:  72%|███████▏  | 720/1001 [41:45<16:17,  3.48s/it]\u001b[A\n",
            "Iteration: 720 Loss: 4.160605549812317 Accuracy: 0.26749384999275205:  72%|███████▏  | 720/1001 [41:49<16:17,  3.48s/it] \u001b[A\n",
            "Iteration: 720 Loss: 4.160605549812317 Accuracy: 0.26749384999275205:  72%|███████▏  | 721/1001 [41:49<16:14,  3.48s/it]\u001b[A\n",
            "Iteration: 720 Loss: 4.160605549812317 Accuracy: 0.26749384999275205:  72%|███████▏  | 722/1001 [41:52<16:10,  3.48s/it]\u001b[A\n",
            "Iteration: 720 Loss: 4.160605549812317 Accuracy: 0.26749384999275205:  72%|███████▏  | 723/1001 [41:56<16:06,  3.48s/it]\u001b[A\n",
            "Iteration: 720 Loss: 4.160605549812317 Accuracy: 0.26749384999275205:  72%|███████▏  | 724/1001 [41:59<16:03,  3.48s/it]\u001b[A\n",
            "Iteration: 720 Loss: 4.160605549812317 Accuracy: 0.26749384999275205:  72%|███████▏  | 725/1001 [42:03<16:02,  3.49s/it]\u001b[A\n",
            "Iteration: 720 Loss: 4.160605549812317 Accuracy: 0.26749384999275205:  73%|███████▎  | 726/1001 [42:06<15:58,  3.49s/it]\u001b[A\n",
            "Iteration: 720 Loss: 4.160605549812317 Accuracy: 0.26749384999275205:  73%|███████▎  | 727/1001 [42:10<15:53,  3.48s/it]\u001b[A\n",
            "Iteration: 720 Loss: 4.160605549812317 Accuracy: 0.26749384999275205:  73%|███████▎  | 728/1001 [42:13<15:50,  3.48s/it]\u001b[A\n",
            "Iteration: 720 Loss: 4.160605549812317 Accuracy: 0.26749384999275205:  73%|███████▎  | 729/1001 [42:17<15:45,  3.48s/it]\u001b[A\n",
            "Iteration: 720 Loss: 4.160605549812317 Accuracy: 0.26749384999275205:  73%|███████▎  | 730/1001 [42:20<15:42,  3.48s/it]\u001b[A\n",
            "Iteration: 730 Loss: 4.022788691520691 Accuracy: 0.25718455463647844:  73%|███████▎  | 730/1001 [42:24<15:42,  3.48s/it]\u001b[A\n",
            "Iteration: 730 Loss: 4.022788691520691 Accuracy: 0.25718455463647844:  73%|███████▎  | 731/1001 [42:24<15:40,  3.48s/it]\u001b[A\n",
            "Iteration: 730 Loss: 4.022788691520691 Accuracy: 0.25718455463647844:  73%|███████▎  | 732/1001 [42:27<15:41,  3.50s/it]\u001b[A\n",
            "Iteration: 730 Loss: 4.022788691520691 Accuracy: 0.25718455463647844:  73%|███████▎  | 733/1001 [42:31<15:36,  3.49s/it]\u001b[A\n",
            "Iteration: 730 Loss: 4.022788691520691 Accuracy: 0.25718455463647844:  73%|███████▎  | 734/1001 [42:34<15:31,  3.49s/it]\u001b[A\n",
            "Iteration: 730 Loss: 4.022788691520691 Accuracy: 0.25718455463647844:  73%|███████▎  | 735/1001 [42:38<15:27,  3.49s/it]\u001b[A\n",
            "Iteration: 730 Loss: 4.022788691520691 Accuracy: 0.25718455463647844:  74%|███████▎  | 736/1001 [42:41<15:22,  3.48s/it]\u001b[A\n",
            "Iteration: 730 Loss: 4.022788691520691 Accuracy: 0.25718455463647844:  74%|███████▎  | 737/1001 [42:45<15:19,  3.48s/it]\u001b[A\n",
            "Iteration: 730 Loss: 4.022788691520691 Accuracy: 0.25718455463647844:  74%|███████▎  | 738/1001 [42:48<15:14,  3.48s/it]\u001b[A\n",
            "Iteration: 730 Loss: 4.022788691520691 Accuracy: 0.25718455463647844:  74%|███████▍  | 739/1001 [42:52<15:12,  3.48s/it]\u001b[A\n",
            "Iteration: 730 Loss: 4.022788691520691 Accuracy: 0.25718455463647844:  74%|███████▍  | 740/1001 [42:55<15:07,  3.48s/it]\u001b[A\n",
            "Iteration: 740 Loss: 3.8083114862442016 Accuracy: 0.26848756819963454:  74%|███████▍  | 740/1001 [42:59<15:07,  3.48s/it]\u001b[A\n",
            "Iteration: 740 Loss: 3.8083114862442016 Accuracy: 0.26848756819963454:  74%|███████▍  | 741/1001 [42:59<15:03,  3.48s/it]\u001b[A\n",
            "Iteration: 740 Loss: 3.8083114862442016 Accuracy: 0.26848756819963454:  74%|███████▍  | 742/1001 [43:02<15:01,  3.48s/it]\u001b[A\n",
            "Iteration: 740 Loss: 3.8083114862442016 Accuracy: 0.26848756819963454:  74%|███████▍  | 743/1001 [43:06<14:57,  3.48s/it]\u001b[A\n",
            "Iteration: 740 Loss: 3.8083114862442016 Accuracy: 0.26848756819963454:  74%|███████▍  | 744/1001 [43:09<14:54,  3.48s/it]\u001b[A\n",
            "Iteration: 740 Loss: 3.8083114862442016 Accuracy: 0.26848756819963454:  74%|███████▍  | 745/1001 [43:13<14:50,  3.48s/it]\u001b[A\n",
            "Iteration: 740 Loss: 3.8083114862442016 Accuracy: 0.26848756819963454:  75%|███████▍  | 746/1001 [43:16<14:46,  3.48s/it]\u001b[A\n",
            "Iteration: 740 Loss: 3.8083114862442016 Accuracy: 0.26848756819963454:  75%|███████▍  | 747/1001 [43:19<14:42,  3.47s/it]\u001b[A\n",
            "Iteration: 740 Loss: 3.8083114862442016 Accuracy: 0.26848756819963454:  75%|███████▍  | 748/1001 [43:23<14:38,  3.47s/it]\u001b[A\n",
            "Iteration: 740 Loss: 3.8083114862442016 Accuracy: 0.26848756819963454:  75%|███████▍  | 749/1001 [43:26<14:35,  3.48s/it]\u001b[A\n",
            "Iteration: 740 Loss: 3.8083114862442016 Accuracy: 0.26848756819963454:  75%|███████▍  | 750/1001 [43:30<14:31,  3.47s/it]\u001b[A\n",
            "Iteration: 750 Loss: 3.773479676246643 Accuracy: 0.2620183750987053:  75%|███████▍  | 750/1001 [43:33<14:31,  3.47s/it]  \u001b[A\n",
            "Iteration: 750 Loss: 3.773479676246643 Accuracy: 0.2620183750987053:  75%|███████▌  | 751/1001 [43:33<14:28,  3.47s/it]\u001b[A\n",
            "Iteration: 750 Loss: 3.773479676246643 Accuracy: 0.2620183750987053:  75%|███████▌  | 752/1001 [43:37<14:25,  3.47s/it]\u001b[A\n",
            "Iteration: 750 Loss: 3.773479676246643 Accuracy: 0.2620183750987053:  75%|███████▌  | 753/1001 [43:40<14:22,  3.48s/it]\u001b[A\n",
            "Iteration: 750 Loss: 3.773479676246643 Accuracy: 0.2620183750987053:  75%|███████▌  | 754/1001 [43:44<14:18,  3.48s/it]\u001b[A\n",
            "Iteration: 750 Loss: 3.773479676246643 Accuracy: 0.2620183750987053:  75%|███████▌  | 755/1001 [43:47<14:14,  3.47s/it]\u001b[A\n",
            "Iteration: 750 Loss: 3.773479676246643 Accuracy: 0.2620183750987053:  76%|███████▌  | 756/1001 [43:51<14:10,  3.47s/it]\u001b[A\n",
            "Iteration: 750 Loss: 3.773479676246643 Accuracy: 0.2620183750987053:  76%|███████▌  | 757/1001 [43:54<14:06,  3.47s/it]\u001b[A\n",
            "Iteration: 750 Loss: 3.773479676246643 Accuracy: 0.2620183750987053:  76%|███████▌  | 758/1001 [43:58<14:03,  3.47s/it]\u001b[A\n",
            "Iteration: 750 Loss: 3.773479676246643 Accuracy: 0.2620183750987053:  76%|███████▌  | 759/1001 [44:01<14:00,  3.47s/it]\u001b[A\n",
            "Iteration: 750 Loss: 3.773479676246643 Accuracy: 0.2620183750987053:  76%|███████▌  | 760/1001 [44:05<13:57,  3.47s/it]\u001b[A\n",
            "Iteration: 760 Loss: 4.015483236312866 Accuracy: 0.2593192607164383:  76%|███████▌  | 760/1001 [44:08<13:57,  3.47s/it]\u001b[A\n",
            "Iteration: 760 Loss: 4.015483236312866 Accuracy: 0.2593192607164383:  76%|███████▌  | 761/1001 [44:08<13:53,  3.47s/it]\u001b[A\n",
            "Iteration: 760 Loss: 4.015483236312866 Accuracy: 0.2593192607164383:  76%|███████▌  | 762/1001 [44:12<13:50,  3.48s/it]\u001b[A\n",
            "Iteration: 760 Loss: 4.015483236312866 Accuracy: 0.2593192607164383:  76%|███████▌  | 763/1001 [44:15<13:50,  3.49s/it]\u001b[A\n",
            "Iteration: 760 Loss: 4.015483236312866 Accuracy: 0.2593192607164383:  76%|███████▋  | 764/1001 [44:19<13:46,  3.49s/it]\u001b[A\n",
            "Iteration: 760 Loss: 4.015483236312866 Accuracy: 0.2593192607164383:  76%|███████▋  | 765/1001 [44:22<13:40,  3.48s/it]\u001b[A\n",
            "Iteration: 760 Loss: 4.015483236312866 Accuracy: 0.2593192607164383:  77%|███████▋  | 766/1001 [44:25<13:36,  3.47s/it]\u001b[A\n",
            "Iteration: 760 Loss: 4.015483236312866 Accuracy: 0.2593192607164383:  77%|███████▋  | 767/1001 [44:29<13:32,  3.47s/it]\u001b[A\n",
            "Iteration: 760 Loss: 4.015483236312866 Accuracy: 0.2593192607164383:  77%|███████▋  | 768/1001 [44:32<13:29,  3.47s/it]\u001b[A\n",
            "Iteration: 760 Loss: 4.015483236312866 Accuracy: 0.2593192607164383:  77%|███████▋  | 769/1001 [44:36<13:25,  3.47s/it]\u001b[A\n",
            "Iteration: 760 Loss: 4.015483236312866 Accuracy: 0.2593192607164383:  77%|███████▋  | 770/1001 [44:39<13:22,  3.47s/it]\u001b[A\n",
            "Iteration: 770 Loss: 3.48223671913147 Accuracy: 0.26693185120821:  77%|███████▋  | 770/1001 [44:43<13:22,  3.47s/it]   \u001b[A\n",
            "Iteration: 770 Loss: 3.48223671913147 Accuracy: 0.26693185120821:  77%|███████▋  | 771/1001 [44:43<13:19,  3.47s/it]\u001b[A\n",
            "Iteration: 770 Loss: 3.48223671913147 Accuracy: 0.26693185120821:  77%|███████▋  | 772/1001 [44:46<13:15,  3.47s/it]\u001b[A\n",
            "Iteration: 770 Loss: 3.48223671913147 Accuracy: 0.26693185120821:  77%|███████▋  | 773/1001 [44:50<13:11,  3.47s/it]\u001b[A\n",
            "Iteration: 770 Loss: 3.48223671913147 Accuracy: 0.26693185120821:  77%|███████▋  | 774/1001 [44:53<13:08,  3.48s/it]\u001b[A\n",
            "Iteration: 770 Loss: 3.48223671913147 Accuracy: 0.26693185120821:  77%|███████▋  | 775/1001 [44:57<13:05,  3.48s/it]\u001b[A\n",
            "Iteration: 770 Loss: 3.48223671913147 Accuracy: 0.26693185120821:  78%|███████▊  | 776/1001 [45:00<13:02,  3.48s/it]\u001b[A\n",
            "Iteration: 770 Loss: 3.48223671913147 Accuracy: 0.26693185120821:  78%|███████▊  | 777/1001 [45:04<12:59,  3.48s/it]\u001b[A\n",
            "Iteration: 770 Loss: 3.48223671913147 Accuracy: 0.26693185120821:  78%|███████▊  | 778/1001 [45:07<12:55,  3.48s/it]\u001b[A\n",
            "Iteration: 770 Loss: 3.48223671913147 Accuracy: 0.26693185120821:  78%|███████▊  | 779/1001 [45:11<12:51,  3.48s/it]\u001b[A\n",
            "Iteration: 770 Loss: 3.48223671913147 Accuracy: 0.26693185120821:  78%|███████▊  | 780/1001 [45:14<12:49,  3.48s/it]\u001b[A\n",
            "Iteration: 780 Loss: 3.625439476966858 Accuracy: 0.26176613569259644:  78%|███████▊  | 780/1001 [45:18<12:49,  3.48s/it]\u001b[A\n",
            "Iteration: 780 Loss: 3.625439476966858 Accuracy: 0.26176613569259644:  78%|███████▊  | 781/1001 [45:18<12:46,  3.48s/it]\u001b[A\n",
            "Iteration: 780 Loss: 3.625439476966858 Accuracy: 0.26176613569259644:  78%|███████▊  | 782/1001 [45:21<12:46,  3.50s/it]\u001b[A\n",
            "Iteration: 780 Loss: 3.625439476966858 Accuracy: 0.26176613569259644:  78%|███████▊  | 783/1001 [45:25<12:42,  3.50s/it]\u001b[A\n",
            "Iteration: 780 Loss: 3.625439476966858 Accuracy: 0.26176613569259644:  78%|███████▊  | 784/1001 [45:28<12:40,  3.51s/it]\u001b[A\n",
            "Iteration: 780 Loss: 3.625439476966858 Accuracy: 0.26176613569259644:  78%|███████▊  | 785/1001 [45:32<12:35,  3.50s/it]\u001b[A\n",
            "Iteration: 780 Loss: 3.625439476966858 Accuracy: 0.26176613569259644:  79%|███████▊  | 786/1001 [45:35<12:33,  3.50s/it]\u001b[A\n",
            "Iteration: 780 Loss: 3.625439476966858 Accuracy: 0.26176613569259644:  79%|███████▊  | 787/1001 [45:39<12:29,  3.50s/it]\u001b[A\n",
            "Iteration: 780 Loss: 3.625439476966858 Accuracy: 0.26176613569259644:  79%|███████▊  | 788/1001 [45:42<12:23,  3.49s/it]\u001b[A\n",
            "Iteration: 780 Loss: 3.625439476966858 Accuracy: 0.26176613569259644:  79%|███████▉  | 789/1001 [45:46<12:18,  3.49s/it]\u001b[A\n",
            "Iteration: 780 Loss: 3.625439476966858 Accuracy: 0.26176613569259644:  79%|███████▉  | 790/1001 [45:49<12:15,  3.48s/it]\u001b[A\n",
            "Iteration: 790 Loss: 3.283496308326721 Accuracy: 0.26913743615150454:  79%|███████▉  | 790/1001 [45:53<12:15,  3.48s/it]\u001b[A\n",
            "Iteration: 790 Loss: 3.283496308326721 Accuracy: 0.26913743615150454:  79%|███████▉  | 791/1001 [45:53<12:12,  3.49s/it]\u001b[A\n",
            "Iteration: 790 Loss: 3.283496308326721 Accuracy: 0.26913743615150454:  79%|███████▉  | 792/1001 [45:56<12:09,  3.49s/it]\u001b[A\n",
            "Iteration: 790 Loss: 3.283496308326721 Accuracy: 0.26913743615150454:  79%|███████▉  | 793/1001 [46:00<12:07,  3.50s/it]\u001b[A\n",
            "Iteration: 790 Loss: 3.283496308326721 Accuracy: 0.26913743615150454:  79%|███████▉  | 794/1001 [46:03<12:05,  3.51s/it]\u001b[A\n",
            "Iteration: 790 Loss: 3.283496308326721 Accuracy: 0.26913743615150454:  79%|███████▉  | 795/1001 [46:07<11:59,  3.50s/it]\u001b[A\n",
            "Iteration: 790 Loss: 3.283496308326721 Accuracy: 0.26913743615150454:  80%|███████▉  | 796/1001 [46:10<11:56,  3.49s/it]\u001b[A\n",
            "Iteration: 790 Loss: 3.283496308326721 Accuracy: 0.26913743615150454:  80%|███████▉  | 797/1001 [46:14<11:55,  3.51s/it]\u001b[A\n",
            "Iteration: 790 Loss: 3.283496308326721 Accuracy: 0.26913743615150454:  80%|███████▉  | 798/1001 [46:17<11:49,  3.50s/it]\u001b[A\n",
            "Iteration: 790 Loss: 3.283496308326721 Accuracy: 0.26913743615150454:  80%|███████▉  | 799/1001 [46:21<11:46,  3.50s/it]\u001b[A\n",
            "Iteration: 790 Loss: 3.283496308326721 Accuracy: 0.26913743615150454:  80%|███████▉  | 800/1001 [46:24<11:41,  3.49s/it]\u001b[A\n",
            "Iteration: 800 Loss: 3.7822344064712525 Accuracy: 0.2550942346453667:  80%|███████▉  | 800/1001 [46:28<11:41,  3.49s/it]\u001b[A\n",
            "Iteration: 800 Loss: 3.7822344064712525 Accuracy: 0.2550942346453667:  80%|████████  | 801/1001 [46:28<11:46,  3.53s/it]\u001b[A\n",
            "Iteration: 800 Loss: 3.7822344064712525 Accuracy: 0.2550942346453667:  80%|████████  | 802/1001 [46:31<11:39,  3.52s/it]\u001b[A\n",
            "Iteration: 800 Loss: 3.7822344064712525 Accuracy: 0.2550942346453667:  80%|████████  | 803/1001 [46:35<11:34,  3.51s/it]\u001b[A\n",
            "Iteration: 800 Loss: 3.7822344064712525 Accuracy: 0.2550942346453667:  80%|████████  | 804/1001 [46:38<11:29,  3.50s/it]\u001b[A\n",
            "Iteration: 800 Loss: 3.7822344064712525 Accuracy: 0.2550942346453667:  80%|████████  | 805/1001 [46:42<11:25,  3.50s/it]\u001b[A\n",
            "Iteration: 800 Loss: 3.7822344064712525 Accuracy: 0.2550942346453667:  81%|████████  | 806/1001 [46:45<11:20,  3.49s/it]\u001b[A\n",
            "Iteration: 800 Loss: 3.7822344064712525 Accuracy: 0.2550942346453667:  81%|████████  | 807/1001 [46:49<11:16,  3.49s/it]\u001b[A\n",
            "Iteration: 800 Loss: 3.7822344064712525 Accuracy: 0.2550942346453667:  81%|████████  | 808/1001 [46:52<11:12,  3.48s/it]\u001b[A\n",
            "Iteration: 800 Loss: 3.7822344064712525 Accuracy: 0.2550942346453667:  81%|████████  | 809/1001 [46:56<11:08,  3.48s/it]\u001b[A\n",
            "Iteration: 800 Loss: 3.7822344064712525 Accuracy: 0.2550942346453667:  81%|████████  | 810/1001 [46:59<11:04,  3.48s/it]\u001b[A\n",
            "Iteration: 810 Loss: 3.9598307371139527 Accuracy: 0.23195940107107163:  81%|████████  | 810/1001 [47:03<11:04,  3.48s/it]\u001b[A\n",
            "Iteration: 810 Loss: 3.9598307371139527 Accuracy: 0.23195940107107163:  81%|████████  | 811/1001 [47:03<11:01,  3.48s/it]\u001b[A\n",
            "Iteration: 810 Loss: 3.9598307371139527 Accuracy: 0.23195940107107163:  81%|████████  | 812/1001 [47:06<10:57,  3.48s/it]\u001b[A\n",
            "Iteration: 810 Loss: 3.9598307371139527 Accuracy: 0.23195940107107163:  81%|████████  | 813/1001 [47:09<10:53,  3.48s/it]\u001b[A\n",
            "Iteration: 810 Loss: 3.9598307371139527 Accuracy: 0.23195940107107163:  81%|████████▏ | 814/1001 [47:13<10:50,  3.48s/it]\u001b[A\n",
            "Iteration: 810 Loss: 3.9598307371139527 Accuracy: 0.23195940107107163:  81%|████████▏ | 815/1001 [47:16<10:46,  3.48s/it]\u001b[A\n",
            "Iteration: 810 Loss: 3.9598307371139527 Accuracy: 0.23195940107107163:  82%|████████▏ | 816/1001 [47:20<10:43,  3.48s/it]\u001b[A\n",
            "Iteration: 810 Loss: 3.9598307371139527 Accuracy: 0.23195940107107163:  82%|████████▏ | 817/1001 [47:23<10:40,  3.48s/it]\u001b[A\n",
            "Iteration: 810 Loss: 3.9598307371139527 Accuracy: 0.23195940107107163:  82%|████████▏ | 818/1001 [47:27<10:37,  3.48s/it]\u001b[A\n",
            "Iteration: 810 Loss: 3.9598307371139527 Accuracy: 0.23195940107107163:  82%|████████▏ | 819/1001 [47:30<10:34,  3.49s/it]\u001b[A\n",
            "Iteration: 810 Loss: 3.9598307371139527 Accuracy: 0.23195940107107163:  82%|████████▏ | 820/1001 [47:34<10:31,  3.49s/it]\u001b[A\n",
            "Iteration: 820 Loss: 3.9437267541885377 Accuracy: 0.2632201507687569:  82%|████████▏ | 820/1001 [47:37<10:31,  3.49s/it] \u001b[A\n",
            "Iteration: 820 Loss: 3.9437267541885377 Accuracy: 0.2632201507687569:  82%|████████▏ | 821/1001 [47:37<10:27,  3.49s/it]\u001b[A\n",
            "Iteration: 820 Loss: 3.9437267541885377 Accuracy: 0.2632201507687569:  82%|████████▏ | 822/1001 [47:41<10:24,  3.49s/it]\u001b[A\n",
            "Iteration: 820 Loss: 3.9437267541885377 Accuracy: 0.2632201507687569:  82%|████████▏ | 823/1001 [47:44<10:19,  3.48s/it]\u001b[A\n",
            "Iteration: 820 Loss: 3.9437267541885377 Accuracy: 0.2632201507687569:  82%|████████▏ | 824/1001 [47:48<10:16,  3.48s/it]\u001b[A\n",
            "Iteration: 820 Loss: 3.9437267541885377 Accuracy: 0.2632201507687569:  82%|████████▏ | 825/1001 [47:51<10:12,  3.48s/it]\u001b[A\n",
            "Iteration: 820 Loss: 3.9437267541885377 Accuracy: 0.2632201507687569:  83%|████████▎ | 826/1001 [47:55<10:09,  3.48s/it]\u001b[A\n",
            "Iteration: 820 Loss: 3.9437267541885377 Accuracy: 0.2632201507687569:  83%|████████▎ | 827/1001 [47:58<10:05,  3.48s/it]\u001b[A\n",
            "Iteration: 820 Loss: 3.9437267541885377 Accuracy: 0.2632201507687569:  83%|████████▎ | 828/1001 [48:02<10:03,  3.49s/it]\u001b[A\n",
            "Iteration: 820 Loss: 3.9437267541885377 Accuracy: 0.2632201507687569:  83%|████████▎ | 829/1001 [48:05<10:00,  3.49s/it]\u001b[A\n",
            "Iteration: 820 Loss: 3.9437267541885377 Accuracy: 0.2632201507687569:  83%|████████▎ | 830/1001 [48:09<09:56,  3.49s/it]\u001b[A\n",
            "Iteration: 830 Loss: 3.518622064590454 Accuracy: 0.26716658025979995:  83%|████████▎ | 830/1001 [48:12<09:56,  3.49s/it]\u001b[A\n",
            "Iteration: 830 Loss: 3.518622064590454 Accuracy: 0.26716658025979995:  83%|████████▎ | 831/1001 [48:12<09:52,  3.49s/it]\u001b[A\n",
            "Iteration: 830 Loss: 3.518622064590454 Accuracy: 0.26716658025979995:  83%|████████▎ | 832/1001 [48:16<09:48,  3.48s/it]\u001b[A\n",
            "Iteration: 830 Loss: 3.518622064590454 Accuracy: 0.26716658025979995:  83%|████████▎ | 833/1001 [48:19<09:45,  3.48s/it]\u001b[A\n",
            "Iteration: 830 Loss: 3.518622064590454 Accuracy: 0.26716658025979995:  83%|████████▎ | 834/1001 [48:23<09:41,  3.48s/it]\u001b[A\n",
            "Iteration: 830 Loss: 3.518622064590454 Accuracy: 0.26716658025979995:  83%|████████▎ | 835/1001 [48:26<09:36,  3.47s/it]\u001b[A\n",
            "Iteration: 830 Loss: 3.518622064590454 Accuracy: 0.26716658025979995:  84%|████████▎ | 836/1001 [48:30<09:33,  3.47s/it]\u001b[A\n",
            "Iteration: 830 Loss: 3.518622064590454 Accuracy: 0.26716658025979995:  84%|████████▎ | 837/1001 [48:33<09:29,  3.47s/it]\u001b[A\n",
            "Iteration: 830 Loss: 3.518622064590454 Accuracy: 0.26716658025979995:  84%|████████▎ | 838/1001 [48:36<09:25,  3.47s/it]\u001b[A\n",
            "Iteration: 830 Loss: 3.518622064590454 Accuracy: 0.26716658025979995:  84%|████████▍ | 839/1001 [48:40<09:22,  3.47s/it]\u001b[A\n",
            "Iteration: 830 Loss: 3.518622064590454 Accuracy: 0.26716658025979995:  84%|████████▍ | 840/1001 [48:43<09:19,  3.47s/it]\u001b[A\n",
            "Iteration: 840 Loss: 3.7939991474151613 Accuracy: 0.2644888788461685:  84%|████████▍ | 840/1001 [48:47<09:19,  3.47s/it]\u001b[A\n",
            "Iteration: 840 Loss: 3.7939991474151613 Accuracy: 0.2644888788461685:  84%|████████▍ | 841/1001 [48:47<09:16,  3.48s/it]\u001b[A\n",
            "Iteration: 840 Loss: 3.7939991474151613 Accuracy: 0.2644888788461685:  84%|████████▍ | 842/1001 [48:50<09:13,  3.48s/it]\u001b[A\n",
            "Iteration: 840 Loss: 3.7939991474151613 Accuracy: 0.2644888788461685:  84%|████████▍ | 843/1001 [48:54<09:09,  3.48s/it]\u001b[A\n",
            "Iteration: 840 Loss: 3.7939991474151613 Accuracy: 0.2644888788461685:  84%|████████▍ | 844/1001 [48:57<09:05,  3.48s/it]\u001b[A\n",
            "Iteration: 840 Loss: 3.7939991474151613 Accuracy: 0.2644888788461685:  84%|████████▍ | 845/1001 [49:01<09:02,  3.47s/it]\u001b[A\n",
            "Iteration: 840 Loss: 3.7939991474151613 Accuracy: 0.2644888788461685:  85%|████████▍ | 846/1001 [49:04<08:58,  3.48s/it]\u001b[A\n",
            "Iteration: 840 Loss: 3.7939991474151613 Accuracy: 0.2644888788461685:  85%|████████▍ | 847/1001 [49:08<08:56,  3.49s/it]\u001b[A\n",
            "Iteration: 840 Loss: 3.7939991474151613 Accuracy: 0.2644888788461685:  85%|████████▍ | 848/1001 [49:11<08:54,  3.49s/it]\u001b[A\n",
            "Iteration: 840 Loss: 3.7939991474151613 Accuracy: 0.2644888788461685:  85%|████████▍ | 849/1001 [49:15<08:49,  3.49s/it]\u001b[A\n",
            "Iteration: 840 Loss: 3.7939991474151613 Accuracy: 0.2644888788461685:  85%|████████▍ | 850/1001 [49:18<08:45,  3.48s/it]\u001b[A\n",
            "Iteration: 850 Loss: 3.6633285522460937 Accuracy: 0.27126877307891845:  85%|████████▍ | 850/1001 [49:22<08:45,  3.48s/it]\u001b[A\n",
            "Iteration: 850 Loss: 3.6633285522460937 Accuracy: 0.27126877307891845:  85%|████████▌ | 851/1001 [49:22<08:42,  3.49s/it]\u001b[A\n",
            "Iteration: 850 Loss: 3.6633285522460937 Accuracy: 0.27126877307891845:  85%|████████▌ | 852/1001 [49:25<08:39,  3.49s/it]\u001b[A\n",
            "Iteration: 850 Loss: 3.6633285522460937 Accuracy: 0.27126877307891845:  85%|████████▌ | 853/1001 [49:29<08:36,  3.49s/it]\u001b[A\n",
            "Iteration: 850 Loss: 3.6633285522460937 Accuracy: 0.27126877307891845:  85%|████████▌ | 854/1001 [49:32<08:34,  3.50s/it]\u001b[A\n",
            "Iteration: 850 Loss: 3.6633285522460937 Accuracy: 0.27126877307891845:  85%|████████▌ | 855/1001 [49:36<08:30,  3.50s/it]\u001b[A\n",
            "Iteration: 850 Loss: 3.6633285522460937 Accuracy: 0.27126877307891845:  86%|████████▌ | 856/1001 [49:39<08:26,  3.49s/it]\u001b[A\n",
            "Iteration: 850 Loss: 3.6633285522460937 Accuracy: 0.27126877307891845:  86%|████████▌ | 857/1001 [49:43<08:22,  3.49s/it]\u001b[A\n",
            "Iteration: 850 Loss: 3.6633285522460937 Accuracy: 0.27126877307891845:  86%|████████▌ | 858/1001 [49:46<08:19,  3.49s/it]\u001b[A\n",
            "Iteration: 850 Loss: 3.6633285522460937 Accuracy: 0.27126877307891845:  86%|████████▌ | 859/1001 [49:50<08:15,  3.49s/it]\u001b[A\n",
            "Iteration: 850 Loss: 3.6633285522460937 Accuracy: 0.27126877307891845:  86%|████████▌ | 860/1001 [49:53<08:11,  3.49s/it]\u001b[A\n",
            "Iteration: 860 Loss: 3.511196160316467 Accuracy: 0.270864363014698:  86%|████████▌ | 860/1001 [49:57<08:11,  3.49s/it]   \u001b[A\n",
            "Iteration: 860 Loss: 3.511196160316467 Accuracy: 0.270864363014698:  86%|████████▌ | 861/1001 [49:57<08:08,  3.49s/it]\u001b[A\n",
            "Iteration: 860 Loss: 3.511196160316467 Accuracy: 0.270864363014698:  86%|████████▌ | 862/1001 [50:00<08:04,  3.49s/it]\u001b[A\n",
            "Iteration: 860 Loss: 3.511196160316467 Accuracy: 0.270864363014698:  86%|████████▌ | 863/1001 [50:04<08:01,  3.49s/it]\u001b[A\n",
            "Iteration: 860 Loss: 3.511196160316467 Accuracy: 0.270864363014698:  86%|████████▋ | 864/1001 [50:07<07:56,  3.48s/it]\u001b[A\n",
            "Iteration: 860 Loss: 3.511196160316467 Accuracy: 0.270864363014698:  86%|████████▋ | 865/1001 [50:11<07:52,  3.48s/it]\u001b[A\n",
            "Iteration: 860 Loss: 3.511196160316467 Accuracy: 0.270864363014698:  87%|████████▋ | 866/1001 [50:14<07:48,  3.47s/it]\u001b[A\n",
            "Iteration: 860 Loss: 3.511196160316467 Accuracy: 0.270864363014698:  87%|████████▋ | 867/1001 [50:18<07:46,  3.48s/it]\u001b[A\n",
            "Iteration: 860 Loss: 3.511196160316467 Accuracy: 0.270864363014698:  87%|████████▋ | 868/1001 [50:21<07:42,  3.48s/it]\u001b[A\n",
            "Iteration: 860 Loss: 3.511196160316467 Accuracy: 0.270864363014698:  87%|████████▋ | 869/1001 [50:25<07:41,  3.49s/it]\u001b[A\n",
            "Iteration: 860 Loss: 3.511196160316467 Accuracy: 0.270864363014698:  87%|████████▋ | 870/1001 [50:28<07:37,  3.49s/it]\u001b[A\n",
            "Iteration: 870 Loss: 3.7352405548095704 Accuracy: 0.2719314619898796:  87%|████████▋ | 870/1001 [50:31<07:37,  3.49s/it]\u001b[A\n",
            "Iteration: 870 Loss: 3.7352405548095704 Accuracy: 0.2719314619898796:  87%|████████▋ | 871/1001 [50:32<07:33,  3.49s/it]\u001b[A\n",
            "Iteration: 870 Loss: 3.7352405548095704 Accuracy: 0.2719314619898796:  87%|████████▋ | 872/1001 [50:35<07:29,  3.49s/it]\u001b[A\n",
            "Iteration: 870 Loss: 3.7352405548095704 Accuracy: 0.2719314619898796:  87%|████████▋ | 873/1001 [50:38<07:25,  3.48s/it]\u001b[A\n",
            "Iteration: 870 Loss: 3.7352405548095704 Accuracy: 0.2719314619898796:  87%|████████▋ | 874/1001 [50:42<07:22,  3.48s/it]\u001b[A\n",
            "Iteration: 870 Loss: 3.7352405548095704 Accuracy: 0.2719314619898796:  87%|████████▋ | 875/1001 [50:45<07:18,  3.48s/it]\u001b[A\n",
            "Iteration: 870 Loss: 3.7352405548095704 Accuracy: 0.2719314619898796:  88%|████████▊ | 876/1001 [50:49<07:14,  3.48s/it]\u001b[A\n",
            "Iteration: 870 Loss: 3.7352405548095704 Accuracy: 0.2719314619898796:  88%|████████▊ | 877/1001 [50:52<07:12,  3.49s/it]\u001b[A\n",
            "Iteration: 870 Loss: 3.7352405548095704 Accuracy: 0.2719314619898796:  88%|████████▊ | 878/1001 [50:56<07:08,  3.48s/it]\u001b[A\n",
            "Iteration: 870 Loss: 3.7352405548095704 Accuracy: 0.2719314619898796:  88%|████████▊ | 879/1001 [50:59<07:04,  3.48s/it]\u001b[A\n",
            "Iteration: 870 Loss: 3.7352405548095704 Accuracy: 0.2719314619898796:  88%|████████▊ | 880/1001 [51:03<07:00,  3.48s/it]\u001b[A\n",
            "Iteration: 880 Loss: 3.7557705640792847 Accuracy: 0.2675859615206718:  88%|████████▊ | 880/1001 [51:06<07:00,  3.48s/it]\u001b[A\n",
            "Iteration: 880 Loss: 3.7557705640792847 Accuracy: 0.2675859615206718:  88%|████████▊ | 881/1001 [51:06<06:57,  3.48s/it]\u001b[A\n",
            "Iteration: 880 Loss: 3.7557705640792847 Accuracy: 0.2675859615206718:  88%|████████▊ | 882/1001 [51:10<06:54,  3.48s/it]\u001b[A\n",
            "Iteration: 880 Loss: 3.7557705640792847 Accuracy: 0.2675859615206718:  88%|████████▊ | 883/1001 [51:13<06:50,  3.48s/it]\u001b[A\n",
            "Iteration: 880 Loss: 3.7557705640792847 Accuracy: 0.2675859615206718:  88%|████████▊ | 884/1001 [51:17<06:47,  3.48s/it]\u001b[A\n",
            "Iteration: 880 Loss: 3.7557705640792847 Accuracy: 0.2675859615206718:  88%|████████▊ | 885/1001 [51:20<06:43,  3.48s/it]\u001b[A\n",
            "Iteration: 880 Loss: 3.7557705640792847 Accuracy: 0.2675859615206718:  89%|████████▊ | 886/1001 [51:24<06:41,  3.49s/it]\u001b[A\n",
            "Iteration: 880 Loss: 3.7557705640792847 Accuracy: 0.2675859615206718:  89%|████████▊ | 887/1001 [51:27<06:38,  3.49s/it]\u001b[A\n",
            "Iteration: 880 Loss: 3.7557705640792847 Accuracy: 0.2675859615206718:  89%|████████▊ | 888/1001 [51:31<06:34,  3.49s/it]\u001b[A\n",
            "Iteration: 880 Loss: 3.7557705640792847 Accuracy: 0.2675859615206718:  89%|████████▉ | 889/1001 [51:34<06:30,  3.48s/it]\u001b[A\n",
            "Iteration: 880 Loss: 3.7557705640792847 Accuracy: 0.2675859615206718:  89%|████████▉ | 890/1001 [51:38<06:26,  3.48s/it]\u001b[A\n",
            "Iteration: 890 Loss: 3.8303783893585206 Accuracy: 0.24967689216136932:  89%|████████▉ | 890/1001 [51:41<06:26,  3.48s/it]\u001b[A\n",
            "Iteration: 890 Loss: 3.8303783893585206 Accuracy: 0.24967689216136932:  89%|████████▉ | 891/1001 [51:41<06:23,  3.49s/it]\u001b[A\n",
            "Iteration: 890 Loss: 3.8303783893585206 Accuracy: 0.24967689216136932:  89%|████████▉ | 892/1001 [51:45<06:20,  3.49s/it]\u001b[A\n",
            "Iteration: 890 Loss: 3.8303783893585206 Accuracy: 0.24967689216136932:  89%|████████▉ | 893/1001 [51:48<06:15,  3.48s/it]\u001b[A\n",
            "Iteration: 890 Loss: 3.8303783893585206 Accuracy: 0.24967689216136932:  89%|████████▉ | 894/1001 [51:52<06:12,  3.48s/it]\u001b[A\n",
            "Iteration: 890 Loss: 3.8303783893585206 Accuracy: 0.24967689216136932:  89%|████████▉ | 895/1001 [51:55<06:08,  3.48s/it]\u001b[A\n",
            "Iteration: 890 Loss: 3.8303783893585206 Accuracy: 0.24967689216136932:  90%|████████▉ | 896/1001 [51:59<06:05,  3.48s/it]\u001b[A\n",
            "Iteration: 890 Loss: 3.8303783893585206 Accuracy: 0.24967689216136932:  90%|████████▉ | 897/1001 [52:02<06:01,  3.47s/it]\u001b[A\n",
            "Iteration: 890 Loss: 3.8303783893585206 Accuracy: 0.24967689216136932:  90%|████████▉ | 898/1001 [52:05<05:57,  3.47s/it]\u001b[A\n",
            "Iteration: 890 Loss: 3.8303783893585206 Accuracy: 0.24967689216136932:  90%|████████▉ | 899/1001 [52:09<05:54,  3.48s/it]\u001b[A\n",
            "Iteration: 890 Loss: 3.8303783893585206 Accuracy: 0.24967689216136932:  90%|████████▉ | 900/1001 [52:12<05:50,  3.47s/it]\u001b[A\n",
            "Iteration: 900 Loss: 3.846857118606567 Accuracy: 0.26773046255111693:  90%|████████▉ | 900/1001 [52:16<05:50,  3.47s/it] \u001b[A\n",
            "Iteration: 900 Loss: 3.846857118606567 Accuracy: 0.26773046255111693:  90%|█████████ | 901/1001 [52:16<05:51,  3.51s/it]\u001b[A\n",
            "Iteration: 900 Loss: 3.846857118606567 Accuracy: 0.26773046255111693:  90%|█████████ | 902/1001 [52:20<05:47,  3.51s/it]\u001b[A\n",
            "Iteration: 900 Loss: 3.846857118606567 Accuracy: 0.26773046255111693:  90%|█████████ | 903/1001 [52:23<05:42,  3.50s/it]\u001b[A\n",
            "Iteration: 900 Loss: 3.846857118606567 Accuracy: 0.26773046255111693:  90%|█████████ | 904/1001 [52:26<05:38,  3.49s/it]\u001b[A\n",
            "Iteration: 900 Loss: 3.846857118606567 Accuracy: 0.26773046255111693:  90%|█████████ | 905/1001 [52:30<05:36,  3.50s/it]\u001b[A\n",
            "Iteration: 900 Loss: 3.846857118606567 Accuracy: 0.26773046255111693:  91%|█████████ | 906/1001 [52:33<05:31,  3.49s/it]\u001b[A\n",
            "Iteration: 900 Loss: 3.846857118606567 Accuracy: 0.26773046255111693:  91%|█████████ | 907/1001 [52:37<05:27,  3.49s/it]\u001b[A\n",
            "Iteration: 900 Loss: 3.846857118606567 Accuracy: 0.26773046255111693:  91%|█████████ | 908/1001 [52:40<05:23,  3.48s/it]\u001b[A\n",
            "Iteration: 900 Loss: 3.846857118606567 Accuracy: 0.26773046255111693:  91%|█████████ | 909/1001 [52:44<05:19,  3.48s/it]\u001b[A\n",
            "Iteration: 900 Loss: 3.846857118606567 Accuracy: 0.26773046255111693:  91%|█████████ | 910/1001 [52:47<05:16,  3.48s/it]\u001b[A\n",
            "Iteration: 910 Loss: 3.9129854917526243 Accuracy: 0.2694846913218498:  91%|█████████ | 910/1001 [52:51<05:16,  3.48s/it]\u001b[A\n",
            "Iteration: 910 Loss: 3.9129854917526243 Accuracy: 0.2694846913218498:  91%|█████████ | 911/1001 [52:51<05:13,  3.48s/it]\u001b[A\n",
            "Iteration: 910 Loss: 3.9129854917526243 Accuracy: 0.2694846913218498:  91%|█████████ | 912/1001 [52:54<05:09,  3.48s/it]\u001b[A\n",
            "Iteration: 910 Loss: 3.9129854917526243 Accuracy: 0.2694846913218498:  91%|█████████ | 913/1001 [52:58<05:07,  3.49s/it]\u001b[A\n",
            "Iteration: 910 Loss: 3.9129854917526243 Accuracy: 0.2694846913218498:  91%|█████████▏| 914/1001 [53:01<05:03,  3.49s/it]\u001b[A\n",
            "Iteration: 910 Loss: 3.9129854917526243 Accuracy: 0.2694846913218498:  91%|█████████▏| 915/1001 [53:05<05:00,  3.49s/it]\u001b[A\n",
            "Iteration: 910 Loss: 3.9129854917526243 Accuracy: 0.2694846913218498:  92%|█████████▏| 916/1001 [53:08<04:56,  3.49s/it]\u001b[A\n",
            "Iteration: 910 Loss: 3.9129854917526243 Accuracy: 0.2694846913218498:  92%|█████████▏| 917/1001 [53:12<04:53,  3.49s/it]\u001b[A\n",
            "Iteration: 910 Loss: 3.9129854917526243 Accuracy: 0.2694846913218498:  92%|█████████▏| 918/1001 [53:15<04:49,  3.48s/it]\u001b[A\n",
            "Iteration: 910 Loss: 3.9129854917526243 Accuracy: 0.2694846913218498:  92%|█████████▏| 919/1001 [53:19<04:45,  3.48s/it]\u001b[A\n",
            "Iteration: 910 Loss: 3.9129854917526243 Accuracy: 0.2694846913218498:  92%|█████████▏| 920/1001 [53:22<04:42,  3.48s/it]\u001b[A\n",
            "Iteration: 920 Loss: 3.5722029209136963 Accuracy: 0.2732168987393379:  92%|█████████▏| 920/1001 [53:26<04:42,  3.48s/it]\u001b[A\n",
            "Iteration: 920 Loss: 3.5722029209136963 Accuracy: 0.2732168987393379:  92%|█████████▏| 921/1001 [53:26<04:38,  3.48s/it]\u001b[A\n",
            "Iteration: 920 Loss: 3.5722029209136963 Accuracy: 0.2732168987393379:  92%|█████████▏| 922/1001 [53:29<04:34,  3.48s/it]\u001b[A\n",
            "Iteration: 920 Loss: 3.5722029209136963 Accuracy: 0.2732168987393379:  92%|█████████▏| 923/1001 [53:33<04:31,  3.48s/it]\u001b[A\n",
            "Iteration: 920 Loss: 3.5722029209136963 Accuracy: 0.2732168987393379:  92%|█████████▏| 924/1001 [53:36<04:29,  3.50s/it]\u001b[A\n",
            "Iteration: 920 Loss: 3.5722029209136963 Accuracy: 0.2732168987393379:  92%|█████████▏| 925/1001 [53:40<04:25,  3.50s/it]\u001b[A\n",
            "Iteration: 920 Loss: 3.5722029209136963 Accuracy: 0.2732168987393379:  93%|█████████▎| 926/1001 [53:43<04:21,  3.49s/it]\u001b[A\n",
            "Iteration: 920 Loss: 3.5722029209136963 Accuracy: 0.2732168987393379:  93%|█████████▎| 927/1001 [53:47<04:17,  3.49s/it]\u001b[A\n",
            "Iteration: 920 Loss: 3.5722029209136963 Accuracy: 0.2732168987393379:  93%|█████████▎| 928/1001 [53:50<04:13,  3.48s/it]\u001b[A\n",
            "Iteration: 920 Loss: 3.5722029209136963 Accuracy: 0.2732168987393379:  93%|█████████▎| 929/1001 [53:54<04:10,  3.48s/it]\u001b[A\n",
            "Iteration: 920 Loss: 3.5722029209136963 Accuracy: 0.2732168987393379:  93%|█████████▎| 930/1001 [53:57<04:06,  3.48s/it]\u001b[A\n",
            "Iteration: 930 Loss: 3.6437696695327757 Accuracy: 0.27339243739843366:  93%|█████████▎| 930/1001 [54:01<04:06,  3.48s/it]\u001b[A\n",
            "Iteration: 930 Loss: 3.6437696695327757 Accuracy: 0.27339243739843366:  93%|█████████▎| 931/1001 [54:01<04:03,  3.48s/it]\u001b[A\n",
            "Iteration: 930 Loss: 3.6437696695327757 Accuracy: 0.27339243739843366:  93%|█████████▎| 932/1001 [54:04<03:59,  3.47s/it]\u001b[A\n",
            "Iteration: 930 Loss: 3.6437696695327757 Accuracy: 0.27339243739843366:  93%|█████████▎| 933/1001 [54:08<03:56,  3.48s/it]\u001b[A\n",
            "Iteration: 930 Loss: 3.6437696695327757 Accuracy: 0.27339243739843366:  93%|█████████▎| 934/1001 [54:11<03:52,  3.47s/it]\u001b[A\n",
            "Iteration: 930 Loss: 3.6437696695327757 Accuracy: 0.27339243739843366:  93%|█████████▎| 935/1001 [54:14<03:49,  3.47s/it]\u001b[A\n",
            "Iteration: 930 Loss: 3.6437696695327757 Accuracy: 0.27339243739843366:  94%|█████████▎| 936/1001 [54:18<03:45,  3.47s/it]\u001b[A\n",
            "Iteration: 930 Loss: 3.6437696695327757 Accuracy: 0.27339243739843366:  94%|█████████▎| 937/1001 [54:21<03:42,  3.47s/it]\u001b[A\n",
            "Iteration: 930 Loss: 3.6437696695327757 Accuracy: 0.27339243739843366:  94%|█████████▎| 938/1001 [54:25<03:38,  3.48s/it]\u001b[A\n",
            "Iteration: 930 Loss: 3.6437696695327757 Accuracy: 0.27339243739843366:  94%|█████████▍| 939/1001 [54:28<03:36,  3.49s/it]\u001b[A\n",
            "Iteration: 930 Loss: 3.6437696695327757 Accuracy: 0.27339243739843366:  94%|█████████▍| 940/1001 [54:32<03:32,  3.48s/it]\u001b[A\n",
            "Iteration: 940 Loss: 3.6089953660964964 Accuracy: 0.2819478139281273:  94%|█████████▍| 940/1001 [54:35<03:32,  3.48s/it] \u001b[A\n",
            "Iteration: 940 Loss: 3.6089953660964964 Accuracy: 0.2819478139281273:  94%|█████████▍| 941/1001 [54:35<03:29,  3.49s/it]\u001b[A\n",
            "Iteration: 940 Loss: 3.6089953660964964 Accuracy: 0.2819478139281273:  94%|█████████▍| 942/1001 [54:39<03:25,  3.48s/it]\u001b[A\n",
            "Iteration: 940 Loss: 3.6089953660964964 Accuracy: 0.2819478139281273:  94%|█████████▍| 943/1001 [54:42<03:22,  3.49s/it]\u001b[A\n",
            "Iteration: 940 Loss: 3.6089953660964964 Accuracy: 0.2819478139281273:  94%|█████████▍| 944/1001 [54:46<03:18,  3.48s/it]\u001b[A\n",
            "Iteration: 940 Loss: 3.6089953660964964 Accuracy: 0.2819478139281273:  94%|█████████▍| 945/1001 [54:49<03:14,  3.48s/it]\u001b[A\n",
            "Iteration: 940 Loss: 3.6089953660964964 Accuracy: 0.2819478139281273:  95%|█████████▍| 946/1001 [54:53<03:11,  3.47s/it]\u001b[A\n",
            "Iteration: 940 Loss: 3.6089953660964964 Accuracy: 0.2819478139281273:  95%|█████████▍| 947/1001 [54:56<03:07,  3.48s/it]\u001b[A\n",
            "Iteration: 940 Loss: 3.6089953660964964 Accuracy: 0.2819478139281273:  95%|█████████▍| 948/1001 [55:00<03:04,  3.48s/it]\u001b[A\n",
            "Iteration: 940 Loss: 3.6089953660964964 Accuracy: 0.2819478139281273:  95%|█████████▍| 949/1001 [55:03<03:00,  3.48s/it]\u001b[A\n",
            "Iteration: 940 Loss: 3.6089953660964964 Accuracy: 0.2819478139281273:  95%|█████████▍| 950/1001 [55:07<02:57,  3.47s/it]\u001b[A\n",
            "Iteration: 950 Loss: 3.6320374488830565 Accuracy: 0.2760117471218109:  95%|█████████▍| 950/1001 [55:10<02:57,  3.47s/it]\u001b[A\n",
            "Iteration: 950 Loss: 3.6320374488830565 Accuracy: 0.2760117471218109:  95%|█████████▌| 951/1001 [55:10<02:53,  3.48s/it]\u001b[A\n",
            "Iteration: 950 Loss: 3.6320374488830565 Accuracy: 0.2760117471218109:  95%|█████████▌| 952/1001 [55:14<02:50,  3.48s/it]\u001b[A\n",
            "Iteration: 950 Loss: 3.6320374488830565 Accuracy: 0.2760117471218109:  95%|█████████▌| 953/1001 [55:17<02:46,  3.48s/it]\u001b[A\n",
            "Iteration: 950 Loss: 3.6320374488830565 Accuracy: 0.2760117471218109:  95%|█████████▌| 954/1001 [55:21<02:43,  3.48s/it]\u001b[A\n",
            "Iteration: 950 Loss: 3.6320374488830565 Accuracy: 0.2760117471218109:  95%|█████████▌| 955/1001 [55:24<02:39,  3.47s/it]\u001b[A\n",
            "Iteration: 950 Loss: 3.6320374488830565 Accuracy: 0.2760117471218109:  96%|█████████▌| 956/1001 [55:28<02:36,  3.49s/it]\u001b[A\n",
            "Iteration: 950 Loss: 3.6320374488830565 Accuracy: 0.2760117471218109:  96%|█████████▌| 957/1001 [55:31<02:33,  3.48s/it]\u001b[A\n",
            "Iteration: 950 Loss: 3.6320374488830565 Accuracy: 0.2760117471218109:  96%|█████████▌| 958/1001 [55:35<02:29,  3.48s/it]\u001b[A\n",
            "Iteration: 950 Loss: 3.6320374488830565 Accuracy: 0.2760117471218109:  96%|█████████▌| 959/1001 [55:38<02:26,  3.48s/it]\u001b[A\n",
            "Iteration: 950 Loss: 3.6320374488830565 Accuracy: 0.2760117471218109:  96%|█████████▌| 960/1001 [55:41<02:22,  3.48s/it]\u001b[A\n",
            "Iteration: 960 Loss: 3.5721184492111204 Accuracy: 0.28055390119552615:  96%|█████████▌| 960/1001 [55:45<02:22,  3.48s/it]\u001b[A\n",
            "Iteration: 960 Loss: 3.5721184492111204 Accuracy: 0.28055390119552615:  96%|█████████▌| 961/1001 [55:45<02:19,  3.48s/it]\u001b[A\n",
            "Iteration: 960 Loss: 3.5721184492111204 Accuracy: 0.28055390119552615:  96%|█████████▌| 962/1001 [55:48<02:15,  3.49s/it]\u001b[A\n",
            "Iteration: 960 Loss: 3.5721184492111204 Accuracy: 0.28055390119552615:  96%|█████████▌| 963/1001 [55:52<02:12,  3.49s/it]\u001b[A\n",
            "Iteration: 960 Loss: 3.5721184492111204 Accuracy: 0.28055390119552615:  96%|█████████▋| 964/1001 [55:55<02:09,  3.49s/it]\u001b[A\n",
            "Iteration: 960 Loss: 3.5721184492111204 Accuracy: 0.28055390119552615:  96%|█████████▋| 965/1001 [55:59<02:05,  3.48s/it]\u001b[A\n",
            "Iteration: 960 Loss: 3.5721184492111204 Accuracy: 0.28055390119552615:  97%|█████████▋| 966/1001 [56:02<02:01,  3.48s/it]\u001b[A\n",
            "Iteration: 960 Loss: 3.5721184492111204 Accuracy: 0.28055390119552615:  97%|█████████▋| 967/1001 [56:06<01:58,  3.48s/it]\u001b[A\n",
            "Iteration: 960 Loss: 3.5721184492111204 Accuracy: 0.28055390119552615:  97%|█████████▋| 968/1001 [56:09<01:54,  3.48s/it]\u001b[A\n",
            "Iteration: 960 Loss: 3.5721184492111204 Accuracy: 0.28055390119552615:  97%|█████████▋| 969/1001 [56:13<01:51,  3.48s/it]\u001b[A\n",
            "Iteration: 960 Loss: 3.5721184492111204 Accuracy: 0.28055390119552615:  97%|█████████▋| 970/1001 [56:16<01:47,  3.48s/it]\u001b[A\n",
            "Iteration: 970 Loss: 3.9185213804244996 Accuracy: 0.27915337681770325:  97%|█████████▋| 970/1001 [56:20<01:47,  3.48s/it]\u001b[A\n",
            "Iteration: 970 Loss: 3.9185213804244996 Accuracy: 0.27915337681770325:  97%|█████████▋| 971/1001 [56:20<01:44,  3.48s/it]\u001b[A\n",
            "Iteration: 970 Loss: 3.9185213804244996 Accuracy: 0.27915337681770325:  97%|█████████▋| 972/1001 [56:23<01:41,  3.49s/it]\u001b[A\n",
            "Iteration: 970 Loss: 3.9185213804244996 Accuracy: 0.27915337681770325:  97%|█████████▋| 973/1001 [56:27<01:37,  3.48s/it]\u001b[A\n",
            "Iteration: 970 Loss: 3.9185213804244996 Accuracy: 0.27915337681770325:  97%|█████████▋| 974/1001 [56:30<01:33,  3.48s/it]\u001b[A\n",
            "Iteration: 970 Loss: 3.9185213804244996 Accuracy: 0.27915337681770325:  97%|█████████▋| 975/1001 [56:34<01:30,  3.48s/it]\u001b[A\n",
            "Iteration: 970 Loss: 3.9185213804244996 Accuracy: 0.27915337681770325:  98%|█████████▊| 976/1001 [56:37<01:26,  3.47s/it]\u001b[A\n",
            "Iteration: 970 Loss: 3.9185213804244996 Accuracy: 0.27915337681770325:  98%|█████████▊| 977/1001 [56:41<01:23,  3.48s/it]\u001b[A\n",
            "Iteration: 970 Loss: 3.9185213804244996 Accuracy: 0.27915337681770325:  98%|█████████▊| 978/1001 [56:44<01:19,  3.47s/it]\u001b[A\n",
            "Iteration: 970 Loss: 3.9185213804244996 Accuracy: 0.27915337681770325:  98%|█████████▊| 979/1001 [56:48<01:16,  3.47s/it]\u001b[A\n",
            "Iteration: 970 Loss: 3.9185213804244996 Accuracy: 0.27915337681770325:  98%|█████████▊| 980/1001 [56:51<01:12,  3.47s/it]\u001b[A\n",
            "Iteration: 980 Loss: 3.8157870531082154 Accuracy: 0.27417569756507876:  98%|█████████▊| 980/1001 [56:55<01:12,  3.47s/it]\u001b[A\n",
            "Iteration: 980 Loss: 3.8157870531082154 Accuracy: 0.27417569756507876:  98%|█████████▊| 981/1001 [56:55<01:09,  3.49s/it]\u001b[A\n",
            "Iteration: 980 Loss: 3.8157870531082154 Accuracy: 0.27417569756507876:  98%|█████████▊| 982/1001 [56:58<01:06,  3.49s/it]\u001b[A\n",
            "Iteration: 980 Loss: 3.8157870531082154 Accuracy: 0.27417569756507876:  98%|█████████▊| 983/1001 [57:02<01:02,  3.49s/it]\u001b[A\n",
            "Iteration: 980 Loss: 3.8157870531082154 Accuracy: 0.27417569756507876:  98%|█████████▊| 984/1001 [57:05<00:59,  3.48s/it]\u001b[A\n",
            "Iteration: 980 Loss: 3.8157870531082154 Accuracy: 0.27417569756507876:  98%|█████████▊| 985/1001 [57:09<00:55,  3.48s/it]\u001b[A\n",
            "Iteration: 980 Loss: 3.8157870531082154 Accuracy: 0.27417569756507876:  99%|█████████▊| 986/1001 [57:12<00:52,  3.48s/it]\u001b[A\n",
            "Iteration: 980 Loss: 3.8157870531082154 Accuracy: 0.27417569756507876:  99%|█████████▊| 987/1001 [57:15<00:48,  3.48s/it]\u001b[A\n",
            "Iteration: 980 Loss: 3.8157870531082154 Accuracy: 0.27417569756507876:  99%|█████████▊| 988/1001 [57:19<00:45,  3.48s/it]\u001b[A\n",
            "Iteration: 980 Loss: 3.8157870531082154 Accuracy: 0.27417569756507876:  99%|█████████▉| 989/1001 [57:22<00:41,  3.48s/it]\u001b[A\n",
            "Iteration: 980 Loss: 3.8157870531082154 Accuracy: 0.27417569756507876:  99%|█████████▉| 990/1001 [57:26<00:38,  3.47s/it]\u001b[A\n",
            "Iteration: 990 Loss: 3.5840180635452272 Accuracy: 0.28084503710269926:  99%|█████████▉| 990/1001 [57:29<00:38,  3.47s/it]\u001b[A\n",
            "Iteration: 990 Loss: 3.5840180635452272 Accuracy: 0.28084503710269926:  99%|█████████▉| 991/1001 [57:29<00:34,  3.48s/it]\u001b[A\n",
            "Iteration: 990 Loss: 3.5840180635452272 Accuracy: 0.28084503710269926:  99%|█████████▉| 992/1001 [57:33<00:31,  3.48s/it]\u001b[A\n",
            "Iteration: 990 Loss: 3.5840180635452272 Accuracy: 0.28084503710269926:  99%|█████████▉| 993/1001 [57:36<00:27,  3.48s/it]\u001b[A\n",
            "Iteration: 990 Loss: 3.5840180635452272 Accuracy: 0.28084503710269926:  99%|█████████▉| 994/1001 [57:40<00:24,  3.48s/it]\u001b[A\n",
            "Iteration: 990 Loss: 3.5840180635452272 Accuracy: 0.28084503710269926:  99%|█████████▉| 995/1001 [57:43<00:20,  3.48s/it]\u001b[A\n",
            "Iteration: 990 Loss: 3.5840180635452272 Accuracy: 0.28084503710269926: 100%|█████████▉| 996/1001 [57:47<00:17,  3.48s/it]\u001b[A\n",
            "Iteration: 990 Loss: 3.5840180635452272 Accuracy: 0.28084503710269926: 100%|█████████▉| 997/1001 [57:50<00:13,  3.48s/it]\u001b[A\n",
            "Iteration: 990 Loss: 3.5840180635452272 Accuracy: 0.28084503710269926: 100%|█████████▉| 998/1001 [57:54<00:10,  3.48s/it]\u001b[A\n",
            "Iteration: 990 Loss: 3.5840180635452272 Accuracy: 0.28084503710269926: 100%|█████████▉| 999/1001 [57:57<00:06,  3.48s/it]\u001b[A\n",
            "Iteration: 990 Loss: 3.5840180635452272 Accuracy: 0.28084503710269926: 100%|█████████▉| 1000/1001 [58:01<00:03,  3.50s/it]\u001b[A\n",
            "Iteration: 1000 Loss: 3.5641879558563234 Accuracy: 0.27684404253959655: 100%|█████████▉| 1000/1001 [58:04<00:03,  3.50s/it]\u001b[A\n",
            "Iteration: 1000 Loss: 3.5641879558563234 Accuracy: 0.27684404253959655: 100%|██████████| 1001/1001 [58:04<00:00,  3.48s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo6AXe81FLrP"
      },
      "source": [
        "# Using the Summarization model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsEOd0GXFLrQ"
      },
      "source": [
        "Now that you have trained a Transformer to perform Summarization, we will use the model on news articles from the wild.\n",
        "\n",
        "The three subsections below explore what the model has learned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyO5f0ltFLrR"
      },
      "source": [
        "## The validation loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwMtIj6cFLrR"
      },
      "source": [
        "Measure the validation loss of your model. This part could be used, as in our previous notebook, in deciding what is a likely, vs. unlikely summary for an article.\n",
        "\n",
        "We will use the code here with the unreleased test-set to evaluate your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M20e8ksXFLrS"
      },
      "source": [
        "gc.collect()\n",
        "model_id = \"test1\"\n",
        "save_dict = th.load(root_folder+'models/part2/'+f\"model_{model_id}.pt\", map_location='cpu')\n",
        "model = Transformer(**save_dict['kwargs'])\n",
        "model.load_state_dict(save_dict['model_state_dict'])\n",
        "set_device('cpu')\n",
        "model.eval()\n",
        "trainer.model = model"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpBA-TXJFLrT",
        "outputId": "ea45d258-c1da-4461-ca3d-c5965225d43c"
      },
      "source": [
        "gc.collect()\n",
        "losses = []\n",
        "for i in tqdm(range(100)):\n",
        "    batch = build_batch(d_valid, 1)\n",
        "    # Build the feed-dict connecting placeholders and mini-batch\n",
        "    batch_input, batch_input_mask, batch_output, batch_output_mask = [th.tensor(tensor) for tensor in batch]\n",
        "    batch = {'source_sequence': batch_input, 'target_sequence': batch_output,\n",
        "            'encoder_mask': batch_input_mask, 'decoder_mask': batch_output_mask}\n",
        "    valid_loss, accuracy = trainer(batch,optimize=False)\n",
        "    losses.append(float(valid_loss.cpu().item()))\n",
        "print(\"Validation loss:\", np.mean(losses))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/100 [00:00<00:18,  5.39it/s]\u001b[A\n",
            "  2%|▏         | 2/100 [00:00<00:18,  5.43it/s]\u001b[A\n",
            "  3%|▎         | 3/100 [00:00<00:18,  5.37it/s]\u001b[A\n",
            "  4%|▍         | 4/100 [00:00<00:17,  5.47it/s]\u001b[A\n",
            "  5%|▌         | 5/100 [00:00<00:17,  5.56it/s]\u001b[A\n",
            "  6%|▌         | 6/100 [00:01<00:17,  5.50it/s]\u001b[A\n",
            "  7%|▋         | 7/100 [00:01<00:16,  5.48it/s]\u001b[A\n",
            "  8%|▊         | 8/100 [00:01<00:16,  5.50it/s]\u001b[A\n",
            "  9%|▉         | 9/100 [00:01<00:16,  5.39it/s]\u001b[A\n",
            " 10%|█         | 10/100 [00:01<00:16,  5.44it/s]\u001b[A\n",
            " 11%|█         | 11/100 [00:02<00:16,  5.49it/s]\u001b[A\n",
            " 12%|█▏        | 12/100 [00:02<00:15,  5.51it/s]\u001b[A\n",
            " 13%|█▎        | 13/100 [00:02<00:15,  5.52it/s]\u001b[A\n",
            " 14%|█▍        | 14/100 [00:02<00:15,  5.49it/s]\u001b[A\n",
            " 15%|█▌        | 15/100 [00:02<00:15,  5.54it/s]\u001b[A\n",
            " 16%|█▌        | 16/100 [00:02<00:15,  5.58it/s]\u001b[A\n",
            " 17%|█▋        | 17/100 [00:03<00:14,  5.58it/s]\u001b[A\n",
            " 18%|█▊        | 18/100 [00:03<00:14,  5.56it/s]\u001b[A\n",
            " 19%|█▉        | 19/100 [00:03<00:14,  5.52it/s]\u001b[A\n",
            " 20%|██        | 20/100 [00:03<00:14,  5.50it/s]\u001b[A\n",
            " 21%|██        | 21/100 [00:03<00:14,  5.49it/s]\u001b[A\n",
            " 22%|██▏       | 22/100 [00:04<00:14,  5.47it/s]\u001b[A\n",
            " 23%|██▎       | 23/100 [00:04<00:14,  5.48it/s]\u001b[A\n",
            " 24%|██▍       | 24/100 [00:04<00:13,  5.53it/s]\u001b[A\n",
            " 25%|██▌       | 25/100 [00:04<00:13,  5.58it/s]\u001b[A\n",
            " 26%|██▌       | 26/100 [00:04<00:13,  5.58it/s]\u001b[A\n",
            " 27%|██▋       | 27/100 [00:04<00:13,  5.54it/s]\u001b[A\n",
            " 28%|██▊       | 28/100 [00:05<00:13,  5.43it/s]\u001b[A\n",
            " 29%|██▉       | 29/100 [00:05<00:12,  5.47it/s]\u001b[A\n",
            " 30%|███       | 30/100 [00:05<00:12,  5.53it/s]\u001b[A\n",
            " 31%|███       | 31/100 [00:05<00:12,  5.51it/s]\u001b[A\n",
            " 32%|███▏      | 32/100 [00:05<00:12,  5.46it/s]\u001b[A\n",
            " 33%|███▎      | 33/100 [00:05<00:12,  5.52it/s]\u001b[A\n",
            " 34%|███▍      | 34/100 [00:06<00:12,  5.50it/s]\u001b[A\n",
            " 35%|███▌      | 35/100 [00:06<00:11,  5.52it/s]\u001b[A\n",
            " 36%|███▌      | 36/100 [00:06<00:11,  5.56it/s]\u001b[A\n",
            " 37%|███▋      | 37/100 [00:06<00:11,  5.43it/s]\u001b[A\n",
            " 38%|███▊      | 38/100 [00:06<00:11,  5.52it/s]\u001b[A\n",
            " 39%|███▉      | 39/100 [00:07<00:10,  5.57it/s]\u001b[A\n",
            " 40%|████      | 40/100 [00:07<00:10,  5.53it/s]\u001b[A\n",
            " 41%|████      | 41/100 [00:07<00:10,  5.53it/s]\u001b[A\n",
            " 42%|████▏     | 42/100 [00:07<00:10,  5.54it/s]\u001b[A\n",
            " 43%|████▎     | 43/100 [00:07<00:10,  5.61it/s]\u001b[A\n",
            " 44%|████▍     | 44/100 [00:07<00:10,  5.60it/s]\u001b[A\n",
            " 45%|████▌     | 45/100 [00:08<00:09,  5.55it/s]\u001b[A\n",
            " 46%|████▌     | 46/100 [00:08<00:09,  5.53it/s]\u001b[A\n",
            " 47%|████▋     | 47/100 [00:08<00:09,  5.50it/s]\u001b[A\n",
            " 48%|████▊     | 48/100 [00:08<00:09,  5.52it/s]\u001b[A\n",
            " 49%|████▉     | 49/100 [00:08<00:09,  5.54it/s]\u001b[A\n",
            " 50%|█████     | 50/100 [00:09<00:09,  5.50it/s]\u001b[A\n",
            " 51%|█████     | 51/100 [00:09<00:09,  5.40it/s]\u001b[A\n",
            " 52%|█████▏    | 52/100 [00:09<00:09,  5.12it/s]\u001b[A\n",
            " 53%|█████▎    | 53/100 [00:09<00:09,  4.96it/s]\u001b[A\n",
            " 54%|█████▍    | 54/100 [00:09<00:09,  4.94it/s]\u001b[A\n",
            " 55%|█████▌    | 55/100 [00:10<00:09,  4.93it/s]\u001b[A\n",
            " 56%|█████▌    | 56/100 [00:10<00:09,  4.87it/s]\u001b[A\n",
            " 57%|█████▋    | 57/100 [00:10<00:08,  4.97it/s]\u001b[A\n",
            " 58%|█████▊    | 58/100 [00:10<00:08,  5.01it/s]\u001b[A\n",
            " 59%|█████▉    | 59/100 [00:10<00:08,  5.01it/s]\u001b[A\n",
            " 60%|██████    | 60/100 [00:11<00:07,  5.02it/s]\u001b[A\n",
            " 61%|██████    | 61/100 [00:11<00:07,  5.18it/s]\u001b[A\n",
            " 62%|██████▏   | 62/100 [00:11<00:07,  5.29it/s]\u001b[A\n",
            " 63%|██████▎   | 63/100 [00:11<00:06,  5.42it/s]\u001b[A\n",
            " 64%|██████▍   | 64/100 [00:11<00:06,  5.44it/s]\u001b[A\n",
            " 65%|██████▌   | 65/100 [00:11<00:06,  5.48it/s]\u001b[A\n",
            " 66%|██████▌   | 66/100 [00:12<00:06,  5.51it/s]\u001b[A\n",
            " 67%|██████▋   | 67/100 [00:12<00:05,  5.53it/s]\u001b[A\n",
            " 68%|██████▊   | 68/100 [00:12<00:05,  5.56it/s]\u001b[A\n",
            " 69%|██████▉   | 69/100 [00:12<00:05,  5.52it/s]\u001b[A\n",
            " 70%|███████   | 70/100 [00:12<00:05,  5.57it/s]\u001b[A\n",
            " 71%|███████   | 71/100 [00:13<00:05,  5.54it/s]\u001b[A\n",
            " 72%|███████▏  | 72/100 [00:13<00:05,  5.55it/s]\u001b[A\n",
            " 73%|███████▎  | 73/100 [00:13<00:04,  5.58it/s]\u001b[A\n",
            " 74%|███████▍  | 74/100 [00:13<00:04,  5.53it/s]\u001b[A\n",
            " 75%|███████▌  | 75/100 [00:13<00:04,  5.52it/s]\u001b[A\n",
            " 76%|███████▌  | 76/100 [00:13<00:04,  5.54it/s]\u001b[A\n",
            " 77%|███████▋  | 77/100 [00:14<00:04,  5.59it/s]\u001b[A\n",
            " 78%|███████▊  | 78/100 [00:14<00:03,  5.58it/s]\u001b[A\n",
            " 79%|███████▉  | 79/100 [00:14<00:03,  5.53it/s]\u001b[A\n",
            " 80%|████████  | 80/100 [00:14<00:03,  5.54it/s]\u001b[A\n",
            " 81%|████████  | 81/100 [00:14<00:03,  5.59it/s]\u001b[A\n",
            " 82%|████████▏ | 82/100 [00:15<00:03,  5.57it/s]\u001b[A\n",
            " 83%|████████▎ | 83/100 [00:15<00:03,  5.59it/s]\u001b[A\n",
            " 84%|████████▍ | 84/100 [00:15<00:02,  5.52it/s]\u001b[A\n",
            " 85%|████████▌ | 85/100 [00:15<00:02,  5.56it/s]\u001b[A\n",
            " 86%|████████▌ | 86/100 [00:15<00:02,  5.56it/s]\u001b[A\n",
            " 87%|████████▋ | 87/100 [00:15<00:02,  5.60it/s]\u001b[A\n",
            " 88%|████████▊ | 88/100 [00:16<00:02,  5.56it/s]\u001b[A\n",
            " 89%|████████▉ | 89/100 [00:16<00:01,  5.59it/s]\u001b[A\n",
            " 90%|█████████ | 90/100 [00:16<00:01,  5.67it/s]\u001b[A\n",
            " 91%|█████████ | 91/100 [00:16<00:01,  5.69it/s]\u001b[A\n",
            " 92%|█████████▏| 92/100 [00:16<00:01,  5.67it/s]\u001b[A\n",
            " 93%|█████████▎| 93/100 [00:17<00:01,  5.65it/s]\u001b[A\n",
            " 94%|█████████▍| 94/100 [00:17<00:01,  5.57it/s]\u001b[A\n",
            " 95%|█████████▌| 95/100 [00:17<00:00,  5.58it/s]\u001b[A\n",
            " 96%|█████████▌| 96/100 [00:17<00:00,  5.57it/s]\u001b[A\n",
            " 97%|█████████▋| 97/100 [00:17<00:00,  5.47it/s]\u001b[A\n",
            " 98%|█████████▊| 98/100 [00:17<00:00,  5.48it/s]\u001b[A\n",
            " 99%|█████████▉| 99/100 [00:18<00:00,  5.52it/s]\u001b[A\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 5.12046933054924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUDdDx4VFLrU"
      },
      "source": [
        "# Your best performing model should go here.\n",
        "os.makedirs(root_folder+\"best_models\",exist_ok=True)\n",
        "best_model_file = root_folder+\"best_models/part2_best_model.pt\"\n",
        "th.save(save_dict,best_model_file)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT_BvxRqFLrV"
      },
      "source": [
        "## Generating an article's summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1225KszFLrW"
      },
      "source": [
        "This model we have built is meant to be used to generate summaries for new articles we do not have summaries for.\n",
        "We got a [news article](https://www.chicagotribune.com/news/local/breaking/ct-met-officer-shot-20190309-story.html) from the Chicago Tribune about a police shooting, and want to use our model to produce a summary.\n",
        "\n",
        "As you will see, our model is still limited in its ability, and will most likely not produce an interpretible summary, however, with more data and training, this model would be able to produce good summaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t9dSsv1FLrW",
        "outputId": "a336f7e4-c592-4619-f73d-08876b99ff54"
      },
      "source": [
        "article_text = \"A 34-year-old Chicago police officer has been shot in the shoulder during the execution of a search warrant in the Humboldt Park neighborhood, police say. The alleged shooter, a 19-year-old woman, was in custody. The shooting happened about 7:20 p.m. in the 2700 block of West Potomac Avenue, police said. The officer, part of the Grand Central District tactical unit, was taken to Stroger Hospital. While officers were serving a \\\"typical\\\" search warrant for \\\"narcotics and illegal weapons\\\" and were attempting to reach a rear door, \\\"a shot was fired,\\\" striking the tactical officer in the shoulder, said Chicago police Superintendent Eddie Johnson during a news briefing outside the hospital. He said the officer, who has about four or five years on the job, was \\\"stable\\\" but in critical condition. \\\"His family is here,\\\" Johnson said. \\\"He’s talking a lot and just wants the ordeal to be over.\\\" He said this incident serves as just another reminder of how dangerous a police officer’s job is. At the scene of the shooting, crime tape closed Potomac from Washtenaw Avenue to California Avenue and encompassed the alley west of the brick apartment building, south of Potomac. Dozens of officers stood in the alley, while even more walked up and down the street. Neighbors gathered at the edge of the yellow tape on the sidewalk along California and watched them work. Standing next to a man, a woman talked to police in the crime scene, across the street. \\\"We're not under arrest? We can go?\\\" the woman checked with officers. They told her she could go, and she and the man walked underneath the yellow tape and out of the crime scene.\"\n",
        "input_length = 400\n",
        "output_length = 100\n",
        "\n",
        "# Process the capitalization with the preprocess_capitalization of the capita package.\n",
        "article_text = capita.preprocess_capitalization(article_text)\n",
        "\n",
        "# Numerize the tokens of the processed text using the loaded sentencepiece model.\n",
        "numerized = sp.EncodeAsIds(article_text)\n",
        "# Pad the sequence and keep the mask of the input\n",
        "padded, mask = pad_sequence(numerized, pad_index, input_length)\n",
        "\n",
        "# Making the news article into a batch of size one, to be fed to the neural network.\n",
        "encoder_input = np.array([padded])\n",
        "encoder_mask = np.array([mask])\n",
        "\n",
        "decoded_so_far = [0]\n",
        "\n",
        "for j in range(output_length):\n",
        "    padded_decoder_input, decoder_mask = pad_sequence(decoded_so_far, pad_index, output_length)\n",
        "    padded_decoder_input = [padded_decoder_input]\n",
        "    decoder_mask = [decoder_mask]\n",
        "    print(\"========================\")\n",
        "    print(padded_decoder_input)\n",
        "    # Use the model to find the distrbution over the vocabulary for the next word\n",
        "    batch = (encoder_input,encoder_mask,padded_decoder_input,decoder_mask)\n",
        "    batch_input, batch_input_mask, batch_output, batch_output_mask = [th.tensor(tensor) for tensor in batch]\n",
        "    batch = {'source_sequence': batch_input, 'target_sequence': batch_output,\n",
        "            'encoder_mask': batch_input_mask, 'decoder_mask': batch_output_mask}\n",
        "    logits = trainer.model(**batch).cpu().detach().numpy()\n",
        "\n",
        "    chosen_words = np.argmax(logits, axis=2) # Take the argmax, getting the most likely next word\n",
        "    decoded_so_far.append(int(chosen_words[0, j])) # We add it to the summary so far\n",
        "\n",
        "\n",
        "print(\"The final summary:\")\n",
        "print(\"\".join([vocab[i] for i in decoded_so_far]).replace(\"▁\", \" \"))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================\n",
            "[[0, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 829, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 829, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 829, 4, 6, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 829, 4, 6, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 829, 4, 6, 3, 3, 9998, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 829, 4, 6, 3, 3, 4, 9998, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 829, 4, 6, 3, 3, 4, 4, 9998, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 829, 4, 6, 3, 3, 4, 4, 3, 9998, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 829, 4, 6, 3, 3, 4, 4, 3, 3, 9998, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 829, 4, 6, 3, 3, 4, 4, 3, 3, 3, 9998, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 829, 4, 6, 3, 3, 4, 4, 3, 3, 3, 3, 9998, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 829, 4, 6, 3, 3, 4, 4, 3, 3, 3, 3, 4, 9998]]\n",
            "========================\n",
            "[[0, 3, 3, 4, 4, 5, 107, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 7410, 7410, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 829, 4, 6, 3, 3, 4, 4, 3, 3, 3, 3, 4, 4]]\n",
            "The final summary:\n",
            "<unk>  ↑↑ the police  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑   ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑  ↑↑    consideration consideration  ↑↑  ↑↑   player↑ .  ↑↑    ↑↑ \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cQee2cnFLrY"
      },
      "source": [
        "## Word vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quAhOWx-FLrZ"
      },
      "source": [
        "The model we train learns word representations for each word in our vocabulary. A word represention is a vector of **dim** size.\n",
        "\n",
        "It is common in NLP to inspect the word vectors, as some properties of language often appear in the embedding structure.\n",
        "\n",
        "\n",
        "We are going to load the word embeddings learned by our model, and inspect it.\n",
        "Because our network was not trained for long, we are going for the simplest patterns, but if we let the network train longer, it learns more complex, semantic patterns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sny1RT7cFLrZ",
        "outputId": "5b211358-439a-4e40-a082-15df128671de"
      },
      "source": [
        "# We help you load the matrix, as it is hidden within the Transformer structure.\n",
        "E = trainer.model.encoder.embedding_layer.embedding.weight.cpu().detach().numpy()\n",
        "\n",
        "print(\"The embedding matrix has shape:\", E.shape)\n",
        "print(\"The vocabulary has length:\", len(vocab))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The embedding matrix has shape: (10000, 160)\n",
            "The vocabulary has length: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR4SX887FLra"
      },
      "source": [
        "Pronouns serve very similar purposes, therefore we should expect the representation of \"he\" and \"she\" to be similar, and have cosine similarity.\n",
        "\n",
        "- **TODO**:  Find the cosine similarity between the vectors that represent words \"she\" and \"he\".\n",
        "- **TODO**:  Find the cosine similarity between the vectors that represent words \"more\" and \"less\".\n",
        "\n",
        "We can contrast that with the cosine similarity to a random, non-related word, like \"ball\", or \"gorilla\".\n",
        "- **TODO**: Compute the cosine similarity between \"she\" and \"ball\".\n",
        "- **TODO**: Compute the cosine similarity between \"more\" and \"protest\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wR1tDGMFLrb",
        "outputId": "89c3fe53-d632-4ae9-cadf-e8343752a3b2"
      },
      "source": [
        "def cosine_sim(v1, v2):\n",
        "    # TODO: Implement the cosine similarity of 2 vectors. Careful: the words might not have unit norm.\n",
        "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "for w1, w2 in [(\"she\", \"he\"), (\"more\", \"less\"), (\"she\", \"ball\"), (\"more\", \"gorilla\")]:\n",
        "    w1_index = vocab.index('▁'+w1) # The index of the first  word in our vocabulary\n",
        "    w2_index = vocab.index('▁'+w2) # The index of the second word in our vocabulary\n",
        "    w1_vec = E[w1_index] # Get the embedding vector of the first  word\n",
        "    w2_vec = E[w2_index] # Get the embedding vector of the second word\n",
        "    \n",
        "    print(w1,\" vs. \", w2, \"similarity:\",cosine_sim(w1_vec, w2_vec))\n",
        "validate_to_array(lambda f,i: (f(*i),i), (cosine_sim,tuple(20*np.random.random((2,1000))-1)),'cosine_sim', root_folder)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "she  vs.  he similarity: -0.012258213\n",
            "more  vs.  less similarity: 0.085029356\n",
            "she  vs.  ball similarity: -0.05001873\n",
            "more  vs.  gorilla similarity: 0.040887583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kJaau6wFLrc"
      },
      "source": [
        "These effects are unfortunately small, as we have only trained the network on a few hours on a few thousand articles.\n",
        "However, the same model trained for longer on more data exhibits many interesting semantic and syntactic patterns, such as:\n",
        "\n",
        "- Words vectors with high cosine similarity usually represent words that have semantic similarity (such as duck and pigeon)\n",
        "- Analogies can occur, a famous case is that of: woman - man + king ≈ queen. Or france - paris + rome ≈ italy.\n",
        "\n",
        "- Looking at top-k similar words can help find synonyms.\n",
        "\n",
        "To read examples of more complex patterns that appear in word embedding spaces, read [this blog](https://explosion.ai/blog/sense2vec-with-spacy). To play with a live demo and try similarities on rich word embeddings, [go here.](https://explosion.ai/demos/sense2vec)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHxM8eDmFLrd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}